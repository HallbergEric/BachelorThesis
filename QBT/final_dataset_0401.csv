Id,PostTypeId,AcceptedAnswerId,ParentId,CreationDate,DeletionDate,Score,ViewCount,Body,OwnerUserId,OwnerDisplayName,LastEditorUserId,LastEditorDisplayName,LastEditDate,LastActivityDate,Title,Tags,AnswerCount,CommentCount,FavoriteCount,ClosedDate,CommunityOwnedDate,ContentLicense,Body_new
54114957,1,,,2019-01-09 16:55:31,,1,881,"<p>I am integrating a payment solution which uses web hooks. The payment provider is given the Cognito userid (or user sub) during checkout. Upon successful checkout, the payment provider calls a web hook which I've implemented using AWS Lambda/Gateway and Python. Within this web hook I get the payment status and the Cognito user id.</p>

<p>What I'd like to do next is update or set an attribute on the given Cognito user id for the subscription status. </p>

<p>I've found pieces of JavaScript code which seem like they get me sort of there but I am missing something from how this should work. Here is the JS code I found to search for a user:</p>

<pre><code>var cog = new AWS.CognitoIdentityServiceProvider();

var filter = ""sub = \"""" + userSub + ""\"""";
var req = {
    ""Filter"": filter,
    ""UserPoolId"": ""your pool id"" // looks like us-east-9_KDFn1cvys
};

cog.listUsers(req, function(err, data) {
    if (err) {
        console.log(err);
    }
    else {
        if (data.Users.length === 1){ //as far as we search by sub, should be only one user.
            var user = data.Users[0];
            var attributes = data.Users[0].Attributes;
        } else {
            console.log(""Something wrong."");
        }
    }
});
</code></pre>

<p>I need a Python version of the above... also I am not quite sure how to initialize the AWS.CognitoIdentityServiceProvider in order to do the search. It seems like I need to have an IAM credentials set up in order to search a user identity pool, no? </p>

<p>Any pointers on how to search for a user in a Cognito identity pool using Python would be appreciated!</p>
",441638.0,,,,,2019-06-17 21:04:14,Looking for Python example to search for a Cognito identity and update attributes,<python-3.x><aws-lambda><boto3><amazon-cognito>,1,0,,,,CC BY-SA 4.0,I am integrating a payment solution which uses web hooks  The payment provider is given the Cognito userid (or user sub) during checkout  Upon successful checkout  the payment provider calls a web hook which Ive implemented using AWS Lambda/Gateway and Python  Within this web hook I get the payment status and the Cognito user id  What Id like to do next is update or set an attribute on the given Cognito user id for the subscription status   Ive found pieces of JavaScript code which seem like they get me sort of there but I am missing something from how this should work  Here is the JS code I found to search for a user:  I need a Python version of the above    also I am not quite sure how to initialize the AWS CognitoIdentityServiceProvider in order to do the search  It seems like I need to have an IAM credentials set up in order to search a user identity pool  no   Any pointers on how to search for a user in a Cognito identity pool using Python would be appreciated  
36036500,1,36161069.0,,2016-03-16 13:02:51,,0,627,"<p>I am using API Gateway to build a patch method.
In then <code>Integration Request - Mapping Template</code> i added:</p>

<pre><code>{ ""id"": ""$input.params('subscription-id')"",
  ""env"": ""$stageVariables['env']"",
  ""street"": $input.json('street'),
  ""address_name"": $input.json('address_name'),
  ""payment_day"": $input.json('payment_day'),
 }
</code></pre>

<p>As a patch http method, the user's API is not required to pass all the parameters.</p>

<p>So if the user doesn't pass, for e.g. payment_day, the field is going to be <code>''</code>. The <code>''</code> can be a valid value field. So i have two options:</p>

<ul>
<li>Put a NULL value on the payment_day field.</li>
<li>Remove the payment_day from JSON request.</li>
</ul>

<p>Is it possible to do this on <code>API Gateway Integration Request -Mapping Template</code>? Does anyone has a workaround?</p>
",312444.0,,,,,2016-03-22 17:18:59,Using AWS API Gateway to build a patch method,<amazon-web-services><aws-lambda><aws-api-gateway>,1,1,,,,CC BY-SA 3.0,I am using API Gateway to build a patch method  In then  i added:  As a patch http method  the users API is not required to pass all the parameters  So if the user doesnt pass  for e g  payment_day  the field is going to be   The  can be a valid value field  So i have two options:  Put a NULL value on the payment_day field  Remove the payment_day from JSON request   Is it possible to do this on   Does anyone has a workaround  
54287228,1,,,2019-01-21 09:47:52,,1,696,"<p>I have an AWS Lambda function which triggers https request to Google API. I want the function to be awaitable, so that it does not end immediately, but only after getting response from Google API.
Yes, I know I pay for the execution, but this will not be called often, so it is fine.</p>

<p>The problem is that the http request does not seem to fire correctly. The callback is never executed.</p>

<p>I have made sure that the async/await works as expected by using setTimeout in a Promise. So the issue is somewhere in the https.request.</p>

<p>Also note that I am using Pulumi to deploy to AWS, so there might be some hidden problem in there. I just can't figure out where.</p>

<hr>

<p><strong>The relevant code:</strong></p>

<p><em>AWS Lambda which calls the Google API</em></p>

<pre><code>import config from '../../config';
import { IUserInfo } from '../../interfaces';
const https = require('https');

function sendHttpsRequest(options: any): Promise&lt;any&gt; {
    console.log(`sending request to ${options.host}`);
    console.log(`Options are ${JSON.stringify(options)}`);

    return new Promise(function (resolve, reject) {
        console.log(` request to ${options.host} has been sent A`);

        let body = new Array&lt;Buffer&gt;();
        const request = https.request(options, function (res: any) {
            console.log('statusCode:', res.statusCode);
            console.log('headers:', res.headers);

            if (res.statusCode != 200) {
                reject(res.statusCode);
            }

            res.on('data', (data: any) =&gt; {
              console.log(`body length is ${body.length}`);
              console.log('data arrived', data);
              body.push(data);
              console.log('pushed to array');
              console.log(data.toString());
            });
        });

        request.on('end', () =&gt; {
            console.error('Request ended');
            // at this point, `body` has the entire request body stored in it as a string
            let result = Buffer.concat(body).toString();
            resolve(result);
        });

        request.on('error', async (err: Error) =&gt; {
          console.error('Errooooorrrr', err.stack);
          console.error('Errooooorrrr request failed');
          reject(err);
        });

        request.end();

      console.log(` request to ${options.host} has been sent B`);
    });
}

/**
 * AWS Lambda to create new Google account in TopMonks domain
 */
export default async function googleLambdaImplementation(userInfo: IUserInfo) {

    const payload = JSON.stringify({
        ""primaryEmail"": userInfo.topmonksEmail,
        ""name"": {
            ""givenName"": userInfo.firstName,
            ""familyName"": userInfo.lastName
        },
        ""password"": config.defaultPassword,
        ""changePasswordAtNextLogin"": true
    });

    const resultResponse: Response = {
        statusCode: 200,
        body: 'Default response. This should not come back to users'
    }     

    console.log('Calling google api via post request');

    try {
        const options = {
            host: 'www.googleapis.com',
            path: '/admin/directory/v1/users',
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
                'Content-Length': payload.length.toString()
            },
            form: payload
        }

        const responseFromGoogle = await sendHttpsRequest(options);
        console.log('responseFromGoogle', JSON.stringify(responseFromGoogle));
    }
    catch (err) {
        console.log('Calling google api failed with error', err);
        resultResponse.statusCode = 503;
        resultResponse.body = `Error creating new Google Account for ${userInfo.topmonksEmail}.`;
        return resultResponse;
    }

    console.log('request to google sent');
    return resultResponse;
 }
</code></pre>
",552194.0,,552194.0,,2019-01-22 13:10:28,2019-01-22 13:10:28,Awaiting http request in AWS Lambda / Pulumi,<node.js><amazon-web-services><aws-lambda><pulumi>,1,3,1.0,,,CC BY-SA 4.0,I have an AWS Lambda function which triggers https request to Google API  I want the function to be awaitable  so that it does not end immediately  but only after getting response from Google API  Yes  I know I pay for the execution  but this will not be called often  so it is fine  The problem is that the http request does not seem to fire correctly  The callback is never executed  I have made sure that the async/await works as expected by using setTimeout in a Promise  So the issue is somewhere in the https request  Also note that I am using Pulumi to deploy to AWS  so there might be some hidden problem in there  I just cant figure out where   The relevant code: AWS Lambda which calls the Google API  
36298053,1,36300028.0,,2016-03-30 01:28:16,,2,634,"<p><strong>My current Situation:</strong></p>

<p>I currently have a Python script that fetches data via HTTP endpoints and calculates and generates hundreds/thousands of reports daily. Currently it runs on an AWS EC2 instance where a queue is used to split the reports it needs to generate across four threads. Four at a time, the script fetches data, computes each report, and saves it to a PostgreSQL Amazon RDS.</p>

<p><strong>The Problem:</strong></p>

<p>As the project scales, my script won't be able to compute fast enough and won't be able to generate all the reports it needs in a day with the current method.</p>

<p><strong>Looking For a Solution:</strong></p>

<p>I stumbled across Amazon Lambda but I haven't found anyone using it for a use case similar to mine. My plan would be to upload/put each report needed to be generated into it's own S3 bucket then have the Lambda Function trigger when the bucket is created. The Lambda function would do all the data fetching (from HTTP endpoints) and all the calculations and save it to a row in my PostgreSQL Amazon RDS. In theory, this would make everything parallel and would eliminate the need for a queue waiting for resources to be freed up.</p>

<p>Basically I am looking for a solution to make sure my script is able to run daily and finish each day without over-running into the next day.</p>

<p><strong>My Questions:</strong></p>

<p>Would Amazon Lambda be suitable for something like this? </p>

<p>Would it be costly to do something like this with Amazon Lambda (creating hundreds/thousands of s3 buckets a day)? </p>

<p>Is there better options?</p>

<p>Any help, recommendations, insight, or tips is greatly appreciated. Thanks!</p>
",1076523.0,,,,,2016-03-30 13:07:38,Would Amazon Lambda Be Suitable for Computing Hundreds of Reports a Day in Parallel?,<python><amazon-web-services><amazon-s3><amazon-ec2><aws-lambda>,3,0,,,,CC BY-SA 3.0,My current Situation: I currently have a Python script that fetches data via HTTP endpoints and calculates and generates hundreds/thousands of reports daily  Currently it runs on an AWS EC2 instance where a queue is used to split the reports it needs to generate across four threads  Four at a time  the script fetches data  computes each report  and saves it to a PostgreSQL Amazon RDS  The Problem: As the project scales  my script wont be able to compute fast enough and wont be able to generate all the reports it needs in a day with the current method  Looking For a Solution: I stumbled across Amazon Lambda but I havent found anyone using it for a use case similar to mine  My plan would be to upload/put each report needed to be generated into its own S3 bucket then have the Lambda Function trigger when the bucket is created  The Lambda function would do all the data fetching (from HTTP endpoints) and all the calculations and save it to a row in my PostgreSQL Amazon RDS  In theory  this would make everything parallel and would eliminate the need for a queue waiting for resources to be freed up  Basically I am looking for a solution to make sure my script is able to run daily and finish each day without over-running into the next day  My Questions: Would Amazon Lambda be suitable for something like this   Would it be costly to do something like this with Amazon Lambda (creating hundreds/thousands of s3 buckets a day)   Is there better options  Any help  recommendations  insight  or tips is greatly appreciated  Thanks  
54473954,1,,,2019-02-01 06:21:53,,1,246,"<p>i am new to aws serverless, and trying to host django app in aws serverless.</p>

<p>now aws serverless uses s3 bucket for static website hosting which cost around $0.50 (I am in free tier). </p>

<p>my question is instead of hosting static website can i not give public access to s3 bucket? as it would save me money. is it possible to use public bucket for aws serverless?  </p>
",8576229.0,,,,,2019-02-01 09:08:51,give public access to s3 bucket rather than serving static website for aws serverless application,<django><amazon-web-services><amazon-s3><aws-serverless>,1,3,,,,CC BY-SA 4.0,i am new to aws serverless  and trying to host django app in aws serverless  now aws serverless uses s3 bucket for static website hosting which cost around $0 50 (I am in free tier)   my question is instead of hosting static website can i not give public access to s3 bucket  as it would save me money  is it possible to use public bucket for aws serverless    
55119544,1,55119883.0,,2019-03-12 10:43:58,,2,4664,"<p>I'm creating a monitoring system for an external API on which I need to invoke a function if the response is different from the initial request.</p>

<p>Currently without any long polling around my functions I've got the following: </p>

<pre><code>  let gameAPI = new game(
      [
        ""email@email.com"",
        ""password"",
        ""secret"",
        ""secret""
      ]
  );


  let lastModified;

  gameAPI.login().then(() =&gt; {

      gameAPI.getProfileStats(process.argv[2], process.argv[3], process.argv[4]).then(stats =&gt; {
          //first run grab this only
          if(lastModified &lt; stats.data.lastModified) {
              //we found new data, stop polling &amp; update mySQL database.
          } else {
              //continue to run every 1 minute...
          }
      });

  });
</code></pre>

<p>Would <strong><em>AWS Lambda</em></strong> be a good solution for this theory? if so, how would I go about doing it since Lambda charges for computer power usage?</p>
",2505093.0,,174777.0,,2019-03-12 22:06:32,2019-03-12 22:06:32,Long polling with AWS Lambda,<node.js><amazon-web-services><aws-lambda>,2,0,,,,CC BY-SA 4.0,Im creating a monitoring system for an external API on which I need to invoke a function if the response is different from the initial request  Currently without any long polling around my functions Ive got the following:   Would AWS Lambda be a good solution for this theory  if so  how would I go about doing it since Lambda charges for computer power usage  
55249817,1,55355058.0,,2019-03-19 20:54:26,,0,286,"<p>I have a Django app deployed on AWS Lambda through Zappa and my app needs to communicate with the public internet, so I need to use a NAT Instance. I am using a NAT instance because it's about 10x cheaper than a NAT Gateway using the free tier. The downside is that unlike NAT Gateway, a NAT Instance needs actual maintenance, and I am unsure what type of maintenance it needs. I want to learn about things I need to do to keep my server running well and healthy.</p>

<p>What are things I can do to make sure of that?</p>

<p>Here is my AWS Architecture:</p>

<p>All of the following is in my VPC. I have 1 subnet in ca-central-1a and 1 in ca-central-1b. In the route table, both subnets point to my NAT Instance. I have a 3rd subnet in ca-central-1b and in the route table it points to an internet gateway. My NAT Instance is in ca-central-1b.</p>

<p>My NAT Instance security group NATSG has HTTP and HTTPS inbounds from both of my subnets in ca-central-1a and ca-central-1b and outbound to 0.0.0.0/0. <strong>Should I make another NAT Instance in ca-central-1a and make it only inbound from the subnet in ca-central-1a i.e 1 NAT Instance for each subnet? Would that be healthier/safer?</strong></p>

<p>Extra information:</p>

<p><strong>I disabled Source/dest check. Was that a good idea?</strong></p>

<p>For my AMI I chose a recent community AMI amzn-ami-vpc-nat and I created an Auto Scale Group which has my NAT instance. It only has 1 instance, <strong>is there any point of the Auto Scale Group if there's only 1 instance in it?</strong> I am not sure that I am using the Auto Scale Group right, I simply created it but haven't configured anything.</p>
",8083096.0,,,,,2019-04-15 13:16:59,NAT Instance maintenance,<aws-lambda><aws-vpc>,2,6,,,,CC BY-SA 4.0,I have a Django app deployed on AWS Lambda through Zappa and my app needs to communicate with the public internet  so I need to use a NAT Instance  I am using a NAT instance because its about 10x cheaper than a NAT Gateway using the free tier  The downside is that unlike NAT Gateway  a NAT Instance needs actual maintenance  and I am unsure what type of maintenance it needs  I want to learn about things I need to do to keep my server running well and healthy  What are things I can do to make sure of that  Here is my AWS Architecture: All of the following is in my VPC  I have 1 subnet in ca-central-1a and 1 in ca-central-1b  In the route table  both subnets point to my NAT Instance  I have a 3rd subnet in ca-central-1b and in the route table it points to an internet gateway  My NAT Instance is in ca-central-1b  My NAT Instance security group NATSG has HTTP and HTTPS inbounds from both of my subnets in ca-central-1a and ca-central-1b and outbound to 0 0 0 0/0  Should I make another NAT Instance in ca-central-1a and make it only inbound from the subnet in ca-central-1a i e 1 NAT Instance for each subnet  Would that be healthier/safer  Extra information: I disabled Source/dest check  Was that a good idea  For my AMI I chose a recent community AMI amzn-ami-vpc-nat and I created an Auto Scale Group which has my NAT instance  It only has 1 instance  is there any point of the Auto Scale Group if theres only 1 instance in it  I am not sure that I am using the Auto Scale Group right  I simply created it but havent configured anything  
55260750,1,,,2019-03-20 12:25:07,,1,450,"<p>I am trying to send a Woocommerce webhook to AWS API Gateway. When i put in my API Gateway URL on Amazon I get the following error:</p>

<pre><code>Error: Delivery URL returned response code: 415
</code></pre>

<p>I think this is related to headers, there is an option to Create CORS in API-Gateway that I have now done. Which then adds an OPTION method but I still get <code>undefined</code> in Cloudwatch</p>

<p><a href=""https://i.stack.imgur.com/TRI1W.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TRI1W.jpg"" alt=""enter image description here""></a></p>

<p>I created a POST method and used the mapping template below with application/json and the setting <code>When there are no templates defined (recommended)</code></p>

<pre><code>{
    ""body"" : $input.json('$'),
        ""headers"": {
        #foreach($header in $input.params().header.keySet())
        ""$header"": ""$util.escapeJavaScript($input.params().header.get($header))"" #if ($foreach.hasNext), #end

        #end
    }
}
</code></pre>

<p><a href=""https://i.stack.imgur.com/8c8tO.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8c8tO.jpg"" alt=""enter image description here""></a></p>

<p>Here is a mini Lambda Node function that just outputs a Woocommerce order number to the console and shows in Cloudwatch whether the API passthrough is working.</p>

<pre><code>exports.handler = (event, context, callback, err) =&gt; {
    //callback(null, event.meta_data);
    if (err) console.log('JSON Pass Fail');  // an error occurred
    else console.log(event.order_key); // successful response
};
</code></pre>

<p>If anyone did want to have a go at re-creating this, you can knock up a quick Wordpress install on cPanel and install the Woocommerce Plugin. Setup a dummy product and then just use the Cash on Delivery as a payment method to get your ""Order Created"" webhooks firing. Only takes 2 minutes.</p>

<p>You can use <a href=""https://requestbin.com/"" rel=""nofollow noreferrer"">https://requestbin.com/</a> or webhook.site to test out webhook outputs.</p>

<p>Can anyone help sorting out the Headers so i can pass a Woocommerce payload to API Gateway?</p>
",9289798.0,,441757.0,,2019-03-20 12:29:26,2019-03-20 12:29:26,Woocommerce webhook to AWS API-Gateway : CORS/Header issue,<amazon-web-services><woocommerce><aws-lambda><aws-api-gateway>,0,6,0.0,,,CC BY-SA 4.0,I am trying to send a Woocommerce webhook to AWS API Gateway  When i put in my API Gateway URL on Amazon I get the following error:  I think this is related to headers  there is an option to Create CORS in API-Gateway that I have now done  Which then adds an OPTION method but I still get  in Cloudwatch  I created a POST method and used the mapping template below with application/json and the setting    Here is a mini Lambda Node function that just outputs a Woocommerce order number to the console and shows in Cloudwatch whether the API passthrough is working   If anyone did want to have a go at re-creating this  you can knock up a quick Wordpress install on cPanel and install the Woocommerce Plugin  Setup a dummy product and then just use the Cash on Delivery as a payment method to get your Order Created webhooks firing  Only takes 2 minutes  You can use  or webhook site to test out webhook outputs  Can anyone help sorting out the Headers so i can pass a Woocommerce payload to API Gateway  
55412624,1,55426800.0,,2019-03-29 07:42:19,,7,5254,"<p>If my Lambda function written in Python takes 1.8 seconds to initialize (during a cold start) and 400 ms to execute, am I charged for the 400 ms execution time or the entire 2.2 seconds of initialization + execution time?</p>

<p>From X-Ray, I see:</p>

<p><a href=""https://i.stack.imgur.com/T3FSQ.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/T3FSQ.png"" alt=""AWS X-Ray trace""></a></p>

<p>From CloudWatch logs, I see:</p>

<pre><code>Duration: 404.42 ms Billed Duration: 500 ms Memory Size: 448 MB Max Memory Used: 113 MB
</code></pre>

<p>What I understand from this is that I was billed for 500ms of execution time, so does that mean code initialization (e.g. importing stuff) is free?</p>
",1768141.0,,,,,2019-03-31 08:35:41,Does AWS Lambda charge for the time spent initializing code?,<amazon-web-services><aws-lambda><aws-billing>,5,0,1.0,,,CC BY-SA 4.0,If my Lambda function written in Python takes 1 8 seconds to initialize (during a cold start) and 400 ms to execute  am I charged for the 400 ms execution time or the entire 2 2 seconds of initialization + execution time  From X-Ray  I see:  From CloudWatch logs  I see:  What I understand from this is that I was billed for 500ms of execution time  so does that mean code initialization (e g  importing stuff) is free  
55716767,1,55719996.0,,2019-04-16 21:19:51,,6,483,"<p>when I make an unauthenticated (public) Cloud Run endpoint to host an API, what are my options to protect this endpoint from malicious users making billions of HTTP requests? </p>

<p>For $10 you can launch a Layer 7 <a href=""https://hackernoon.com/we-survived-a-ddos-attack-that-peaked-at-250k-requests-per-second-it-cost-us-10-b0bdfe028dd2"" rel=""nofollow noreferrer"">HTTP flood attack</a> that can send 250k requests per second. Let's assume your Cloud Run endpoints scale up and all requests are handled. For invocations alone, you will pay $360,-/hour (at $0.40 per million requests). </p>

<p>Note that there is a <a href=""https://cloud.google.com/run/docs/about-concurrency#concurrency_values"" rel=""nofollow noreferrer"">concurrency limit</a> and a <a href=""https://cloud.google.com/run/quotas#cloud_run_limits"" rel=""nofollow noreferrer"">max instance limit</a> that you might hit if the attack is not distributed over multiple Cloud Run endpoints. What other controls do I have? </p>

<p>As I understand, the usual defenses with <a href=""https://cloud.google.com/armor/docs/"" rel=""nofollow noreferrer"">Cloud Armor</a> and <a href=""https://cloud.google.com/cdn/docs/"" rel=""nofollow noreferrer"">Cloud CDN</a> are bound to the <a href=""https://cloud.google.com/load-balancing/docs/load-balancing-overview"" rel=""nofollow noreferrer"">Global Load Balancer</a>, which is unavailable for Cloud Run, but is available for Cloud Run on GKE. </p>
",141427.0,,141427.0,,2019-04-18 09:33:14,2019-05-13 06:00:29,Protect an unauthenticated Cloud Run endpoint,<google-cloud-platform><serverless><google-cloud-run>,1,3,2.0,,,CC BY-SA 4.0,when I make an unauthenticated (public) Cloud Run endpoint to host an API  what are my options to protect this endpoint from malicious users making billions of HTTP requests   For $10 you can launch a Layer 7  that can send 250k requests per second  Lets assume your Cloud Run endpoints scale up and all requests are handled  For invocations alone  you will pay $360 -/hour (at $0 40 per million requests)   Note that there is a  and a  that you might hit if the attack is not distributed over multiple Cloud Run endpoints  What other controls do I have   As I understand  the usual defenses with  and  are bound to the   which is unavailable for Cloud Run  but is available for Cloud Run on GKE   
37971987,1,38024600.0,,2016-06-22 15:10:33,,19,27766,"<p>For example I have lambda functions that consume messages from a KinesisStream. How do stop and resume the function so that I don't incur charges and I don't loose data in the stream.</p>

<p>I know that if the events keep failing, Kinesis will keep retrying and the cost can be very high.</p>

<p>I cannot delete the function because there is lots of automation around it through CloudFormation. Is there a way to stop and restart the function?</p>

<p><strong>SOLUTION</strong>:  <a href=""http://alestic.com/2015/11/aws-lambda-kinesis-pause-resume"" rel=""noreferrer"">http://alestic.com/2015/11/aws-lambda-kinesis-pause-resume</a> </p>

<p><strong>NOTE</strong>: Event sources for rules, log streaming, cannot be disable using the event source. You will not event get it in the list when calling the API using the SDK. For those you have to disable the Event Rule, or the Log Subscription.</p>
",5660156.0,,6382901.0,,2017-07-03 03:39:47,2018-03-04 16:03:52,How to pause / resume a aws lambda function,<amazon-web-services><amazon-dynamodb><aws-lambda><amazon-kinesis>,2,4,4.0,,,CC BY-SA 3.0,For example I have lambda functions that consume messages from a KinesisStream  How do stop and resume the function so that I dont incur charges and I dont loose data in the stream  I know that if the events keep failing  Kinesis will keep retrying and the cost can be very high  I cannot delete the function because there is lots of automation around it through CloudFormation  Is there a way to stop and restart the function  SOLUTION:    NOTE: Event sources for rules  log streaming  cannot be disable using the event source  You will not event get it in the list when calling the API using the SDK  For those you have to disable the Event Rule  or the Log Subscription  
56374391,1,56398545.0,,2019-05-30 08:22:24,,1,413,"<p>I have to make an HTTP request that takes a long time to receive a response.  I don't want AWS Lambda to make this request as I will be charged for the time it is waiting for the response.  Is there any way to use AWS Lambda to handle the response without being charged while waiting?</p>
",192798.0,,,,,2019-05-31 16:14:29,Is it possible to use AWS Lambda to process an HTTP response without making the corresponding HTTP request?,<amazon-web-services><http><aws-lambda>,1,10,,,,CC BY-SA 4.0,I have to make an HTTP request that takes a long time to receive a response   I dont want AWS Lambda to make this request as I will be charged for the time it is waiting for the response   Is there any way to use AWS Lambda to handle the response without being charged while waiting  
56499028,1,,,2019-06-07 17:41:30,,0,782,"<p>For our new startup we have implemented our new marketing site on AWS. </p>

<p>We are using Amazon CloudFront as our CDN, AWS S3 for website hosting and AWS Lambda for our sign up API writing to our AWS RDS database. Cloudfront routes to the Lambda function through API gateway.</p>

<p>The Lambda function operates within the VPC so it can write to Amazon RDS.</p>

<p>We want to implement the server side ReCaptcha V3 check within our Lambda function but as Lambdas within a VPC do not have external internet access, the call to validate with Google times out.</p>

<p>I've read we can overcome this with setting up a NAT gateway but running a gateway continuously is a cost we would like to defer for the moment if we can.</p>

<p>I have thought of the following options:</p>

<ol>
<li><p>Create a non vpc lambda function to validate the ReCaptcha token. Call this from the VPC lambda function. Will this work?</p></li>
<li><p>As we are using CloudFront, create a Lambda@Edge function to validate the ReCaptcha token. If the validation passes, route to the VPC lambda function as normal. If the validation fails, return a response directly error to the client.</p></li>
<li><p>Use SES. Ideally we wanted a synchronous response (i.e. any errors are responded to the client). This would be a bit more work but my only query would be can you write to a SES queue from a non VPC lambda function?</p></li>
</ol>

<p>Any pointers or any other options would be much appreciated.</p>
",1839104.0,,1839104.0,,2019-06-07 18:37:42,2019-06-07 18:37:42,Implementing ReCaptcha V3 with AWS Lambda within VPC,<aws-lambda><recaptcha>,0,4,,,,CC BY-SA 4.0,For our new startup we have implemented our new marketing site on AWS   We are using Amazon CloudFront as our CDN  AWS S3 for website hosting and AWS Lambda for our sign up API writing to our AWS RDS database  Cloudfront routes to the Lambda function through API gateway  The Lambda function operates within the VPC so it can write to Amazon RDS  We want to implement the server side ReCaptcha V3 check within our Lambda function but as Lambdas within a VPC do not have external internet access  the call to validate with Google times out  Ive read we can overcome this with setting up a NAT gateway but running a gateway continuously is a cost we would like to defer for the moment if we can  I have thought of the following options:  Create a non vpc lambda function to validate the ReCaptcha token  Call this from the VPC lambda function  Will this work  As we are using CloudFront  create a Lambda@Edge function to validate the ReCaptcha token  If the validation passes  route to the VPC lambda function as normal  If the validation fails  return a response directly error to the client  Use SES  Ideally we wanted a synchronous response (i e  any errors are responded to the client)  This would be a bit more work but my only query would be can you write to a SES queue from a non VPC lambda function   Any pointers or any other options would be much appreciated  
38673310,1,38674436.0,,2016-07-30 11:16:07,,2,2511,"<p>I have an <code>AWS Lambda</code> function that makes use of an <code>ElastiCache Redis</code> cluster.
Since the <code>Redis</code> cluster is ""locked"" in a <code>VPC</code>, the <code>Lambda</code> function must reside in that <code>VPC</code> too.</p>

<p>For some reason, if the <code>Lambda</code> is allocated an <code>IP</code> of a <code>public subnet</code>, which has an <code>Internet gateway</code> - it still cannot make connections to the outside (the internet), thus making it impossible to use <code>Kinesis</code>.</p>

<p>For that, they suggest using a <code>NAT</code> gateway which lets the <code>Lambda</code> connect to the outside.</p>

<p>Basically, this works for me - but my issue is the money.
This solution is expensive for large amount of data transfers and I'm looking for some way to make it cheaper.</p>

<p>For a small <code>POC</code> that I've made, I paid <code>~$10</code>.
<a href=""https://i.stack.imgur.com/YErGa.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YErGa.png"" alt=""enter image description here""></a></p>

<p>This is too much for <code>~30GB</code> as my production pipeline will run hundreds of <code>gigabytes</code> / month.</p>

<p>How do you suggest I let the <code>Lambda</code> function connect the outside (specifically <code>Kinesis</code>) without using a <code>NAT</code> gateway?</p>

<p>Thank you!</p>
",3454745.0,,174777.0,,2018-06-01 11:24:15,2018-06-01 11:24:15,AWS Lambda - use Kinesis under VPC,<amazon-web-services><aws-lambda><nat><amazon-vpc>,2,4,,,,CC BY-SA 3.0,I have an  function that makes use of an  cluster  Since the  cluster is locked in a   the  function must reside in that  too  For some reason  if the  is allocated an  of a   which has an  - it still cannot make connections to the outside (the internet)  thus making it impossible to use   For that  they suggest using a  gateway which lets the  connect to the outside  Basically  this works for me - but my issue is the money  This solution is expensive for large amount of data transfers and Im looking for some way to make it cheaper  For a small  that Ive made  I paid    This is too much for  as my production pipeline will run hundreds of  / month  How do you suggest I let the  function connect the outside (specifically ) without using a  gateway  Thank you  
56771017,1,,,2019-06-26 10:46:30,,0,30,"<p>I am trying to upload large files (100+ Gb) to several cloud storage services from a Django app. I have written a first view to do this for S3 and it is working: </p>

<pre><code>class FilePolicyAPI(APIView):
...
</code></pre>

<p>I have written these views for S3 multipart upload, my question is, should I write similar views for the other cloud services (Google Cloud Storage, Azure), or would it be best to implement a S3 lambda function that responds to the upload to the S3 bucket and replicates these files in the other cloud services? What would be less costly for my Web App? thanks </p>
",1525838.0,,,,,2019-06-26 10:46:30,Best sequence when uploading large files to different cloud services,<django><amazon-s3><google-cloud-platform><aws-lambda>,0,3,,,,CC BY-SA 4.0,I am trying to upload large files (100+ Gb) to several cloud storage services from a Django app  I have written a first view to do this for S3 and it is working:   I have written these views for S3 multipart upload  my question is  should I write similar views for the other cloud services (Google Cloud Storage  Azure)  or would it be best to implement a S3 lambda function that responds to the upload to the S3 bucket and replicates these files in the other cloud services  What would be less costly for my Web App  thanks  
56772299,1,,,2019-06-26 11:59:23,,6,3068,"<p>We have been using AWS S3 notifications to trigger lambda functions when files land on S3 and this model has worked reasonably well until we noticed that some files are processed multiple times, generating duplicates in our datastore.
We noticed that it happened for about 0.05% of our files.</p>

<p>I know can guard against this by performing an upsert, but what is of concern to us is the potential cost of running unnecessary lambda functions, as this impacts our cost.</p>

<p>I've searched Google and SO, but only found similar-ish issues. We are not having a timeout problem, as the files have been processed fully. Our files are rather small, with the biggest file being less than 400k. We are not receiving the same event twice, as the events have different request ids, even though they are running on the same file.</p>
",2002024.0,,,,,2021-09-17 16:26:47,S3 notification creates multiple events,<amazon-web-services><amazon-s3><aws-lambda>,3,3,,,,CC BY-SA 4.0,We have been using AWS S3 notifications to trigger lambda functions when files land on S3 and this model has worked reasonably well until we noticed that some files are processed multiple times  generating duplicates in our datastore  We noticed that it happened for about 0 05% of our files  I know can guard against this by performing an upsert  but what is of concern to us is the potential cost of running unnecessary lambda functions  as this impacts our cost  Ive searched Google and SO  but only found similar-ish issues  We are not having a timeout problem  as the files have been processed fully  Our files are rather small  with the biggest file being less than 400k  We are not receiving the same event twice  as the events have different request ids  even though they are running on the same file  
38982603,1,,,2016-08-16 19:03:36,,3,1089,"<p>What is a good way to deploy a WebSockets client on AWS?</p>

<p>I'm building an app on AWS which needs to subscribe to several WebSockets and several REST sources and process incoming messages (WebSockets) or make periodic requests (REST). I'm trying to go server-less and maximize use of AWS platform services, to eliminate the need to manage VMs, OS patches, etc. (and hopefully reduce cost).</p>

<p>My idea so far is to trigger a Lambda function every time a message arrives. The function can then transform/normalize the message and push it to an SQS queue for further processing by other subsystems.</p>

<p>There would be two types of such Lambda clients, one that subscribes to WebSockets messages and another that makes HTTP request periodically when invoked by a CloudWatch schedule. It would look something like this:</p>

<p><a href=""https://i.stack.imgur.com/MNQgh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MNQgh.png"" alt=""http://i.imgur.com/8M7YGif.png""></a></p>

<p>This approach seems reasonable for my REST clients, but I haven't been able to determine if it's possible to subscribe to WebSockets messages using Lambda. Lambdas can be triggered by IoT, and apparently IoT supports WebSockets now, but apparently only as a transport for the MQTT protocol:</p>

<p><a href=""https://aws.amazon.com/about-aws/whats-new/2016/01/aws-iot-now-supports-websockets-custom-keepalive-intervals-and-enhanced-console/"" rel=""nofollow noreferrer"">AWS IoT Now Supports WebSockets, Custom Keepalive Intervals, and Enhanced Console</a></p>

<p>What is the best/easiest/cheapest way to deploy a WebSockets client without deploying an entire EC2 or Docker instance?</p>
",3498454.0,,3498454.0,,2016-12-04 15:45:41,2016-12-04 15:45:41,What is the simplest way to process WebSockets messages on AWS?,<amazon-web-services><aws-lambda><aws-iot>,0,2,,,,CC BY-SA 3.0,What is a good way to deploy a WebSockets client on AWS  Im building an app on AWS which needs to subscribe to several WebSockets and several REST sources and process incoming messages (WebSockets) or make periodic requests (REST)  Im trying to go server-less and maximize use of AWS platform services  to eliminate the need to manage VMs  OS patches  etc  (and hopefully reduce cost)  My idea so far is to trigger a Lambda function every time a message arrives  The function can then transform/normalize the message and push it to an SQS queue for further processing by other subsystems  There would be two types of such Lambda clients  one that subscribes to WebSockets messages and another that makes HTTP request periodically when invoked by a CloudWatch schedule  It would look something like this:  This approach seems reasonable for my REST clients  but I havent been able to determine if its possible to subscribe to WebSockets messages using Lambda  Lambdas can be triggered by IoT  and apparently IoT supports WebSockets now  but apparently only as a transport for the MQTT protocol:  What is the best/easiest/cheapest way to deploy a WebSockets client without deploying an entire EC2 or Docker instance  
54137776,1,,,2019-01-10 22:25:28,,0,1794,"<p>I have data in Google Cloud Storage that I need to transfer to s3 bucket in a serverless fashion...
one possible approach is to use cloud function and transmit data from cloud storage to s3 bucket using gsutil and boto3 for was credentials. I believe their is an extra fee from Google for outbound network request but this approach is possible. </p>

<p>Does anyone has a better approach or a suggestion?</p>
",9790313.0,,7757976.0,,2019-02-04 09:38:16,2019-08-01 18:45:18,Moving data from google cloud storage to Amazon s3 via cloud function in a serverless fashion,<python><amazon-s3><google-cloud-storage><google-cloud-functions><serverless>,1,2,,,,CC BY-SA 4.0,I have data in Google Cloud Storage that I need to transfer to s3 bucket in a serverless fashion    one possible approach is to use cloud function and transmit data from cloud storage to s3 bucket using gsutil and boto3 for was credentials  I believe their is an extra fee from Google for outbound network request but this approach is possible   Does anyone has a better approach or a suggestion  
54242090,1,54242411.0,,2019-01-17 18:25:21,,0,453,"<p>I have a CodePipeline build running in AWS, and everything works great, except there's no good notification mechanism directly from within CodePipeline.  Digging around, it seems that the accepted solution for this is to configure CloudWatch to call Lambda or SNS to send off your message.</p>

<p>OK, so I built a small Java program to send a message to Slack based on CloudWatch CodePipeline events.  It works well, except that <em>every single message</em> is repeated a half dozen times or more.  This seems different from all the other posts I've read around duplicated Lambda executions where you get an occasional duplicate here or there.</p>

<p>I know the standard answer is that Lambda wants to ensure delivery, so events may be retried, which is fine - if every 20 or 100 messages I got a duplicate, I'd be fine.  But I can't flood a Slack channel with 50+ messages for every simple CP run.  Not to mention I assume I'm getting billed for every one of those Lambda executions, when really I should only be getting 4 per run.</p>

<p>I don't want to have to set up a DB to track unique IDs - again that's adding both complexity and cost (the executions still happen).  It feels like something is mis-configured.  E.g. Is there a reason CloudWatch would be picking up the same message and forwarding it to Lambda for some reason?  Or a reason that Lambda would think the execution needs to be retried even though it exits successfully?</p>
",10525932.0,,303598.0,,2019-01-17 20:59:02,2019-01-17 20:59:02,Does AWS Lambda duplicate every message?,<amazon-web-services><aws-lambda><aws-codepipeline><amazon-cloudwatch>,1,0,,,,CC BY-SA 4.0,I have a CodePipeline build running in AWS  and everything works great  except theres no good notification mechanism directly from within CodePipeline   Digging around  it seems that the accepted solution for this is to configure CloudWatch to call Lambda or SNS to send off your message  OK  so I built a small Java program to send a message to Slack based on CloudWatch CodePipeline events   It works well  except that every single message is repeated a half dozen times or more   This seems different from all the other posts Ive read around duplicated Lambda executions where you get an occasional duplicate here or there  I know the standard answer is that Lambda wants to ensure delivery  so events may be retried  which is fine - if every 20 or 100 messages I got a duplicate  Id be fine   But I cant flood a Slack channel with 50+ messages for every simple CP run   Not to mention I assume Im getting billed for every one of those Lambda executions  when really I should only be getting 4 per run  I dont want to have to set up a DB to track unique IDs - again thats adding both complexity and cost (the executions still happen)   It feels like something is mis-configured   E g  Is there a reason CloudWatch would be picking up the same message and forwarding it to Lambda for some reason   Or a reason that Lambda would think the execution needs to be retried even though it exits successfully  
54294795,1,,,2019-01-21 17:11:31,,3,1393,"<p>Is there a way to make a python aws-lambda on a local machine which can read and write data from an S3 bucket. I can get this to run on a lambda in AWS's web-page with the following code with no problems.</p>

<pre><code>import json

def lambda_handler(event, context):
    # TODO implement
    now = datetime.datetime.now()
    cur_day = ""{}{}{}"".format(now.strftime('%d'),now.strftime('%m'),now.year)
    print(cur_day)
    my_contents = get_data_from_s3_file('myBucket', 'myFile')
    return {
        'statusCode': 200,
        'body': json.dumps('Hello from Lambda!')
    }

def get_data_from_s3_file(bucket, my_file):
    """"""Read the contents of the file as a string and split by lines""""""
    my_data = s3.get_object(Bucket=bucket, Key=my_file)
    my_text = my_data['Body'].read().decode('utf-8').split('\n')

    return my_text
</code></pre>

<p>The issue is that this is a terrible environment to write and debug code so I would like to do it on my own local machine. I have set up AWS-CLI and installed an app that lets you run lambda code in a local environment called 'python-lambda-local', as shown <a href=""https://stackoverflow.com/questions/53493767/how-to-setup-pycharm-to-develop-aws-lambda-function-on-local-machine"">here</a>.</p>

<pre><code>pip install python-lambda-local
python-lambda-local -l lib/ -f lambda_handler -t 5 pythonLambdaLocalTest.py event.json
</code></pre>

<p>The file 'pythonLambdaLocalTest.py' contains the same code that I ran on AWS from the console - the advantage here is that I can use an IDE such as visual studio code to write it. If I run it without calling 'get_data_from_s3_file' then the code seems to run fine on the local machine and 'cur_day' is printed to the console. However, if I run the full script and try to connect to the bucket then I get the following error:</p>

<pre><code>raise EndpointConnectionError(endpoint_url=request.url, error=e)
botocore.exceptions.EndpointConnectionError: Could not connect to the 
endpoint URL: ""https://myBucket/myfile""
</code></pre>

<p>Does anyone have a method to connect to the s3 from the local machine? I'm sure there must be a way to use aws-cli? or the serverless-application-model (sam)? However I can't find any guides which are complete enough to follow.</p>

<p>I've also tried downloading the .yaml file from the console and putting it in the local directory and running:</p>

<pre><code>sam local start-api -t pythonLambdaLocalTest.yaml
</code></pre>

<p>and I get:</p>

<pre><code>2019-01-21 16:56:30 Found credentials in shared credentials file: ~/.aws/credentials
Error: Template does not have any APIs connected to Lambda functions
</code></pre>

<p>This suggests that potentially an api could connect my local machine to the aws s3 bucket but I have very little experience in setting this kind of thing up and am struggling with the jargon. Any help on getting this approach to run would be great? I've recently started using docker so some approach using this would also be great?</p>

<p>I've also tried this approach <a href=""https://github.com/aws/aws-toolkit-vscode"" rel=""nofollow noreferrer"">here</a> and can see my lambda functions listed in visual studio code but I can't seem to see or edit any of the code and there is no obvious link to do so - most of the support seems to be around node.js and my lambda's are python.</p>

<p>I also realise that cloud9 is an option but appears to require a running EC2 instance which I would rather not pay for. </p>

<p>I have tried a lot of approaches but there doesn't seem to be any complete guides. Any help highly appreciated!</p>
",3062260.0,,,,,2019-01-21 17:11:31,how to develop and test a python AWS-lambda on local machine which is dependent on an S3 bucket,<python><docker><amazon-s3><aws-lambda><aws-cli>,0,0,1.0,,,CC BY-SA 4.0,Is there a way to make a python aws-lambda on a local machine which can read and write data from an S3 bucket  I can get this to run on a lambda in AWSs web-page with the following code with no problems   The issue is that this is a terrible environment to write and debug code so I would like to do it on my own local machine  I have set up AWS-CLI and installed an app that lets you run lambda code in a local environment called python-lambda-local  as shown    The file pythonLambdaLocalTest py contains the same code that I ran on AWS from the console - the advantage here is that I can use an IDE such as visual studio code to write it  If I run it without calling get_data_from_s3_file then the code seems to run fine on the local machine and cur_day is printed to the console  However  if I run the full script and try to connect to the bucket then I get the following error:  Does anyone have a method to connect to the s3 from the local machine  Im sure there must be a way to use aws-cli  or the serverless-application-model (sam)  However I cant find any guides which are complete enough to follow  Ive also tried downloading the  yaml file from the console and putting it in the local directory and running:  and I get:  This suggests that potentially an api could connect my local machine to the aws s3 bucket but I have very little experience in setting this kind of thing up and am struggling with the jargon  Any help on getting this approach to run would be great  Ive recently started using docker so some approach using this would also be great  Ive also tried this approach  and can see my lambda functions listed in visual studio code but I cant seem to see or edit any of the code and there is no obvious link to do so - most of the support seems to be around node js and my lambdas are python  I also realise that cloud9 is an option but appears to require a running EC2 instance which I would rather not pay for   I have tried a lot of approaches but there doesnt seem to be any complete guides  Any help highly appreciated  
36268775,1,36269213.0,,2016-03-28 18:39:51,,2,1943,"<p>I've been tinkering with nodejs code in AWS Lambda, called by some API Gateway endpoints. I've noticed that after a certain amount of time passes without any API Gateway calls, the next API Gateway request will time out. I'll get the standard Lambda error message saying the function timed out. However, subsequent HTTP requests to trigger my Lambda work fine. </p>

<p>Superficially, it looks like something is going into ""idle"" mode and needs to be charged up before the API Gateway-Lambda request can work properly. I've considered setting up a wget cron to keep things non-idle, but is there a real fix and how can I better understand what's happening? </p>
",1807888.0,,,,,2016-03-28 19:03:47,"AWS Lambda and API Gateway - goes idle; needs to ""wake up""/no response on first request?",<amazon-web-services><aws-lambda><aws-api-gateway>,1,2,1.0,,,CC BY-SA 3.0,Ive been tinkering with nodejs code in AWS Lambda  called by some API Gateway endpoints  Ive noticed that after a certain amount of time passes without any API Gateway calls  the next API Gateway request will time out  Ill get the standard Lambda error message saying the function timed out  However  subsequent HTTP requests to trigger my Lambda work fine   Superficially  it looks like something is going into idle mode and needs to be charged up before the API Gateway-Lambda request can work properly  Ive considered setting up a wget cron to keep things non-idle  but is there a real fix and how can I better understand whats happening   
54557325,1,,,2019-02-06 15:40:19,,0,609,"<p>I have a little bit of a problem concerning the design of a planned application, especially database engine and Serverless/not serverless.
The goal is a Web Application which talks via the Rest API to the database. The Rest API itself is really just CRUD operations, so for that the Serverless aproach (AWS Lambda) would fit pretty good in my opinion. For that, the probably most <strong>efficient</strong> database to choose would be DynamoDB (NoSQL). </p>

<p>I am familliar with RDBMS and have only little knowledge of NoSQL databases.</p>

<p>The Schema of the application is not yet finished and should be expandable at later points, because there could be new features to implement and so on. Because of this, i would rather use a RDBMS and not a NoSQL database, because they don't scale that well in terms of editing the schema at later points. (at least that's what i read the last couple of hours)</p>

<p>Choosing for example Amazon RDS MySQL database, would be much more expensive and i don't know how well they do with the Serverless aproach of the Rest API. </p>

<p>So i am standing at a point i really don't know what services to use here. Could i still use DynamoDB? The schema would propably be very relational. </p>
",3475915.0,,,,,2019-02-06 20:48:43,RDBMS vs. DynamoDB and Serverless vs. not Serverless,<mysql><amazon-web-services><aws-lambda><amazon-dynamodb><serverless-framework>,1,0,,,,CC BY-SA 4.0,I have a little bit of a problem concerning the design of a planned application  especially database engine and Serverless/not serverless  The goal is a Web Application which talks via the Rest API to the database  The Rest API itself is really just CRUD operations  so for that the Serverless aproach (AWS Lambda) would fit pretty good in my opinion  For that  the probably most efficient database to choose would be DynamoDB (NoSQL)   I am familliar with RDBMS and have only little knowledge of NoSQL databases  The Schema of the application is not yet finished and should be expandable at later points  because there could be new features to implement and so on  Because of this  i would rather use a RDBMS and not a NoSQL database  because they dont scale that well in terms of editing the schema at later points  (at least thats what i read the last couple of hours) Choosing for example Amazon RDS MySQL database  would be much more expensive and i dont know how well they do with the Serverless aproach of the Rest API   So i am standing at a point i really dont know what services to use here  Could i still use DynamoDB  The schema would propably be very relational   
54562914,1,,,2019-02-06 21:36:56,,0,209,"<p>I'm trying to take advantage of db connection reuse in Lambda, by keeping the code outside of the handler. </p>

<p>For example - something like: </p>

<pre><code>import dbconnection from './connection'

const handler(event, context, callback){
    //use dbconnection 
}
</code></pre>

<p>The issue is I don't decide what database to connect to until I do a lookup to see where they should be connecting. In my specific case I have <strong>'customer=foo'</strong> in a query param then I can look to see that foo should connect to <strong>database1</strong>. </p>

<p>So what I need to do is something like this : </p>

<pre><code>    const dbconnection = require('./connection)('database1') 
</code></pre>

<p>The way it is now I need to do this in every handler method which is expensive. </p>

<p>Is there some way I can pull the query parameter, look up my database and set it / switch it globally within the Lambda execution context? </p>

<p>I've tried this: </p>

<pre><code>import dbconnection from './connection'

const handler(event, context, callback){
    const client = dbconnection.setDatabase('database1') 
}

....
./connection.js

 setDatabase(database) {
      if(this.currentDatabase !== database) {
         // connect to different database
         this.currentDatabase = database; 
      }
 }
</code></pre>

<p>Everything works locally with <code>sls offline</code> but doesn't work through the AWS Lambda execution context. Thoughts? </p>
",257535.0,,174777.0,,2019-02-06 22:25:43,2019-02-06 22:25:43,AWS Lambda Dynamic DB Switching Singelton (Node),<node.js><amazon-web-services><aws-lambda><singleton>,1,0,,,,CC BY-SA 4.0,Im trying to take advantage of db connection reuse in Lambda  by keeping the code outside of the handler   For example - something like:   The issue is I dont decide what database to connect to until I do a lookup to see where they should be connecting  In my specific case I have customer=foo in a query param then I can look to see that foo should connect to database1   So what I need to do is something like this :   The way it is now I need to do this in every handler method which is expensive   Is there some way I can pull the query parameter  look up my database and set it / switch it globally within the Lambda execution context   Ive tried this:   Everything works locally with  but doesnt work through the AWS Lambda execution context  Thoughts   
54883254,1,,,2019-02-26 10:23:02,,1,498,"<p>So I have already got as far as the below, I am trying to make the lambda function apply to a specific VPC (all instances within that VPC). There is a costed way of doing it within AWS but getting customers to pay the extra $2.20 per instance for detailed monitoring is beyond difficult to justify. Based on what I've read so far it is ridiculously easy to write the function per instance, again it can be done almost by clicking next next finish within AWS, but we don't want to have to define the individual instances based on how our clients grow so rapidly. </p>

<pre><code>import boto3


def put_cpu_alarm(instance_id): 
    cloudWatch = boto3.client('cloudwatch') 
    cloudWatch.put_metric_alarm( 
    AlarmName = f'CPU_ALARM_{instance_id}'
    AlarmDescription = 'Alarm when server CPU does not exceed 10%'
    AlarmActions = ['arn:aws:automate:eu-west-1:ec2:stop'] 
    MetricName = 'CPUUtilization'
    Namespace = 'AWS/EC2' 
    Statistic = 'Average'
    Dimensions = [{'Name': 'InstanceId', 'Value': instance_id}]
    Period = 300
    EvaluationPeriods = 12
    Threshold = 10
    ComparisonOperator = 'LessThanOrEqualToThreshold'
    TreatMissingData = 'notBreaching' 
    )

def lambda_handler(event, context): 
    instance_id = event['detail']['instance-id'] 
    ec2 = boto3.resource('ec2') 
    instance = ec2.Instance(instance_id)  
    if instance.instance_type.endswith('xlarge'): put_cpu_alarm(instance_id)
</code></pre>

<p>I was hoping the small if statement in the last line could possibly be updated to run against the specific VPC, but haven't had much look finding the correct line to reference a single VPC, or security group that may help.</p>

<p>Any advice massively appreciated.</p>
",11118631.0,,401096.0,,2019-02-26 20:08:40,2019-02-26 20:08:40,Create an AWS lambda function to shut down instances based on low CPUUtilisation,<python-3.x><amazon-web-services><aws-lambda>,1,2,,,,CC BY-SA 4.0,So I have already got as far as the below  I am trying to make the lambda function apply to a specific VPC (all instances within that VPC)  There is a costed way of doing it within AWS but getting customers to pay the extra $2 20 per instance for detailed monitoring is beyond difficult to justify  Based on what Ive read so far it is ridiculously easy to write the function per instance  again it can be done almost by clicking next next finish within AWS  but we dont want to have to define the individual instances based on how our clients grow so rapidly    I was hoping the small if statement in the last line could possibly be updated to run against the specific VPC  but havent had much look finding the correct line to reference a single VPC  or security group that may help  Any advice massively appreciated  
58805577,1,,,2019-11-11 16:54:23,,1,450,"<p>I am trying to come up with a solution for aws cost optimization. I am able to do it with cloudwatch rule and Lambda, this approach is working fine if I have to directly stop the instance and no auto-scaling group is associated with that instance.
Problem is when we are ,managing the instances using auto-scaling group. I am able to scale up the only instance associated with one auto-scaling group at one time using lambda. My use case is that I have to scale up multiple instances associated with multiple auto-scaling groups using lambda.</p>

<h2>There is one method which aws provide to update the autoscaling configuration as below:</h2>

<pre><code>var params = {
  AutoScalingGroupName: ""my-auto-scaling-group"", 
  MaxSize: 3, 
  MinSize: 1
 }
 autoscaling.updateAutoScalingGroup(params, function(err, data) {
   if (err) console.log(err, err.stack); // an error occurred
   else     console.log(data);           // successful response
 })
</code></pre>

<hr>

<p>But this way I can only update the configurations of 1 auto-scaling group at one time. Is there any way we can do it for multiple instances associated with multiple auto-scaling groups?</p>
",11618266.0,,,,,2020-04-23 07:19:02,How to update aws auto-scaling group min & max configurations using lambda,<amazon-web-services><amazon-ec2><aws-lambda><amazon-cloudwatch>,1,0,,,,CC BY-SA 4.0,I am trying to come up with a solution for aws cost optimization  I am able to do it with cloudwatch rule and Lambda  this approach is working fine if I have to directly stop the instance and no auto-scaling group is associated with that instance  Problem is when we are  managing the instances using auto-scaling group  I am able to scale up the only instance associated with one auto-scaling group at one time using lambda  My use case is that I have to scale up multiple instances associated with multiple auto-scaling groups using lambda  There is one method which aws provide to update the autoscaling configuration as below:   But this way I can only update the configurations of 1 auto-scaling group at one time  Is there any way we can do it for multiple instances associated with multiple auto-scaling groups  
59057565,1,,,2019-11-26 18:46:55,,0,161,"<p>I'd like to have a lambda expression that every hour makes a query on RDS database, pull some ARN (device tokens) and then sends these devices a notification via SNS. My desire is to remain inside the VPC and I'd like to avoid using NAT due to its cost. Should i create a VPC endpoint (is this called AWS PrivateLink?) that can reach out SNS+RDS? Is NAT and Endpoint similar in billing? Globally is this the right way to achieve a ""cron sending notifications"" on AWS? </p>

<p>RDS is reachable inside the VPC without the endpoint isn't it?</p>
",2373113.0,,8239061.0,,2019-11-26 19:17:16,2019-11-26 23:55:52,Using Lambda inside VPC with SNS+RDS,<amazon-web-services><aws-lambda><amazon-rds><amazon-sns><amazon-vpc>,1,0,,,,CC BY-SA 4.0,Id like to have a lambda expression that every hour makes a query on RDS database  pull some ARN (device tokens) and then sends these devices a notification via SNS  My desire is to remain inside the VPC and Id like to avoid using NAT due to its cost  Should i create a VPC endpoint (is this called AWS PrivateLink ) that can reach out SNS+RDS  Is NAT and Endpoint similar in billing  Globally is this the right way to achieve a cron sending notifications on AWS   RDS is reachable inside the VPC without the endpoint isnt it  
59306187,1,,,2019-12-12 13:54:01,,0,186,"<p>I noticed that we are racking up quite a bit of data transfer costs from RDS. I would like to understand what is causing it and if our networking architecture is flawed. </p>

<p>So here's the architecture:</p>

<ol>
<li>Aurora cluster running on (Availability Zone) AZ-c , belonging to 3 public subnets (1 for each AZ)</li>
<li>Lambda in 3 private subnets (1 for each AZ)</li>
<li>Lambda is allowed to communicate with the RDS instance through a security group</li>
</ol>

<p>I think the only reason why the RDS instance is in a public subnet is to avoid using a proxy to connect to it from the office (we have an SG allowing access from the lambda and our office IP).</p>

<p>Should I try running the lambda only from 1 subnet, that is in the same AZ as the RDS cluster?</p>
",3608449.0,,,,,2019-12-12 13:54:01,AWS Lambda to Aurora RDS data transfer costs and networking architecture,<amazon-web-services><aws-lambda><amazon-rds><amazon-vpc>,0,6,,,,CC BY-SA 4.0,I noticed that we are racking up quite a bit of data transfer costs from RDS  I would like to understand what is causing it and if our networking architecture is flawed   So heres the architecture:  Aurora cluster running on (Availability Zone) AZ-c   belonging to 3 public subnets (1 for each AZ) Lambda in 3 private subnets (1 for each AZ) Lambda is allowed to communicate with the RDS instance through a security group  I think the only reason why the RDS instance is in a public subnet is to avoid using a proxy to connect to it from the office (we have an SG allowing access from the lambda and our office IP)  Should I try running the lambda only from 1 subnet  that is in the same AZ as the RDS cluster  
54357901,1,54358106.0,,2019-01-25 01:44:52,,0,4088,"<p>We have a number of (micro) services, some as simple as a single lambda function, and some are full RDS apps.  One thing that they all need to do is access an foreign ID key mapping.  That is, they all are passed a certain type of ID, but need to included the ""name"" field associated with that ID in their responses.  Basically, just a key/value store.</p>

<p>I could build a separate microservice to manage these mappings (and maybe still will), but that also needs the simple database.</p>

<p>My services don't usually get a huge amount of throughput, but I need them to be performant.</p>

<p>I'd like to keep it cheap, and I'd like it to be as low maintenance as possible.</p>

<p>Basically, I'd like something ""Serverless"" (i.e. doesn't require an ongoing EC2 instance or RDS instance running), fast, and straightforward to access.</p>

<p>I've thought about just having each ""key"" be an S3 key, with the value being the object, but that's not super performant when I need to access a bunch at a time. (should I look into redshift spectrum? is that way overkill?  does it matter if is?)</p>

<p>I know AWS used to offer simpledb, which is probably basically what I'm after.  So what would be closest now?</p>

<p>Thanks for your advice!</p>
",603871.0,,,,,2019-01-25 02:17:15,What's the best AWS service to use for a simple database (essentially just a key/value store) shared by a number of different services?,<database><amazon-web-services><amazon-s3><serverless><aws-serverless>,2,1,,2019-01-25 17:04:47,,CC BY-SA 4.0,We have a number of (micro) services  some as simple as a single lambda function  and some are full RDS apps   One thing that they all need to do is access an foreign ID key mapping   That is  they all are passed a certain type of ID  but need to included the name field associated with that ID in their responses   Basically  just a key/value store  I could build a separate microservice to manage these mappings (and maybe still will)  but that also needs the simple database  My services dont usually get a huge amount of throughput  but I need them to be performant  Id like to keep it cheap  and Id like it to be as low maintenance as possible  Basically  Id like something Serverless (i e  doesnt require an ongoing EC2 instance or RDS instance running)  fast  and straightforward to access  Ive thought about just having each key be an S3 key  with the value being the object  but thats not super performant when I need to access a bunch at a time  (should I look into redshift spectrum  is that way overkill   does it matter if is ) I know AWS used to offer simpledb  which is probably basically what Im after   So what would be closest now  Thanks for your advice  
54459262,1,,,2019-01-31 11:10:41,,1,47,"<p>I have a simple one. Static website available after login - easily build and running on nginx.</p>

<p>But it seems to me cumbersome to server static web with small amount of connections (few per day) and spent a hours to set properly and maintain virtual machine during the lifespan of this project.</p>

<p>I use mostly GCP and consider their load balancer, or buckets made for static content, but none of this is able to give you a chance to auth users with passwords.</p>

<p>Do you have any tip for this? Provider is not concern or some kind of serverless with a pay as go will be perfect in my situation.</p>

<p>Thank you for your time</p>
",9832950.0,,,,,2019-01-31 12:08:28,Run static website with password auth in cloud,<web><nginx><cloud><serverless>,1,0,,,,CC BY-SA 4.0,I have a simple one  Static website available after login - easily build and running on nginx  But it seems to me cumbersome to server static web with small amount of connections (few per day) and spent a hours to set properly and maintain virtual machine during the lifespan of this project  I use mostly GCP and consider their load balancer  or buckets made for static content  but none of this is able to give you a chance to auth users with passwords  Do you have any tip for this  Provider is not concern or some kind of serverless with a pay as go will be perfect in my situation  Thank you for your time 
54901763,1,,,2019-02-27 09:12:24,,3,66,"<p>One of the scary things about being a small fish in the AWS ocean is what seems to be unlimited liability in the unlikely event of a flood of transactions on a publicly exposed service. AWS is designed for the big guys, for whom availability is the be-all and end-all. There's a great deal of effort in keeping services up, but apparently none when it comes to saving us from astronomical bills. Amazon seems to have been repeatedly asked for a spending cap, but AFAIK, the best they've come up with is billing alerts. I've read some horror stories, like the guy who set up a small website for family and friends and got a bill for $10,000. The best Amazon has done is provide billing alerts, and what if you are AFK? Quite large sums could disappear before you can shut down.</p>

<p>So, my cunning plan - to write a Lambda fired from a billing threshold event which raises the drawbridge and shuts down any public facing facilities.</p>

<p>So, I started with CloudFront and that's quite do-able, though disabling a distribution take some time to propagate.</p>

<p>Now we come to my first technical quandary. I thought I could just check all the buckets for public permissions and revoke them. But, since I last looked in detail at S3, there's this static website setting. So, how do I, programmatically, turn this off? Obviously I don't want to delete the bucket. To my surprise turning this on does not create a public permission in the ACL. There's a service in the API to update properties on one of these static websites, but not to revoke it.</p>

<p>It seems to be somewhat the same with the API gateway. You can delete an API, but not suspend it. And if you delete it, then re-building will probably give you a different hostname, which creates some hastle.</p>

<p>I'm trying to write a fairly generic Lambda and make it public domain.</p>
",6473791.0,,,,,2019-03-03 07:38:45,Locking down public site access in a billing panic,<amazon-web-services><amazon-s3><aws-lambda>,1,0,2.0,,,CC BY-SA 4.0,One of the scary things about being a small fish in the AWS ocean is what seems to be unlimited liability in the unlikely event of a flood of transactions on a publicly exposed service  AWS is designed for the big guys  for whom availability is the be-all and end-all  Theres a great deal of effort in keeping services up  but apparently none when it comes to saving us from astronomical bills  Amazon seems to have been repeatedly asked for a spending cap  but AFAIK  the best theyve come up with is billing alerts  Ive read some horror stories  like the guy who set up a small website for family and friends and got a bill for $10 000  The best Amazon has done is provide billing alerts  and what if you are AFK  Quite large sums could disappear before you can shut down  So  my cunning plan - to write a Lambda fired from a billing threshold event which raises the drawbridge and shuts down any public facing facilities  So  I started with CloudFront and thats quite do-able  though disabling a distribution take some time to propagate  Now we come to my first technical quandary  I thought I could just check all the buckets for public permissions and revoke them  But  since I last looked in detail at S3  theres this static website setting  So  how do I  programmatically  turn this off  Obviously I dont want to delete the bucket  To my surprise turning this on does not create a public permission in the ACL  Theres a service in the API to update properties on one of these static websites  but not to revoke it  It seems to be somewhat the same with the API gateway  You can delete an API  but not suspend it  And if you delete it  then re-building will probably give you a different hostname  which creates some hastle  Im trying to write a fairly generic Lambda and make it public domain  
54961279,1,,,2019-03-02 17:45:11,,2,247,"<p>We have a aws lambda function that processes some data and incase there is an error it sends out an email.</p>

<p>We experienced a surge of email from the lambda function so we changed the script and disabled the part where it sends email.   Unfortunately we still see the emails comming in.</p>

<p>So we deleted the function and we still keep receiving the error emails.</p>

<p>How can the lambda function still be running.   Would i be experiencing charges since the function is running.</p>

<p>John</p>
",2871162.0,,174777.0,,2019-03-03 01:03:28,2019-03-03 01:03:28,Deleted Lambda Function Still Executing,<function><amazon-web-services><aws-lambda>,0,2,,,,CC BY-SA 4.0,We have a aws lambda function that processes some data and incase there is an error it sends out an email  We experienced a surge of email from the lambda function so we changed the script and disabled the part where it sends email    Unfortunately we still see the emails comming in  So we deleted the function and we still keep receiving the error emails  How can the lambda function still be running    Would i be experiencing charges since the function is running  John 
55013881,1,55055042.0,,2019-03-06 00:54:34,,1,127,"<p>I extracted ~348 million row from DynamoDB using the data pipeline.  The pipeline completed with no errors.</p>

<p>I noticed the number of files in the S3 bucket is not the same as the number of files indicated in the manifest ""entries"" tag.  Each file contains 100,000 records and so there are 3,479 files.  There are 3,469 files in the ""entries"" tag in the resulting manifest.</p>

<p>Does anyone have any idea why?</p>

<p>The manifest file is required in order to import the files back into DynamoDB.  This is production data and the 10 file discrepancy could cost us 1 million rows.</p>
",3365886.0,,3365886.0,,2019-03-06 01:03:36,2019-03-08 00:45:16,AWS Data Pipeline extracting data from DynamoDB has different number of files than indicated in the manifest file,<amazon-web-services><aws-lambda><amazon-dynamodb>,1,0,,,,CC BY-SA 4.0,I extracted ~348 million row from DynamoDB using the data pipeline   The pipeline completed with no errors  I noticed the number of files in the S3 bucket is not the same as the number of files indicated in the manifest entries tag   Each file contains 100 000 records and so there are 3 479 files   There are 3 469 files in the entries tag in the resulting manifest  Does anyone have any idea why  The manifest file is required in order to import the files back into DynamoDB   This is production data and the 10 file discrepancy could cost us 1 million rows  
55087016,1,55088051.0,,2019-03-10 11:05:29,,3,125,"<p>I've got the order microservice that's written as go AWS lambda function.</p>

<p>The main function named order-service bound to API Gateway. It receives several parameters like <code>user_id:int, product_ids:array</code> of int, creates an order with artifacts and returns serialized order with order_id and total price.
This function invokes a function named order-item which creates an order-item and returns them in parallel(per product). These functions invoke product and user functions to retrieve information about user and products by their ids. 
Then, the order function invokes another lambda called fee-function which takes just total price and user id and gives back fee price. Of course, it calls some other function like user function and so on. Basically, this is a simple example of how the service works in general. Any function calls some others like user-discount, state taxes etc.</p>

<p>The questions are:</p>

<ol>
<li>Is it good that order function invokes fee function through Amazon, but it can just import fee handler package and run it inside itself?(However, fee function may be called from outside so it must be deployed as a separate function as well)</li>
<li>Is it good that each function receives just user id and loads the user invoking user function? Perhaps, better to preload it and pass it through everywhere? Something else?</li>
<li>Is it good that one function calls other functions and they call some others and so on? is there any better approach in my situation? Use SNS, Step function, dependency injections/aws layers.</li>
</ol>

<p>The main reason why I asked is to withstand thousands of RPM and not pay a lot.</p>

<p>Thanks for helping. I appreciate this. </p>
",11179064.0,,174777.0,,2019-03-10 20:06:19,2019-03-10 20:06:19,Organize invoking lambda functions,<amazon-web-services><go><aws-lambda><domain-driven-design><microservices>,1,0,1.0,,,CC BY-SA 4.0,Ive got the order microservice thats written as go AWS lambda function  The main function named order-service bound to API Gateway  It receives several parameters like  of int  creates an order with artifacts and returns serialized order with order_id and total price  This function invokes a function named order-item which creates an order-item and returns them in parallel(per product)  These functions invoke product and user functions to retrieve information about user and products by their ids   Then  the order function invokes another lambda called fee-function which takes just total price and user id and gives back fee price  Of course  it calls some other function like user function and so on  Basically  this is a simple example of how the service works in general  Any function calls some others like user-discount  state taxes etc  The questions are:  Is it good that order function invokes fee function through Amazon  but it can just import fee handler package and run it inside itself (However  fee function may be called from outside so it must be deployed as a separate function as well) Is it good that each function receives just user id and loads the user invoking user function  Perhaps  better to preload it and pass it through everywhere  Something else  Is it good that one function calls other functions and they call some others and so on  is there any better approach in my situation  Use SNS  Step function  dependency injections/aws layers   The main reason why I asked is to withstand thousands of RPM and not pay a lot  Thanks for helping  I appreciate this   
55083541,1,,,2019-03-10 01:06:17,,0,29,"<p>Looking in CloudWatch logs, I see these errors repeated continuously, approximately twice a second:</p>

<pre><code>2019-03-10 00:56:12 2b1be9674700[AWS_OSCAR_ERROR]:Failed to read persisted replica status info. Error code = 11
2019-03-10 00:56:12 2b1be9674700[AWS_OSCAR_ERROR]:Failed to update replication status table (stage 2)
</code></pre>

<p>Does anyone know what these are? I've tried Googling the exact and partial errors and was amazed to see that there's nothing out there (well Google, anyway, has never seen them!)</p>

<p>I am concerned because --even if they are nothing to worry about-- they are generating millions of log entries and thus costing me lots in AWS fees.</p>

<p>Thanks in advance.</p>
",1028855.0,,,,,2019-03-11 19:48:59,AWS Aurora Serverless spewing 2 particular errors continuously,<amazon-rds><amazon-cloudwatch><aws-serverless><amazon-aurora>,1,1,,,,CC BY-SA 4.0,Looking in CloudWatch logs  I see these errors repeated continuously  approximately twice a second:  Does anyone know what these are  Ive tried Googling the exact and partial errors and was amazed to see that theres nothing out there (well Google  anyway  has never seen them ) I am concerned because --even if they are nothing to worry about-- they are generating millions of log entries and thus costing me lots in AWS fees  Thanks in advance  
36962722,1,,,2016-05-01 03:25:31,,3,538,"<p>I'm using Lambda and S3 in conjunction with Amazon's Skills Kit. The Lambda is running Node.js and referencing audio files which I use SSML for playback on Alexa commands. However, since I get charged for GET requests, I'd like to limit the requests to just the servers requesting specifically from Alexa. Although I can set an IAM user to restrict access, I think I'll get an access denied response when trying to play back from Alexa. Can I restrict playback based on a known Alexa IP? It appears that the IAM information isn't passed through regular HTTP requests to S3 GETs. </p>
",,user393219,,,,2016-10-17 14:43:01,Amazon Alexa Skill S3 Restrictions,<amazon-s3><aws-lambda><alexa-skills-kit><alexa-skill>,1,0,,,,CC BY-SA 3.0,Im using Lambda and S3 in conjunction with Amazons Skills Kit  The Lambda is running Node js and referencing audio files which I use SSML for playback on Alexa commands  However  since I get charged for GET requests  Id like to limit the requests to just the servers requesting specifically from Alexa  Although I can set an IAM user to restrict access  I think Ill get an access denied response when trying to play back from Alexa  Can I restrict playback based on a known Alexa IP  It appears that the IAM information isnt passed through regular HTTP requests to S3 GETs   
55216273,1,,,2019-03-18 07:18:54,,1,736,"<p>From my understanding, I can run AWS Lambda outside a VPC, as well as the RDS and have that set to publicly accessible. That would be the perfect solution for me because my lambda functions need internet access and a NAT Gateway which would allow that when inside a VPC is way too expensive. In which cases is it safe to go for that option? When is it a bad idea to have an RDS outside a VPC? What are the risks?</p>
",8083096.0,,,,,2019-04-21 09:28:59,AWS Lambda and RDS outside a VPC,<amazon-web-services><aws-lambda><amazon-rds><aws-vpc>,1,3,,,,CC BY-SA 4.0,From my understanding  I can run AWS Lambda outside a VPC  as well as the RDS and have that set to publicly accessible  That would be the perfect solution for me because my lambda functions need internet access and a NAT Gateway which would allow that when inside a VPC is way too expensive  In which cases is it safe to go for that option  When is it a bad idea to have an RDS outside a VPC  What are the risks  
38757271,1,,,2016-08-04 01:54:29,,5,576,"<p><a href=""https://i.stack.imgur.com/1aK1D.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1aK1D.png"" alt=""Half serverless blog with react front end""></a></p>

<p>I want to host a scalable blog or application of this sort in nodeJS on AWS making use of AWS technologies. The idea here is to have a small EC2 server that is not responsible for serving the website, but only for running the CMS/admin panel. While these operations could be serverless as well, I think having a dedicated small VM EC2 instance could be more efficient, and works better with existing frameworks, etc.</p>

<p>In my diagram above, you can see there's two type of users <code>audiences</code> and <code>admin/writers</code>.  Admin CRUD operations also cause lambda to run. Lambda generates the static site after Admin changes, which is delivered to S3. Users are directed to the static site hosted in S3. Only admins/writers have access to the server-connecting part of the site.</p>

<p>I think this is a good design for an extremely scalable and relatively cheap site, as long as the user-facing side is all static. An alternative to this is a CDN, but then I have to deal with cache invalidation issues, a site that updates slower, and a larger server.</p>

<p>This seems like a win-win to me. Feedback?</p>
",919256.0,,13070.0,,2016-08-04 02:47:26,2016-08-20 12:10:56,I need feedback on this partly serverless architecture design,<amazon-web-services><amazon-s3><amazon-ec2><aws-lambda><static-generator>,1,8,2.0,,,CC BY-SA 3.0, I want to host a scalable blog or application of this sort in nodeJS on AWS making use of AWS technologies  The idea here is to have a small EC2 server that is not responsible for serving the website  but only for running the CMS/admin panel  While these operations could be serverless as well  I think having a dedicated small VM EC2 instance could be more efficient  and works better with existing frameworks  etc  In my diagram above  you can see theres two type of users  and    Admin CRUD operations also cause lambda to run  Lambda generates the static site after Admin changes  which is delivered to S3  Users are directed to the static site hosted in S3  Only admins/writers have access to the server-connecting part of the site  I think this is a good design for an extremely scalable and relatively cheap site  as long as the user-facing side is all static  An alternative to this is a CDN  but then I have to deal with cache invalidation issues  a site that updates slower  and a larger server  This seems like a win-win to me  Feedback  
56704657,1,,,2019-06-21 13:43:18,,1,31,"<p>I'm confused about Firebase and serverless in general, as I was just introduced to this concept recently (note that I'm still studying computer science and I'm just exploring on my own now).</p>

<p>In the past I've been part of a project that had the following structure:</p>

<ul>
<li><p>Front End in a Single-page-app</p></li>
<li><p>Back End built as a REST API</p></li>
</ul>

<p>Now let's say I want to build a product, that might have a website and mobile apps. It also has to have backend logic as there are accounts, objects owned by users, and possible integration with a payment service.</p>

<p>What I initially expected from this, before knowning about serverless, is that you build a backend using something like Go (was my case), where you handle all the database data and third-party integrations, build a front end with something like Vue, and then use the backend's REST API to communicate between both of them.</p>

<p>Is this still the case with serverless? Do you build the whole backend server code, or does it work in another way?</p>

<p>I don't need/want you to explain all of it to me. I just need some insight on what is done and common so I can investigate further.</p>
",11375178.0,,,,,2019-10-03 19:41:35,How to build an app using a serverless architecture?,<rest><backend><serverless-framework><serverless>,0,0,,,,CC BY-SA 4.0,Im confused about Firebase and serverless in general  as I was just introduced to this concept recently (note that Im still studying computer science and Im just exploring on my own now)  In the past Ive been part of a project that had the following structure:  Front End in a Single-page-app Back End built as a REST API  Now lets say I want to build a product  that might have a website and mobile apps  It also has to have backend logic as there are accounts  objects owned by users  and possible integration with a payment service  What I initially expected from this  before knowning about serverless  is that you build a backend using something like Go (was my case)  where you handle all the database data and third-party integrations  build a front end with something like Vue  and then use the backends REST API to communicate between both of them  Is this still the case with serverless  Do you build the whole backend server code  or does it work in another way  I dont need/want you to explain all of it to me  I just need some insight on what is done and common so I can investigate further  
56843940,1,,,2019-07-02 00:33:04,,0,675,"<p>I am creating a MVP(Minimum Viable Product) that has a nodejs server using express for a rest api and a socket.io connection for chat features. </p>

<p>My concern is not so much about cost or scalability, but about setup time/maintenance as this is an MVP. Would serverless or not serverless take less time to setup/maintain on AWS?</p>
",6406073.0,,,,,2020-05-15 19:19:30,Should I use Serverless Computing,<amazon-web-services><aws-lambda><serverless-framework>,3,0,,2020-05-15 20:20:32,,CC BY-SA 4.0,I am creating a MVP(Minimum Viable Product) that has a nodejs server using express for a rest api and a socket io connection for chat features   My concern is not so much about cost or scalability  but about setup time/maintenance as this is an MVP  Would serverless or not serverless take less time to setup/maintain on AWS  
57120893,1,,,2019-07-20 00:31:48,,1,254,"<p>My problem statement is as follows:
I have configured AWS SES to receive emails on a subdomain. SES then send a notification to our web application via SNS. Now, SNS has a 150kb limit and therefore any emails with an attachment of sive>150kb is bounced. </p>

<p>My question is:
Is there a way to strip the SES email of the attachments before dispatching through SNS? </p>

<p>One solution is to save the attachments in S3, but we have absolutely no use for the attachments at this point and would prefer not incurring additional S3 costs for nothing. I have looked at multiple AWS documentation and have not been able to find a solution. Any pointers will be greatly appreciated.</p>
",1306651.0,,,,,2019-07-22 06:32:37,How do you strip the attachments from AWS SES-SNS incoming emails?,<amazon-web-services><amazon-ec2><aws-lambda><amazon-sns><amazon-ses>,1,0,,,,CC BY-SA 4.0,My problem statement is as follows: I have configured AWS SES to receive emails on a subdomain  SES then send a notification to our web application via SNS  Now  SNS has a 150kb limit and therefore any emails with an attachment of sive&gt;150kb is bounced   My question is: Is there a way to strip the SES email of the attachments before dispatching through SNS   One solution is to save the attachments in S3  but we have absolutely no use for the attachments at this point and would prefer not incurring additional S3 costs for nothing  I have looked at multiple AWS documentation and have not been able to find a solution  Any pointers will be greatly appreciated  
39249813,1,,,2016-08-31 12:33:51,,1,237,"<p>I am submitting tasks (eventually this will be hundreds of thousands of tasks) to AWS lambda using <a href=""http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/lambda/model/InvokeRequest.html"" rel=""nofollow""><code>InvokeRequest</code> class</a>, and I am interested in performance characteristics of the task: execution time, memory usage, etc.</p>

<p>Note that I will eventually want to compute various performance statistics and plots of the submitted tasks in costum code.</p>
",2483038.0,,,,,2016-08-31 13:01:22,How to get performance stats of the invoked AWS lambda task?,<java><amazon-web-services><aws-sdk><aws-lambda>,1,0,,,,CC BY-SA 3.0,I am submitting tasks (eventually this will be hundreds of thousands of tasks) to AWS lambda using   and I am interested in performance characteristics of the task: execution time  memory usage  etc  Note that I will eventually want to compute various performance statistics and plots of the submitted tasks in costum code  
57155426,1,57161421.0,,2019-07-23 01:09:24,,1,1230,"<p>I currently have one AWS Lambda function that is updating a DynamoDB table, and I need another Lambda function that needs to run after the data is updated. Is there any benefit to using a DynamoDB trigger in this case instead of invoking the second Lambda using the first one? </p>

<p>It looks like the programmatic invocation would give me more control over when the Lambda is called (ie. I could wait for several updates to occur before calling), and reading from a DynamoDB Stream costs money while simply invoking the Lambda does not. </p>

<p>So, is there a benefit to using a trigger here? Or would I be better off invoking the Lambda myself?</p>
",2558618.0,,,,,2019-07-23 09:53:44,When should I use a DynamoDB trigger over calling the Lambda with another?,<aws-lambda><amazon-dynamodb><amazon-dynamodb-streams>,3,0,,,,CC BY-SA 4.0,I currently have one AWS Lambda function that is updating a DynamoDB table  and I need another Lambda function that needs to run after the data is updated  Is there any benefit to using a DynamoDB trigger in this case instead of invoking the second Lambda using the first one   It looks like the programmatic invocation would give me more control over when the Lambda is called (ie  I could wait for several updates to occur before calling)  and reading from a DynamoDB Stream costs money while simply invoking the Lambda does not   So  is there a benefit to using a trigger here  Or would I be better off invoking the Lambda myself  
57690843,1,57692091.0,,2019-08-28 10:57:12,,1,101,"<p>I have an interesting real-life situation:</p>

<ul>
<li>I have a huge SQS (with really plenty of messages, constantly refilling (worst-case scenario, 50 messages per second);</li>
<li>A Lambda that will consume SQS and push them to an external database (the database will process the messages in a sync way, unfortunately).</li>
</ul>

<p><strong>Constraints:</strong></p>

<ul>
<li><p>The final external database shouldn't process data continuously but needs some idle time. This is due to the database activity costs (cost per CPU usage);</p></li>
<li><p>SQS is already part of the architecture;</p></li>
<li><p>the final external database is not in my control.</p></li>
</ul>

<p><strong>Here are my solutions, concerns and questions:</strong></p>

<ul>
<li>Sol 1: Using Cloudwatch to schedule the Lambda every hour to give time to consume part of the queue and then leaving the final DB idle for a bit.</li>
</ul>

<p>Issue: the queue could ""explode"" meaning it could fill really quickly and the processing would be slow.</p>

<ul>
<li>Sol 2: Create a Lambda (A) triggered by Cloudwatch every X time. This Lambda would cycle 10 times and trigger another Lambda (B) that would consume 10 messages (max of msgs per ""time?"" in SQS) and should autoscale as well.</li>
</ul>

<p>Issue: Not sure about the autoscaling criteria... it looks more fantasy.</p>

<ul>
<li>Sol 3 (bonus): Create a second SQS (2). An intermediate Lambda (I) will be triggered every X time and move the messages from SQS (1) to SQS (2). There is an event on SQS (2) that will trigger a Lambda (2) and this will autoscale.</li>
</ul>

<p>Issue: too messy, overcomplicated, tied to the number of messages moved from SQS (1) to SQS (2).</p>

<p><strong>Now</strong></p>

<p>Now, it is clear my concern is related to the Lambda autoscaling in consuming SQS and feeding the database.
Also, the Lambda should scale enough to consume a good amount of SQS messages but, at the same time, should leave some idle time to the final database.</p>

<p>I hope I've explained the situation well and would be happy to have your advice on that (happy to learn!).</p>

<p>Thanks,
Mauro</p>
",2038081.0,,,,,2019-08-28 12:09:28,"Lambda, SQS and Cloudwatch - Architectural question",<amazon-web-services><architecture><aws-lambda><bigdata><amazon-sqs>,1,0,1.0,,,CC BY-SA 4.0,I have an interesting real-life situation:  I have a huge SQS (with really plenty of messages  constantly refilling (worst-case scenario  50 messages per second); A Lambda that will consume SQS and push them to an external database (the database will process the messages in a sync way  unfortunately)   Constraints:  The final external database shouldnt process data continuously but needs some idle time  This is due to the database activity costs (cost per CPU usage); SQS is already part of the architecture; the final external database is not in my control   Here are my solutions  concerns and questions:  Sol 1: Using Cloudwatch to schedule the Lambda every hour to give time to consume part of the queue and then leaving the final DB idle for a bit   Issue: the queue could explode meaning it could fill really quickly and the processing would be slow   Sol 2: Create a Lambda (A) triggered by Cloudwatch every X time  This Lambda would cycle 10 times and trigger another Lambda (B) that would consume 10 messages (max of msgs per time  in SQS) and should autoscale as well   Issue: Not sure about the autoscaling criteria    it looks more fantasy   Sol 3 (bonus): Create a second SQS (2)  An intermediate Lambda (I) will be triggered every X time and move the messages from SQS (1) to SQS (2)  There is an event on SQS (2) that will trigger a Lambda (2) and this will autoscale   Issue: too messy  overcomplicated  tied to the number of messages moved from SQS (1) to SQS (2)  Now Now  it is clear my concern is related to the Lambda autoscaling in consuming SQS and feeding the database  Also  the Lambda should scale enough to consume a good amount of SQS messages but  at the same time  should leave some idle time to the final database  I hope Ive explained the situation well and would be happy to have your advice on that (happy to learn )  Thanks  Mauro 
57782987,1,,,2019-09-04 06:54:43,,1,172,"<p>I have a setup were i want to have 2 S3 buckets serving the exact same data, for redundancy. To have them being served (both) by CloudFront, i created an Origin Group. The Origin Group is the origin for the behavior with higher precedence.</p>

<p>My questions are:</p>

<p>1: Are there any fees for keeping content cached for more than the default 24h ? I assume not because it might be cheaper to store than to transfer data.</p>

<p>2: The origin access inside the group seems to always be the same, respecting the failover sequence (primary origin -> secondary origin -> ...). Is there any possibility to make the origin group be load balanced ? ie, actually pulling from the origin with the lowest latency inside the origin group ? I saw that i can use Lambda@Edge to change the domain for the request. How does that work with conjunction with Origin Groups and failover ? If i have a bucket A and a bucket B and they both form an Origin Group (A primary, B secondary) and i use the Lambda@Edge to route to B and the request fails, does it route back to A ? </p>

<p>Thanks !</p>
",10440482.0,,,,,2019-09-04 13:15:31,Is it possible to have AWS to prioritize CF Origin Group origins by country?,<amazon-web-services><aws-lambda><amazon-cloudfront>,1,0,,,,CC BY-SA 4.0,I have a setup were i want to have 2 S3 buckets serving the exact same data  for redundancy  To have them being served (both) by CloudFront  i created an Origin Group  The Origin Group is the origin for the behavior with higher precedence  My questions are: 1: Are there any fees for keeping content cached for more than the default 24h   I assume not because it might be cheaper to store than to transfer data  2: The origin access inside the group seems to always be the same  respecting the failover sequence (primary origin -&gt; secondary origin -&gt;    )  Is there any possibility to make the origin group be load balanced   ie  actually pulling from the origin with the lowest latency inside the origin group   I saw that i can use Lambda@Edge to change the domain for the request  How does that work with conjunction with Origin Groups and failover   If i have a bucket A and a bucket B and they both form an Origin Group (A primary  B secondary) and i use the Lambda@Edge to route to B and the request fails  does it route back to A    Thanks   
57796313,1,,,2019-09-04 22:25:12,,0,252,"<p>I'm currently working on a project where I use DynamoDB as my nosql database. Before I started I tried to learn how to model nosql databases since its really different to our known relational databases. I learned that I have to stick on the <strong><code>single table</code></strong> model. <br><br>Im using DynamoDB Streams to aggregate some data, for instance the customer count for a product (there are some more complex cases than that). Since I have only one single table, my lambda function writes in the same table, from which the stream came. <br><code>Add new customer to TableA -&gt; DynamoDB Stream triggers Lambda for TableA -&gt; Lambda updates a row in TableA -&gt; DynamoDB Stream triggers Lambda for Table A -&gt; Lambda function terminates</code>. If I understand it right, this scenario could lead to an infinite loop since the first insert trigger triggers an update trigger. This is something im escaping in my lambda function. <br><br>My question now is <br><strong>Am I  getting billed for two lambda functions and two DynamoDb streams each time I make an insert in my db?</strong> 
If yes should I ignore the best practise way of a nosql db and split the table into multiple or should I invest the money ? Because I am doubling my bill in this case. <br><strong>What are the cons of splitting my table in multiple tables ?</strong> Would be the effect of the cons that big ?</p>
",4035864.0,,,,,2019-09-05 19:10:38,DynamoDB Streams billing twice when using single table architecture,<amazon-web-services><aws-lambda><amazon-dynamodb><amazon-dynamodb-streams>,1,1,,,,CC BY-SA 4.0,Im currently working on a project where I use DynamoDB as my nosql database  Before I started I tried to learn how to model nosql databases since its really different to our known relational databases  I learned that I have to stick on the  model  Im using DynamoDB Streams to aggregate some data  for instance the customer count for a product (there are some more complex cases than that)  Since I have only one single table  my lambda function writes in the same table  from which the stream came    If I understand it right  this scenario could lead to an infinite loop since the first insert trigger triggers an update trigger  This is something im escaping in my lambda function  My question now is Am I  getting billed for two lambda functions and two DynamoDb streams each time I make an insert in my db   If yes should I ignore the best practise way of a nosql db and split the table into multiple or should I invest the money   Because I am doubling my bill in this case  What are the cons of splitting my table in multiple tables   Would be the effect of the cons that big   
58410362,1,,,2019-10-16 09:44:14,,0,101,"<p>I've already managed to create a lambda function that loads a model.pb from S3 and apply object detection to an input image (installed tensorflow 1.12)</p>

<p>Is it possible to load a Sagemaker model/endpoint-configuration inside a lambda function ? I mean install all packages needed inside the lambda, without deploying an endpoint/ec2-like instance.</p>

<p>I guess inference performance would drop, but the solution seems to be more cost effective and scalable ready.</p>
",7358899.0,,,,,2019-12-12 20:37:29,Sagemaker ML - Load Tensorflow model endpoint directly inside a lambda function,<tensorflow><machine-learning><aws-lambda><amazon-sagemaker>,1,0,,,,CC BY-SA 4.0,Ive already managed to create a lambda function that loads a model pb from S3 and apply object detection to an input image (installed tensorflow 1 12) Is it possible to load a Sagemaker model/endpoint-configuration inside a lambda function   I mean install all packages needed inside the lambda  without deploying an endpoint/ec2-like instance  I guess inference performance would drop  but the solution seems to be more cost effective and scalable ready  
54144737,1,,,2019-01-11 10:33:43,,0,732,"<p>In my Lambda I am trying to parse the content of a document from s3 bucket. The document I am processing is a txt file with more than 100Mb. I need to parse only the first line of the file. 
What is the best cost-effective way to read the file?</p>

<p>Currently, I am taking the content using getObjectContent() method and taking the 1st line from it like this. </p>

<pre><code>private AmazonS3 s3 =  AmazonS3ClientBuilder.standard().build ();
GetObjectRequest getObjectRequest = new GetObjectRequest(bucket, key);
S3Object s3Object = s3.getObject(getObjectRequest);


BufferedReader reader = new BufferedReader(new InputStreamReader(s3Object.getObjectContent()));
String firstLine;
try {
  while ((firstLine = reader.readLine()) != null) {
    logger.log(""META PROCESSOR | FIRST LINE OF FILE : "" + firstLine);
    break;
  }
} catch (IOException e) {
  logger.log(""META PROCESSOR | FAILED TO LOAD FIRST LINE "");
  return null;
}
</code></pre>

<p>Is it a good way to read the entire content just to read the first line? Is there any method available to read n number of lines from a file or n number of bytes from a file? </p>
",3343195.0,,,,,2019-01-11 10:33:43,Read n number of lines from s3 object using AWS lambda,<amazon-web-services><amazon-s3><aws-lambda>,0,4,0.0,,,CC BY-SA 4.0,In my Lambda I am trying to parse the content of a document from s3 bucket  The document I am processing is a txt file with more than 100Mb  I need to parse only the first line of the file   What is the best cost-effective way to read the file  Currently  I am taking the content using getObjectContent() method and taking the 1st line from it like this    Is it a good way to read the entire content just to read the first line  Is there any method available to read n number of lines from a file or n number of bytes from a file   
65774612,1,,,2021-01-18 12:32:49,,1,1036,"<p>I am creating website on AWS using serverless framework.
There are 3 parts which I need to serve clients:</p>
<ol>
<li>endpoints of lambda functions (/getusers)</li>
<li>index.html served throught lambda (I need to insert some constants to it)</li>
<li>static html, js, css, img files, which should be served directly from S3, without going throught costly lambda.</li>
</ol>
<p>How do I accomplish that? I need all parts of the app to be able to cooperate thus, probably on the same domain. I need index.html and js files to be able to call the lambda endpoints (setted up by API Gateway). Also I would like all files be cached throught CloudFront. I searched google but did not found any example.</p>
",1942590.0,,,,,2021-06-02 02:55:14,AWS Serverless how to serve static files from S3 and some other through lambda,<amazon-web-services><amazon-s3><aws-lambda><amazon-cloudfront><serverless-framework>,2,0,,,,CC BY-SA 4.0,I am creating website on AWS using serverless framework  There are 3 parts which I need to serve clients:  endpoints of lambda functions (/getusers) index html served throught lambda (I need to insert some constants to it) static html  js  css  img files  which should be served directly from S3  without going throught costly lambda   How do I accomplish that  I need all parts of the app to be able to cooperate thus  probably on the same domain  I need index html and js files to be able to call the lambda endpoints (setted up by API Gateway)  Also I would like all files be cached throught CloudFront  I searched google but did not found any example  
65790802,1,65791057.0,,2021-01-19 11:40:29,,1,155,"<p>I'm accessing Secrets Manager in my serverless.yml via</p>
<pre><code>${ssm:/aws/reference/secretsmanager/&lt;path-to-secret&gt;~true}
</code></pre>
<p>which works out nicely.</p>
<p>Problem is, I'm keeping all my secrets in a single region &amp; now I'm trying to add another region (= stage in serverless), but I don't know how to reference secrets from my main region. To keep costs low, I don't want to replicate my secrets to the secondary regions though.</p>
<p>Is there any way to achieve this without some pre-scripting magic but solely using Serverless?</p>
",5908014.0,,,,,2021-01-19 11:57:07,Cross-Region Secrets Manager Access in Serverless Framework Template,<amazon-web-services><serverless-framework>,1,0,,,,CC BY-SA 4.0,Im accessing Secrets Manager in my serverless yml via  which works out nicely  Problem is  Im keeping all my secrets in a single region &amp; now Im trying to add another region (= stage in serverless)  but I dont know how to reference secrets from my main region  To keep costs low  I dont want to replicate my secrets to the secondary regions though  Is there any way to achieve this without some pre-scripting magic but solely using Serverless  
65786713,1,,,2021-01-19 06:53:08,,0,167,"<p>I'm having a Microservice Architecture completely build around AWS Lambda, also including a producer/listener which is loosely-coupled via SQS to the business lambda functions. It's working nicely via following approach:</p>
<ul>
<li>lambda has a reserved concurrency of 1</li>
<li>gets triggered every 1 minute via CloudWatch Events: as the max concurrency is 1, if one instance is running, there won't be a parallel invocation if a function is already provisioned</li>
<li>if the function approaches the timeout of 15 minutes, it will shutdown itself gracefully</li>
</ul>
<p>I did not experience any troubles yet &amp; the solution is really cost-efficient but as you probably already noticed, there's at least one <strong>major downside</strong>: the consumer/producer will be &quot;offline&quot; for up to 1.x minutes (next CloudWatch invocation could take up to 59s + bootstrapping the producer/listener) every ~15 minutes (max execution time for Lambda).</p>
<p>I'm not having any hard real-time requirements, but I'm sure there's a better approach or improvements without switching to running a container in ECS, what I really want to avoid.</p>
<p>What are my options?</p>
",5908014.0,,,,,2021-07-07 17:03:16,Approaches on running Kafka Producer/Listener on Serverless Architecture / Lambda,<amazon-web-services><lambda><serverless><serverless-architecture>,2,0,,,,CC BY-SA 4.0,Im having a Microservice Architecture completely build around AWS Lambda  also including a producer/listener which is loosely-coupled via SQS to the business lambda functions  Its working nicely via following approach:  lambda has a reserved concurrency of 1 gets triggered every 1 minute via CloudWatch Events: as the max concurrency is 1  if one instance is running  there wont be a parallel invocation if a function is already provisioned if the function approaches the timeout of 15 minutes  it will shutdown itself gracefully  I did not experience any troubles yet &amp; the solution is really cost-efficient but as you probably already noticed  theres at least one major downside: the consumer/producer will be offline for up to 1 x minutes (next CloudWatch invocation could take up to 59s + bootstrapping the producer/listener) every ~15 minutes (max execution time for Lambda)  Im not having any hard real-time requirements  but Im sure theres a better approach or improvements without switching to running a container in ECS  what I really want to avoid  What are my options  
48031259,1,48198280.0,,2017-12-30 05:27:41,,0,542,"<p>Firstly I'm not sure if my approach is right one.</p>

<p>This is what I'm doing. I've some video processing work to do, for which I'm gonna use FFMPEG. And it can take from 1 minute to 20 minutes to do work.</p>

<p>so my questions are: </p>

<ul>
<li>are CFs good fit for this. I don't want my main server to do these
tasks.</li>
<li>can i make CF to only time out after say 60 minutes, or before if finished.</li>
<li>Is it going to be cheaper than just using server instead. </li>
</ul>

<p>From what I know it's perfect scenario to use cloud functions. Alternative is to use build queue, wait for available processes to finish and then when time comes finish task.</p>
",1319799.0,,,,,2018-01-11 00:39:01,Prevent google cloud function timeout for video processing?,<ffmpeg><aws-lambda><google-cloud-functions>,1,0,,,,CC BY-SA 3.0,Firstly Im not sure if my approach is right one  This is what Im doing  Ive some video processing work to do  for which Im gonna use FFMPEG  And it can take from 1 minute to 20 minutes to do work  so my questions are:   are CFs good fit for this  I dont want my main server to do these tasks  can i make CF to only time out after say 60 minutes  or before if finished  Is it going to be cheaper than just using server instead    From what I know its perfect scenario to use cloud functions  Alternative is to use build queue  wait for available processes to finish and then when time comes finish task  
52220991,1,,,2018-09-07 10:50:21,,0,478,"<p>I am using a lambda function in a VPC to connect to an RDS instance in the same VPC. I am considering removing the lambda from the VPC to massively reduce the cold-start time but I want to keep my RDS instance in the VPC.</p>

<p>Can anyone foresee major problems with making the lambda function use an SSH tunnel to connect to a bastion instance within the VPC and subsequently to the RDS instance? Or something similar with a VPN?</p>

<p>There will obviously be some over-head as the traffic has an extra 'jump' so to speak, but would it be significant enough to make this approach non-feasible? Or is the only current approach to keep the Lambda in the same VPC and try to keep and few invocations running?</p>

<p>I also pay for a NAT gateway so my Lambda in a VPC can access the internet. If I can get it out of the VPC by using an SSH tunnel to connect to the RDS instance it will also simplify my architecture here &amp; reduce my operating costs.</p>
",3707095.0,,,,,2019-09-03 18:12:01,AWS Lambda potential alternatives to connect to RDS in VPC,<amazon-web-services><ssh><aws-lambda><amazon-vpc><aws-serverless>,1,2,,,,CC BY-SA 4.0,I am using a lambda function in a VPC to connect to an RDS instance in the same VPC  I am considering removing the lambda from the VPC to massively reduce the cold-start time but I want to keep my RDS instance in the VPC  Can anyone foresee major problems with making the lambda function use an SSH tunnel to connect to a bastion instance within the VPC and subsequently to the RDS instance  Or something similar with a VPN  There will obviously be some over-head as the traffic has an extra jump so to speak  but would it be significant enough to make this approach non-feasible  Or is the only current approach to keep the Lambda in the same VPC and try to keep and few invocations running  I also pay for a NAT gateway so my Lambda in a VPC can access the internet  If I can get it out of the VPC by using an SSH tunnel to connect to the RDS instance it will also simplify my architecture here &amp; reduce my operating costs  
48054733,1,48055119.0,,2018-01-02 01:53:58,,11,2387,"<p>Very interested in getting hands-on with <strong><a href=""https://serverless.com/"" rel=""noreferrer"">Serverless</a></strong> in 2018. Already looking to implement usage of <strong><a href=""https://aws.amazon.com/lambda/"" rel=""noreferrer"">AWS Lambda</a></strong> in several decentralized app projects. However, I don't yet understand how you can prevent abuse of your endpoint from a 3rd-party app <em>(perhaps even a competitor),</em> from driving up your usage costs. </p>

<p>I'm not talking about a DDoS, or where all the traffic is coming from a single IP, which can happen on any network, but specifically having a 3rd-party app's customers directly make the REST calls, which cause your usage costs to rise, because their app is piggy-backing on your <strong>""open""</strong> endpoints.</p>

<p><strong>For example:</strong></p>

<p>I wish to create an endpoint on AWS Lambda to give me the <strong><a href=""https://github.com/ccxt/ccxt/wiki/Manual#price-tickers"" rel=""noreferrer"">current price of Ethereum ETH/USD.</a></strong> What would prevent another <em>(or every)</em> dapp developer from using <strong>MY</strong> lambda endpoint and causing excessive billing charges to my account?</p>
",514914.0,,514914.0,,2018-01-05 05:23:28,2019-08-11 23:11:24,How To Prevent AWS Lambda Abuse by 3rd-party apps,<amazon-web-services><lambda><serverless>,3,1,4.0,,,CC BY-SA 3.0,Very interested in getting hands-on with  in 2018  Already looking to implement usage of  in several decentralized app projects  However  I dont yet understand how you can prevent abuse of your endpoint from a 3rd-party app (perhaps even a competitor)  from driving up your usage costs   Im not talking about a DDoS  or where all the traffic is coming from a single IP  which can happen on any network  but specifically having a 3rd-party apps customers directly make the REST calls  which cause your usage costs to rise  because their app is piggy-backing on your open endpoints  For example: I wish to create an endpoint on AWS Lambda to give me the  What would prevent another (or every) dapp developer from using MY lambda endpoint and causing excessive billing charges to my account  
65833840,1,65834501.0,,2021-01-21 18:42:43,,0,31,"<p>When the subscription is created, like this:</p>
<pre><code>import Stripe from 'stripe';

const stripe = Stripe(process.env.STRIPE_API_KEY_SERVER);
const freePlan =
  process.env.NODE_ENV === 'production'
    ? 'price_xxxx'
    : 'price_xxxx';

export default async (req, res) =&gt; {
  const { customerId, tokenId, price } = req.body;

  try {
    const source = await stripe.customers.createSource(customerId, {
      source: tokenId,
    });

    if (!source) {
      return res.json({ ok: false, error: 'Stripe failed to attach card' });
    }

    const sub = await stripe.subscriptions.create({
      customer: customerId,
      items: [{ price }],
      trial_end:
        price === freePlan
          ? new Date(Date.now() + 30 * 24 * 60 * 60 * 1000)
          : undefined,
    });

    return res.json({ ok: true, subId: sub.id });
  } catch (error) {
    console.error(error);
    return res.json({ ok: false, error });
  }
};
</code></pre>
<p>The petition is successful, but when we see the stripe console, nothing was charge...</p>
<p><a href=""https://i.stack.imgur.com/bHZnN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bHZnN.png"" alt=""enter image description here"" /></a></p>
<p>And the product has its price... (In this case, was used &quot;Plan mensual&quot;).</p>
<p><a href=""https://i.stack.imgur.com/sWIMn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sWIMn.png"" alt=""enter image description here"" /></a></p>
<p>What can we do? Thanks!</p>
",8341544.0,,8341544.0,,2021-01-21 18:50:45,2021-01-21 19:31:38,"When a user creates a subscription and pays, in the ""stripe"" console nothing was charge, 0 pesos were sent",<node.js><stripe-payments><vercel>,1,2,,,,CC BY-SA 4.0,When the subscription is created  like this:  The petition is successful  but when we see the stripe console  nothing was charge     And the product has its price    (In this case  was used Plan mensual)   What can we do  Thanks  
71417552,1,,,2022-03-10 00:34:33,,0,20,"<p>I need to do a search in my dynamoDB table that matches multiple values from a single item.</p>
<p>This is the type of Items i am storing:</p>
<pre><code>{
   &quot;id&quot;: &quot;&lt;product id&gt;&quot;,
   &quot;name&quot;: &quot;Product Name&quot;,
   &quot;price&quot;: 1.23,
   &quot;tags&quot;: [
     &quot;tag1&quot;,
     &quot;tag2&quot;,
     &quot;tag3&quot;
]
</code></pre>
<p>I need to return an array of items having tags that match all of the tags a the comma-separated list.</p>
<p>For example: i am looking for items that only contains tags &quot;tag1&quot; and &quot;tag2&quot;.</p>
<p>My first aproach was getting all the items from the dynamoDB table and then iterating each item to check if this condition matchs, then add the target item to an object of objects.</p>
<p>My approach is definetly not cost effective, Any suggestions with node.js?</p>
",13670010.0,,,,,2022-03-10 01:00:08,Search Items by multiple Tags DynamoDB NodeJS,<node.js><amazon-web-services><aws-lambda><amazon-dynamodb>,1,0,,,,CC BY-SA 4.0,I need to do a search in my dynamoDB table that matches multiple values from a single item  This is the type of Items i am storing:  I need to return an array of items having tags that match all of the tags a the comma-separated list  For example: i am looking for items that only contains tags tag1 and tag2  My first aproach was getting all the items from the dynamoDB table and then iterating each item to check if this condition matchs  then add the target item to an object of objects  My approach is definetly not cost effective  Any suggestions with node js  
65846053,1,,,2021-01-22 13:24:59,,0,33,"<p>I'm using Athena and I'm dealing with about 1000 raw compressed data files daily (each of 13MB). I need to process and store efficiently to improve queries speed and cost and I do it by partitions.</p>
<p>Im currently using lambdas (being triggered for each file creation) as my ETL and it - loads the raw data file, reorganizes columns, changes the file format (to parquet) and then splits it and saves it to multiple files on S3 bucket.
then another lambda starts and creates new partitions for the Athena table:</p>
<p>s3://raw_data/2020/01/01  --&gt; s3://table_name/2020/01/01/0-50/ (Athena table built over these files)</p>
<p>Many partitions affects on the queries speed, so I've heard of a
new feature called Partitions Projection. using this feature I can speed up the query processing and automate partition management.</p>
<p>Im a little bit confused of the options.</p>
<p>Another point is that theoretically, if I understand it correctly, I can get rid of the lambdas and save some cost by using Athena <strong>CTAS</strong> once to create the table and then executing <strong>INSERT INTO</strong> daily (limited up to 100 partitions) but then I still use partitions and can't use Partitions Projection.</p>
<p>What is the most efficient way to deal ETL when it comes to daily provided files?</p>
",15059554.0,,,,,2021-01-22 13:24:59,Daily ETL job big data files,<amazon-web-services><aws-lambda><aws-glue><amazon-athena>,0,2,,,,CC BY-SA 4.0,Im using Athena and Im dealing with about 1000 raw compressed data files daily (each of 13MB)  I need to process and store efficiently to improve queries speed and cost and I do it by partitions  Im currently using lambdas (being triggered for each file creation) as my ETL and it - loads the raw data file  reorganizes columns  changes the file format (to parquet) and then splits it and saves it to multiple files on S3 bucket  then another lambda starts and creates new partitions for the Athena table: s3://raw_data/2020/01/01  --&gt; s3://table_name/2020/01/01/0-50/ (Athena table built over these files) Many partitions affects on the queries speed  so Ive heard of a new feature called Partitions Projection  using this feature I can speed up the query processing and automate partition management  Im a little bit confused of the options  Another point is that theoretically  if I understand it correctly  I can get rid of the lambdas and save some cost by using Athena CTAS once to create the table and then executing INSERT INTO daily (limited up to 100 partitions) but then I still use partitions and cant use Partitions Projection  What is the most efficient way to deal ETL when it comes to daily provided files  
71426926,1,,,2022-03-10 15:44:04,,0,25,"<p>I need to setup a proxy apigateway in AWS that gets requests from the internet, stores the body (and headers) in a dynamodb and then forwards it to another external API gateway. Obviously, I need to wait for the response from the external API gateway and then send it back to the original client.</p>
<p>One possible solution is setting up a Lambda function as apigateway backend integration. This Lambda function stores the request in dynamodb and then simply makes a http call to the other external API Gateway. The problem I now have is that this lambda function is active while waiting for the response from the external API Gateway. This could take up several seconds and therefore is not a viable solution for high traffic environments, since hundreds of lambda functions are active for several seconds at all times. This quickly becomes too expensive.</p>
<p>AWS API Gateway also supports to simply proxy the traffic to a HTTP endpoint, which works, but this way I cannot get access to the request body and store it in dynamodb.</p>
<p>Since this task is at its core so easy I assume there is a simple way, but I can't seem to find a solution after a whole day of research. I'm open to any feedback and suggestions :)</p>
",18429999.0,,,,,2022-03-10 17:46:33,How to store a request from AWS API Gateway in dynamodb and then send it to an endpoint,<amazon-web-services><aws-lambda><amazon-dynamodb><aws-api-gateway><http-proxy>,1,0,,,,CC BY-SA 4.0,I need to setup a proxy apigateway in AWS that gets requests from the internet  stores the body (and headers) in a dynamodb and then forwards it to another external API gateway  Obviously  I need to wait for the response from the external API gateway and then send it back to the original client  One possible solution is setting up a Lambda function as apigateway backend integration  This Lambda function stores the request in dynamodb and then simply makes a http call to the other external API Gateway  The problem I now have is that this lambda function is active while waiting for the response from the external API Gateway  This could take up several seconds and therefore is not a viable solution for high traffic environments  since hundreds of lambda functions are active for several seconds at all times  This quickly becomes too expensive  AWS API Gateway also supports to simply proxy the traffic to a HTTP endpoint  which works  but this way I cannot get access to the request body and store it in dynamodb  Since this task is at its core so easy I assume there is a simple way  but I cant seem to find a solution after a whole day of research  Im open to any feedback and suggestions :) 
52275235,1,52276849.0,,2018-09-11 11:50:33,,39,25263,"<p>I'm pretty new to the whole Serverless landscape, and am trying to wrap my head around when to use Fargate vs Lambda. </p>

<p>I am aware that Fargate is a serverless subset of ECS, and Lambda is serverless as well but driven by events. But I'd like to be able to explain the two paradigms in simple terms to other folks that are familiar with containers but not that much with AWS and serverless. </p>

<p>Currently we have a couple of physical servers in charge of receiving text files, parsing them out, and populating several db tables with the results. Based on my understanding, I think this would be a use case better suited for Lambda because the process that parses out the text files is triggered by a schedule, is not long running, and ramps down when not in use. </p>

<p>However, if we were to port over one of our servers that receive API calls, we would probably want to use Fargate because we would always need at least one instance of the image up and running. </p>

<p>In terms of containers, and in very general terms would it be safe to say that if the container is designed to do:  </p>

<pre><code>docker run &lt;some_input&gt;
</code></pre>

<p>Then it is a job for Lambda. </p>

<p>But if the container is designed to do something like:</p>

<pre><code>docker run --expose 80
</code></pre>

<p>Then it is a job for Fargate. </p>

<p>Is this a good analogy? </p>
",2486401.0,,,,,2021-03-29 20:34:46,"Fargate vs Lambda, when to use which?",<aws-lambda><amazon-ecs><serverless><aws-fargate>,4,0,6.0,,,CC BY-SA 4.0,Im pretty new to the whole Serverless landscape  and am trying to wrap my head around when to use Fargate vs Lambda   I am aware that Fargate is a serverless subset of ECS  and Lambda is serverless as well but driven by events  But Id like to be able to explain the two paradigms in simple terms to other folks that are familiar with containers but not that much with AWS and serverless   Currently we have a couple of physical servers in charge of receiving text files  parsing them out  and populating several db tables with the results  Based on my understanding  I think this would be a use case better suited for Lambda because the process that parses out the text files is triggered by a schedule  is not long running  and ramps down when not in use   However  if we were to port over one of our servers that receive API calls  we would probably want to use Fargate because we would always need at least one instance of the image up and running   In terms of containers  and in very general terms would it be safe to say that if the container is designed to do:    Then it is a job for Lambda   But if the container is designed to do something like:  Then it is a job for Fargate   Is this a good analogy   
65906702,1,,,2021-01-26 17:58:20,,0,55,"<p>Seems a little inefficient the way it currently is:</p>
<pre><code>response.body = {
   user: await userService(userID) // calls a user service to get info on user
   friends: await friendsService(userID) // calls a friends service to get info on friends for 
}

</code></pre>
<p>Let's say the userService and friendsService are configured on different API Gateway endpoints.</p>
<p>Then wouldn't that make the network request take longer than if I were to just package my entire backend into one zip file that's uploaded to AWS Lambda.</p>
<p>Seems like this is very inefficient.</p>
<p>Is there a way to call other lambdas without having to make a network request? I understand putting the lambdas/gateway in the same VPC as the main Gateway endpoint exposed to the internet, but this is expensive?</p>
<p>Anyway to do this more efficiently?</p>
",11111151.0,,174777.0,,2021-01-27 00:07:10,2021-01-27 00:07:10,What is the best way to internally call an AWS Lambda function within another Lambda function?,<amazon-web-services><aws-lambda>,2,0,,,,CC BY-SA 4.0,Seems a little inefficient the way it currently is:  Lets say the userService and friendsService are configured on different API Gateway endpoints  Then wouldnt that make the network request take longer than if I were to just package my entire backend into one zip file thats uploaded to AWS Lambda  Seems like this is very inefficient  Is there a way to call other lambdas without having to make a network request  I understand putting the lambdas/gateway in the same VPC as the main Gateway endpoint exposed to the internet  but this is expensive  Anyway to do this more efficiently  
65933141,1,,,2021-01-28 08:04:15,,0,208,"<p>I hope to evaluate AWS Lambda's suitability for long running computing intensive batch jobs, vs EC2 instances. To do that, I need to find out a 24h cost of Lambda vs EC2 for similar computing power. (In the 24h I'll run many small jobs, each a few seconds)</p>
<p>From <a href=""https://docs.aws.amazon.com/lambda/latest/dg/configuration-memory.html"" rel=""nofollow noreferrer"">Configuring Lambda function memory</a> I can see &quot;At 1,769 MB, a function has the equivalent of one vCPU (one vCPU-second of credits per second)&quot;. But there is no description of how powerful this CPU will be.</p>
<p>For EC2 nodes, I can find out the CPU specs of T2 or C5 so I know what to expect, but not the case for Lambda. Anyone has an idea? Thanks.</p>
",337863.0,,337863.0,,2021-01-28 08:31:21,2021-01-29 07:52:02,CPU Spec for AWS Lambda?,<amazon-web-services><aws-lambda>,1,3,,,,CC BY-SA 4.0,I hope to evaluate AWS Lambdas suitability for long running computing intensive batch jobs  vs EC2 instances  To do that  I need to find out a 24h cost of Lambda vs EC2 for similar computing power  (In the 24h Ill run many small jobs  each a few seconds) From  I can see At 1 769 MB  a function has the equivalent of one vCPU (one vCPU-second of credits per second)  But there is no description of how powerful this CPU will be  For EC2 nodes  I can find out the CPU specs of T2 or C5 so I know what to expect  but not the case for Lambda  Anyone has an idea  Thanks  
66604016,1,,,2021-03-12 16:54:05,,2,822,"<p>This question is inspired by this other: <a href=""https://stackoverflow.com/questions/42964429/is-it-a-good-idea-to-invoke-lambda-from-other-lambda-in-aws-lambda-service"">Is it a good idea to invoke lambda from other lambda in AWS lambda service?</a></p>
<p>I am using netcoreapp3.1 and C# to develop lambda.</p>
<p>I have one lambda A that accepts HTTP requests from API Gateway. I am using the dotnet template <code>serverless.AspNetCoreWebAPI</code> as per <a href=""https://aws.amazon.com/blogs/developer/running-serverless-asp-net-core-web-apis-with-amazon-lambda"" rel=""nofollow noreferrer"">https://aws.amazon.com/blogs/developer/running-serverless-asp-net-core-web-apis-with-amazon-lambda</a> where my lambda function reacts to invokations as if it was a web api.</p>
<p>I have another lambda B that also gets invoked by API Gateway, only this time it gets websocket events (CONNECT, DISCONNECT, MESSAGE).</p>
<p>I am thinking of having lambda B invoking lambda A synchronously.</p>
<p>The flow would be like this: API Gateway &gt; Lambda B &gt; Lambda A</p>
<p>I have here a dilemma:</p>
<ul>
<li>Is it a <strong>good idea to invoke directly Lambda A from Lambda B?</strong></li>
<li>Or is it <strong>better to have Lambda B send an http request to API Gateway</strong> and leave the gateway do its thing invoking Lambda A?</li>
<li>Or should I forget about exposing Lambda A functionality as &quot;http&quot; and <strong>have a single entry point in Lambda A that accepts ANY payload so that Lambda B can invoke it with a custom message?</strong></li>
</ul>
<p>Cost and performance wise I guess there is no much difference, but invoking Lambda A from Lambda B seems &quot;easier&quot; as the only thing I really need is to know the name of the function.</p>
<p>Here comes my attempt to do that, I use the</p>
<pre><code>public async Task&lt;APIGatewayProxyResponse&gt; FunctionHandler(
    APIGatewayProxyRequest apiGatewayProxyRequest, 
    ILambdaContext context)
{
    context.Logger.LogLine(&quot;Get Request\n&quot;);
    var isWebSocketRequest = apiGatewayProxyRequest.HttpMethod is null;
    if (!isWebSocketRequest)
    {
        throw new ArgumentException(
            $&quot;Received Http method {apiGatewayProxyRequest.HttpMethod}, not a valid websocket event&quot;);
    }

    var result = await ProcessWebsocketEvent(apiGatewayProxyRequest, context.Logger);
    return result;
}

private async Task&lt;APIGatewayProxyResponse&gt; ProcessWebsocketEvent(APIGatewayProxyRequest apiGatewayProxyRequest, ILambdaLogger logger)
{
    var eventType = apiGatewayProxyRequest.RequestContext.EventType;
    string body = eventType;
    if (&quot;CONNECT&quot; == eventType)
    {
        body += $&quot;, connection={apiGatewayProxyRequest.RequestContext.ConnectionId}&quot;;
    }
    else if (&quot;DISCONNECT&quot; == eventType)
    {
        body += $&quot;, connection={apiGatewayProxyRequest.RequestContext.ConnectionId}&quot;;
    }
    else if (&quot;MESSAGE&quot; == eventType)
    {
        body += $&quot;, body={apiGatewayProxyRequest.Body}&quot;;
    }

    logger.LogLine(&quot;Creating amazon lambda client..&quot;);
    using var lambdaClient = new AmazonLambdaClient(RegionEndpoint.EUWest3);
    var payload =
        new
        {
            Foo = &quot;bar&quot;,
            Bar = &quot;foo&quot;
        };
    var json = JsonSerializer.Serialize(payload);
    logger.LogLine($&quot;With payload {payload}&quot;);
    var request =
        new InvokeRequest
        {
            FunctionName = &quot;lambda-dotnet-webapi-function&quot;,
            Payload = json
        };
    
    var responseInvoke = await lambdaClient.InvokeAsync(request);
    using var stream = new StreamReader(responseInvoke.Payload);
    var responsePayload = await stream.ReadToEndAsync();

    var response = new APIGatewayProxyResponse
    {
        StatusCode = (int) HttpStatusCode.OK,
        Body = responsePayload,
        Headers = new Dictionary&lt;string, string&gt; {{&quot;Content-Type&quot;, &quot;text/plain&quot;}}
    };
    return response;
}
</code></pre>
<p>First thing I noticed. If I have a 3 seconds timeout, it times out. I have to increase it.
If I increase it to 10 seconds I am able to get an invocation as the invoke request's payload is not &quot;correct&quot;</p>
<pre><code>{&quot;statusCode&quot;:200,&quot;headers&quot;:{&quot;Content-Type&quot;:&quot;text/plain&quot;},&quot;body&quot;:&quot;{\n  \&quot;errorType\&quot;: \&quot;NullReferenceException\&quot;,\n  \&quot;errorMessage\&quot;: \&quot;Object reference not set to an instance of an object.\&quot;,\n  \&quot;stackTrace\&quot;: [\n    \&quot;at Amazon.Lambda.AspNetCoreServer.APIGatewayProxyFunction.MarshallRequest(InvokeFeatures features, APIGatewayProxyRequest apiGatewayRequest, ILambdaContext lambdaContext)\&quot;,\n    \&quot;at Amazon.Lambda.AspNetCoreServer.AbstractAspNetCoreFunction`2.FunctionHandlerAsync(TREQUEST request, ILambdaContext lambdaContext)\&quot;,\n    \&quot;at lambda_method(Closure , Stream , Stream , LambdaContextInternal )\&quot;\n  ]\n}\n&quot;,&quot;isBase64Encoded&quot;:false}
</code></pre>
<p>The Lambda A runs for 800ms
The Lambda B runs for 9000ms (9 seconds, a lot..)</p>
<p>This brings me to another question. <strong>If this is an acceptable approach, how can I send the request as if Lambda B was a &quot;proxy&quot;?</strong></p>
<p><em>I am thinking of invoking Lambda A in an http-ish way, mainly because I really like the library <a href=""https://github.com/aws/aws-lambda-dotnet/tree/master/Libraries/src/Amazon.Lambda.AspNetCoreServer"" rel=""nofollow noreferrer"">https://github.com/aws/aws-lambda-dotnet/tree/master/Libraries/src/Amazon.Lambda.AspNetCoreServer</a> which allows my Lambda to be implemented as a standard aspNet web api, read appsettings, dependency injection, test with test server, etc.</em></p>
<p>Looking at Lambda A logs I can see the following warning</p>
<pre><code>[Warning] Amazon.Lambda.AspNetCoreServer.AbstractAspNetCoreFunction: Request does not contain domain name information but is derived from APIGatewayProxyFunction. 
</code></pre>
<p>and the exception that I am getting as the response in Lambda B:</p>
<pre><code>Object reference not set to an instance of an object.: NullReferenceException
   at Amazon.Lambda.AspNetCoreServer.APIGatewayProxyFunction.MarshallRequest(InvokeFeatures features, APIGatewayProxyRequest apiGatewayRequest, ILambdaContext lambdaContext)
   at Amazon.Lambda.AspNetCoreServer.AbstractAspNetCoreFunction`2.FunctionHandlerAsync(TREQUEST request, ILambdaContext lambdaContext)
   at lambda_method(Closure , Stream , Stream , LambdaContextInternal )
</code></pre>
",2948212.0,,,,,2021-03-13 04:24:15,Is it a good idea to call one AWS Lambda from another with an APIGatewayProxyRequest?,<c#><.net-core><aws-lambda><aws-api-gateway>,1,2,,,,CC BY-SA 4.0,This question is inspired by this other:  I am using netcoreapp3 1 and C# to develop lambda  I have one lambda A that accepts HTTP requests from API Gateway  I am using the dotnet template  as per  where my lambda function reacts to invokations as if it was a web api  I have another lambda B that also gets invoked by API Gateway  only this time it gets websocket events (CONNECT  DISCONNECT  MESSAGE)  I am thinking of having lambda B invoking lambda A synchronously  The flow would be like this: API Gateway &gt; Lambda B &gt; Lambda A I have here a dilemma:  Is it a good idea to invoke directly Lambda A from Lambda B  Or is it better to have Lambda B send an http request to API Gateway and leave the gateway do its thing invoking Lambda A  Or should I forget about exposing Lambda A functionality as http and have a single entry point in Lambda A that accepts ANY payload so that Lambda B can invoke it with a custom message   Cost and performance wise I guess there is no much difference  but invoking Lambda A from Lambda B seems easier as the only thing I really need is to know the name of the function  Here comes my attempt to do that  I use the  First thing I noticed  If I have a 3 seconds timeout  it times out  I have to increase it  If I increase it to 10 seconds I am able to get an invocation as the invoke requests payload is not correct  The Lambda A runs for 800ms The Lambda B runs for 9000ms (9 seconds  a lot  ) This brings me to another question  If this is an acceptable approach  how can I send the request as if Lambda B was a proxy  I am thinking of invoking Lambda A in an http-ish way  mainly because I really like the library  which allows my Lambda to be implemented as a standard aspNet web api  read appsettings  dependency injection  test with test server  etc  Looking at Lambda A logs I can see the following warning  and the exception that I am getting as the response in Lambda B:  
32561554,1,,,2015-09-14 09:37:39,,1,840,"<p>I need to copy a large chunk of data, around 300 GB of files from say bucket A which is in us-east region and to bucket B which is in ap-southeast region. Also I need to change the structure of the bucket. Like I need to push the files to different folders on bucket B according to the image name which is in the bucket A. I tried to using AWS Lambda but it's not available in ap-southeast. </p>

<p>Also how much would it cost since data will be transferred between regions?</p>
",449344.0,,174777.0,,2015-09-14 23:54:24,2015-09-14 23:54:24,Copying multiple files in large volume between two s3 buckets which are in the different regions,<amazon-web-services><amazon-s3><amazon-ec2><aws-lambda>,2,0,,,,CC BY-SA 3.0,I need to copy a large chunk of data  around 300 GB of files from say bucket A which is in us-east region and to bucket B which is in ap-southeast region  Also I need to change the structure of the bucket  Like I need to push the files to different folders on bucket B according to the image name which is in the bucket A  I tried to using AWS Lambda but its not available in ap-southeast   Also how much would it cost since data will be transferred between regions  
49019544,1,,,2018-02-27 23:18:27,,0,120,"<p>I have a simple lambda function that recursively traverses a tree structure. At each node in the tree, a few database calls are executed (look up to see if the current object exists, possible create and update parent node record with knowledge of child node.) and lambda calls itself (creating a new lambda execution) passing the list of connected child nodes.</p>

<p>the tree we are traversing isn't massive but is big enough to hit our concurrency almost immediately. </p>

<p>what are solutions to throttle the lambda so we avoid concurrency issues. </p>

<p>a few thoughts:</p>

<p>using a queue system: have a first in first out queue that does the work.
- the pain here is you lose the connection to the parent
- size of the message in sqs is very limited.
- will have to watch execution time to make sure it dies before it runs out of time.</p>

<p>using sns to artificially throttle the requests. (This doesn't seem to be the right approach but people have mentioned it online.) adding an sns call adds more processing time (costs more) but still will call the same number of lambdas at a slower pace.
- the pain here is we have a limited size with the message. (we are passing down all children in each node traversal.) 
- it will cost more in the long run throwing in extra processing just to slow things down a bit.</p>

<p>are there other strictly serverless solutions without upping our concurrency abilities? ideas thoughts?</p>
",1008896.0,,,,,2019-03-24 22:10:56,AWS Lambda Concurrency Artificial throttling?,<amazon-web-services><serverless>,1,2,,,,CC BY-SA 3.0,I have a simple lambda function that recursively traverses a tree structure  At each node in the tree  a few database calls are executed (look up to see if the current object exists  possible create and update parent node record with knowledge of child node ) and lambda calls itself (creating a new lambda execution) passing the list of connected child nodes  the tree we are traversing isnt massive but is big enough to hit our concurrency almost immediately   what are solutions to throttle the lambda so we avoid concurrency issues   a few thoughts: using a queue system: have a first in first out queue that does the work  - the pain here is you lose the connection to the parent - size of the message in sqs is very limited  - will have to watch execution time to make sure it dies before it runs out of time  using sns to artificially throttle the requests  (This doesnt seem to be the right approach but people have mentioned it online ) adding an sns call adds more processing time (costs more) but still will call the same number of lambdas at a slower pace  - the pain here is we have a limited size with the message  (we are passing down all children in each node traversal )  - it will cost more in the long run throwing in extra processing just to slow things down a bit  are there other strictly serverless solutions without upping our concurrency abilities  ideas thoughts  
66651066,1,66651169.0,,2021-03-16 07:58:05,,1,262,"<p>I am deploying an application where I am using a NAT gateway with a lambda inside a private subnet to talk to other AWS services outside the vpc. Everything is working fine but the NAT gateway adds alot of extra costs to the billing. I am assuming if I can replace the NAT gateway and use and interface vpc endpoint instead?</p>
",11897758.0,,,,,2021-03-16 08:05:35,can a lambda function inside a private subnet access aws services outside the vpc through vpc endpoints?,<amazon-web-services><aws-lambda><amazon-vpc><aws-nat-gateway><vpc-endpoint>,1,0,,,,CC BY-SA 4.0,I am deploying an application where I am using a NAT gateway with a lambda inside a private subnet to talk to other AWS services outside the vpc  Everything is working fine but the NAT gateway adds alot of extra costs to the billing  I am assuming if I can replace the NAT gateway and use and interface vpc endpoint instead  
49053158,1,,,2018-03-01 15:32:46,,3,2131,"<p>I have a sync job (in Node.js) which has to process several hundred of documents in one batch. For each of them also perform several tasks. As usually, after deployment, such job will become a blackbox: without propper logging it is impossible to find a problem.</p>

<p>Therefore, I log any reasonable information - which document job is being processed, what task is performing now etc. I use console.log / console.error for logging. This results in a very large log file, which is not that big problem when running localy.</p>

<p>Once deployed on AWS, is there any best practice / limitation for logging? I am afraid of costs also.</p>

<p>Thanks!</p>
",8244989.0,,,,,2019-12-17 12:05:15,Amazon AWS Lambda NodeJS logging,<node.js><amazon-web-services><aws-lambda><amazon-cloudwatchlogs>,2,0,,,,CC BY-SA 3.0,I have a sync job (in Node js) which has to process several hundred of documents in one batch  For each of them also perform several tasks  As usually  after deployment  such job will become a blackbox: without propper logging it is impossible to find a problem  Therefore  I log any reasonable information - which document job is being processed  what task is performing now etc  I use console log / console error for logging  This results in a very large log file  which is not that big problem when running localy  Once deployed on AWS  is there any best practice / limitation for logging  I am afraid of costs also  Thanks  
49070843,1,49090081.0,,2018-03-02 14:10:20,,1,92,"<p>As a quick resume i need to build a product feed that ll be constantly updating based on the people searches on site. Important things to have in mind:</p>

<ul>
<li>Prices for the same ID change so i need to always keep the lowest (for a given month)</li>
<li>Every day this feed would be uploaded to different marketing providers for custom ads generation.</li>
</ul>

<p>With all of this in mind i go straight for explaining the possible Architecture i thought (im open and encourage to new ones):</p>

<p>In both i ll be getting the product information making people on site make a request to API gateway with the param of the product and thought proxy implementation to Lambda where i have all the data parsed. After that i can:</p>

<ol>
<li>Store by day on S3 and run a daily EC2 that retrieves all registers from the day before and cross the to a query on a redshift cluster. After all rows that need to be updated are detected update the redshift table.</li>
<li>Use elasticache and in realtime evaluate if the row (by id) needs to be updated (and update it) directly from lambda. </li>
</ol>

<p>My biggest concern is being cost efficient. Thoughts? Any other variable i should consider? Any other solution i should look into?   </p>
",4784639.0,,,,,2018-03-04 11:02:20,AWS Architecture for product feed,<amazon-web-services><amazon-ec2><architecture><aws-lambda><amazon-redshift>,2,0,,,,CC BY-SA 3.0,As a quick resume i need to build a product feed that ll be constantly updating based on the people searches on site  Important things to have in mind:  Prices for the same ID change so i need to always keep the lowest (for a given month) Every day this feed would be uploaded to different marketing providers for custom ads generation   With all of this in mind i go straight for explaining the possible Architecture i thought (im open and encourage to new ones): In both i ll be getting the product information making people on site make a request to API gateway with the param of the product and thought proxy implementation to Lambda where i have all the data parsed  After that i can:  Store by day on S3 and run a daily EC2 that retrieves all registers from the day before and cross the to a query on a redshift cluster  After all rows that need to be updated are detected update the redshift table  Use elasticache and in realtime evaluate if the row (by id) needs to be updated (and update it) directly from lambda    My biggest concern is being cost efficient  Thoughts  Any other variable i should consider  Any other solution i should look into     
42964429,1,42965253.0,,2017-03-22 23:16:25,,0,571,"<p>I have couple of lambda functions and all of these functions have one function common. Since it is common and most of the code is in that common function. So it is logical to consider separate lambda function for that particular function.</p>

<p>But It discouraged me when i think about cost prospective. I mean, invoking one lambda will automatically invoke another lambda. So one event will hire two different resources. and it does not seems me cost effective. Is it so?</p>
",1567877.0,,1190388.0,,2017-08-09 12:08:08,2017-08-09 12:08:08,Is it a good idea to invoke lambda from other lambda in AWS lambda service?,<aws-lambda><aws-sdk><invoke>,1,0,,,,CC BY-SA 3.0,I have couple of lambda functions and all of these functions have one function common  Since it is common and most of the code is in that common function  So it is logical to consider separate lambda function for that particular function  But It discouraged me when i think about cost prospective  I mean  invoking one lambda will automatically invoke another lambda  So one event will hire two different resources  and it does not seems me cost effective  Is it so  
63127037,1,63127523.0,,2020-07-28 04:28:33,,1,516,"<p>Let's say I upload folder/key.jpg to a S3 bucket. How would I trigger a lambda function only when a file contains jpg at the end of File Name, is uploaded?</p>
<p>Is this possible or do I need to check the filename in the function and early-out it doesn't match what I'm looking for?</p>
<p>The reason I ask is that a lot of stuff will be uploaded to the bucket, and it seems inefficient (and costly) for the function to trigger every time.</p>
",2779185.0,,,,,2020-07-28 05:24:16,lambda function triggers only for a upload of filename pattern,<amazon-web-services><amazon-s3><aws-lambda>,3,0,,,,CC BY-SA 4.0,Lets say I upload folder/key jpg to a S3 bucket  How would I trigger a lambda function only when a file contains jpg at the end of File Name  is uploaded  Is this possible or do I need to check the filename in the function and early-out it doesnt match what Im looking for  The reason I ask is that a lot of stuff will be uploaded to the bucket  and it seems inefficient (and costly) for the function to trigger every time  
63130531,1,,,2020-07-28 08:59:26,,0,127,"<p>This is my use case:</p>
<p>I have a JSON Api with 200k objects. The dataset looks a little something like this: date, bike model, production time in min. I use Lambda to read from a JSON Api and write in DynamoDB via http request. The Lambda function runs everyday and updates DynamoDB with the most recent data.</p>
<p>I then retrieve the data by date since I want to calculate the average production time for each day and put it in a second table. An Alexa skill is connected to the second table and reads out the average value for each day.</p>
<p>First question: Since the same bike model is produced multiple times per day, using a composite primary key with date and bike model won't give me a unique key. Shall I create a UUID for the entries instead? Or is there a better solution?</p>
<p>Second question: For the calculation I would need to do a full table scan each time, which is very costly and advised against by many. How can I solve this problem without doing a full table scan?</p>
<p>Third question: Is it better to avoid DynamoDB altogether for my use case? Which AWS database is more suitable for my use case then?</p>
",9274390.0,,9274390.0,,2020-07-28 09:16:46,2020-07-28 10:09:58,DynamoDB - UUID and avoiding a full table scan,<amazon-web-services><aws-lambda><amazon-dynamodb>,2,1,,,,CC BY-SA 4.0,This is my use case: I have a JSON Api with 200k objects  The dataset looks a little something like this: date  bike model  production time in min  I use Lambda to read from a JSON Api and write in DynamoDB via http request  The Lambda function runs everyday and updates DynamoDB with the most recent data  I then retrieve the data by date since I want to calculate the average production time for each day and put it in a second table  An Alexa skill is connected to the second table and reads out the average value for each day  First question: Since the same bike model is produced multiple times per day  using a composite primary key with date and bike model wont give me a unique key  Shall I create a UUID for the entries instead  Or is there a better solution  Second question: For the calculation I would need to do a full table scan each time  which is very costly and advised against by many  How can I solve this problem without doing a full table scan  Third question: Is it better to avoid DynamoDB altogether for my use case  Which AWS database is more suitable for my use case then  
63139851,1,,,2020-07-28 17:40:10,,8,793,"<p>I'm pretty excited about HTTP Gateways due to the drastically reduced pricing in comparison to REST Gateways, but I'm stuck on creating routes that do not completely blow up my <code>serverless.yml</code> file.</p>
<p>The documentation for <a href=""https://www.serverless.com/framework/docs/providers/aws/events/http-api/"" rel=""noreferrer"">HTTP Gateway at Serverless</a> describes this to define routes:</p>
<pre><code>functions:
  params:
    handler: handler.params
    events:
      - httpApi:
          method: GET
          path: /get/for/any/{param}
</code></pre>
<p>There is a support for <code>'*'</code>, but this causes issues with <code>OPTIONS</code> cause those will overwrite the created CORS policies (so OPTIONS requests would actually get to the application, which does not make any sense, especially if the route is protected via an authorizer).</p>
<p>Also, it's not possible to define multiple methods.</p>
<pre><code># does not work
method: GET,POST,DELETE

# also not possible
method:
 - GET
 - POST
 - DELETE
</code></pre>
<p>The only configuration I found is to define all routes separately:</p>
<pre><code>events:
  - httpApi:
      path: /someApi/{proxy+}
      method: GET
  - httpApi:
      path: /someApi/{proxy+}
      method: POST
  - httpApi:
      path: /someApi/{proxy+}
      method: DELETE
</code></pre>
<p>This works fine &amp; the UI is even bundling the routes cause they're on the same prefixed path:</p>
<p><a href=""https://i.stack.imgur.com/s3efw.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/s3efw.png"" alt=""enter image description here"" /></a></p>
<p>But with this I have to define all HTTP methods for all my main resources separately including the attached authorizer.</p>
<p><strong>Is there some way to combine this?</strong></p>
",5908014.0,,5908014.0,,2020-07-29 04:34:29,2020-07-29 04:34:29,AWS HTTP Gateway: Multiple Methods for same Route excluding OPTIONS with Serverless Framework,<amazon-web-services><aws-api-gateway><serverless-framework>,0,1,,,,CC BY-SA 4.0,Im pretty excited about HTTP Gateways due to the drastically reduced pricing in comparison to REST Gateways  but Im stuck on creating routes that do not completely blow up my  file  The documentation for  describes this to define routes:  There is a support for   but this causes issues with  cause those will overwrite the created CORS policies (so OPTIONS requests would actually get to the application  which does not make any sense  especially if the route is protected via an authorizer)  Also  its not possible to define multiple methods   The only configuration I found is to define all routes separately:  This works fine &amp; the UI is even bundling the routes cause theyre on the same prefixed path:  But with this I have to define all HTTP methods for all my main resources separately including the attached authorizer  Is there some way to combine this  
51130235,1,,,2018-07-02 06:31:04,,2,222,"<p><strong>* APOLOGIES for the IMAGES Posted *</strong> </p>

<p>I am looking for potential cost cutting measures for an application that is hosted on AWS with the following configuration / setup.</p>

<ul>
<li>EC2 Instances</li>
<li>Java Dynamic Web Application with Web Server, App Server</li>
<li>RDS - MySQL</li>
</ul>

<p>The application is used 95% of the time for 1 week in a month, and the remaining time 5% distributed over the remaining 3 weeks.</p>

<p>For the sake of cost cutting so as to not pay for the EC2 instances, we are looking to go serverless.</p>

<p><strong>My Background w.r.t. Cloud / AWS:</strong></p>

<ul>
<li>No Previous Professional Working Knowledge in any Cloud Based System except for EC2 where I was managing a Java Web Application with a similar setup as above.</li>
<li>I have only been searching online and reading a lot of websites / blogs.</li>
</ul>

<p>I have prepared a presentation based on the materials I have read online. But I need expert opinion / confirmation that the solution options in the presentation are </p>

<ol>
<li>Possible to Implement</li>
<li>Not Too Complex to implement compared to the EC2 Implementation </li>
<li>Potentially saves money as a generic case, compared to renting EC2 instances</li>
</ol>

<p><strong>The presentations slides are as below:</strong></p>

<hr>

<p><a href=""https://i.stack.imgur.com/9uAro.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9uAro.png"" alt=""enter image description here""></a></p>

<hr>

<p><a href=""https://i.stack.imgur.com/g4AWe.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/g4AWe.png"" alt=""enter image description here""></a></p>

<hr>

<p><a href=""https://i.stack.imgur.com/MhQZe.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MhQZe.png"" alt=""enter image description here""></a></p>

<hr>

<p><a href=""https://i.stack.imgur.com/8LDMd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8LDMd.png"" alt=""enter image description here""></a></p>

<hr>

<p><a href=""https://i.stack.imgur.com/MNrkq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MNrkq.png"" alt=""enter image description here""></a></p>

<hr>

<p><a href=""https://i.stack.imgur.com/H5NfU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/H5NfU.png"" alt=""enter image description here""></a></p>

<hr>

<p><a href=""https://i.stack.imgur.com/30POj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/30POj.png"" alt=""enter image description here""></a></p>

<hr>

<p>I am not limiting to AWS, but since current application is hosted on AWS would like to use the same platform.
But if similar implementations are possible using Azure or other platforms, we are open.</p>

<p>All that we are looking for is to cut costs.</p>

<p>I may be missing to provide some important information for you to help advice me. Please let me know what you need and I will get it for you.</p>

<p>Thanks.</p>

<p>Regards</p>
",619380.0,,619380.0,,2018-07-02 06:44:07,2018-07-10 21:18:42,What are the Possible Serverless System Architectures for Web Application (AWS) for Cost Cutting,<azure><amazon-ec2><aws-lambda><cloud><serverless-framework>,1,2,1.0,,,CC BY-SA 4.0,* APOLOGIES for the IMAGES Posted *  I am looking for potential cost cutting measures for an application that is hosted on AWS with the following configuration / setup   EC2 Instances Java Dynamic Web Application with Web Server  App Server RDS - MySQL  The application is used 95% of the time for 1 week in a month  and the remaining time 5% distributed over the remaining 3 weeks  For the sake of cost cutting so as to not pay for the EC2 instances  we are looking to go serverless  My Background w r t  Cloud / AWS:  No Previous Professional Working Knowledge in any Cloud Based System except for EC2 where I was managing a Java Web Application with a similar setup as above  I have only been searching online and reading a lot of websites / blogs   I have prepared a presentation based on the materials I have read online  But I need expert opinion / confirmation that the solution options in the presentation are   Possible to Implement Not Too Complex to implement compared to the EC2 Implementation  Potentially saves money as a generic case  compared to renting EC2 instances  The presentations slides are as below:                I am not limiting to AWS  but since current application is hosted on AWS would like to use the same platform  But if similar implementations are possible using Azure or other platforms  we are open  All that we are looking for is to cut costs  I may be missing to provide some important information for you to help advice me  Please let me know what you need and I will get it for you  Thanks  Regards 
68998621,1,,,2021-08-31 12:24:35,,0,190,"<p>I have a vue.js/aws-nodejs/mongodb-atlas website.</p>
<p>To lock things down better, I'm switching the mongodb-atlas database to VPC peering with lambda.  That works just fine.  But the other aws services now are giving me problems.  They tend to just hang and never return.</p>
<p>I understand that I should use vpc endpoints specific to the aws services to make them work, but they are not consistently working or do not exist.  Here's what I have:</p>
<ul>
<li>lambda -&gt; aws secret manager using secretmanager endpoint: works fine</li>
<li>lambda -&gt; invoking other lamdas using lambda endpoint: works fine</li>
<li>lambda -&gt; s3 does not work with interface endpoint, but does work with gateway endpoint.</li>
<li>lambda -&gt; aws ses using smtp-email endpoint: hangs</li>
<li>lambda -&gt; aws cognito admin functions (such as adminCreateUser), cannot find a cognito endpoint type:  hangs</li>
</ul>
<p>I have created a separate, non-vpc lambda that calls the SES api.  My vpc lambda invokes this non-vpc lambda with parameters to send email.</p>
<p>This does work, but seems kludgey.   My old non-vpc code worked fine.   Before calling ses or doing anything dangerous, I checked custom permissions in my database.  But this new non-vpc lambda does not have access to the database and therefore is missing this check close to the api call.  This non-vpc lambda feels like potential weapon of mass destruction.</p>
<p>Apparently, I could use a NAT gateway. But a NAT gateway is expensive, especially if I want redundancy.  And using the public internet defeats the purpose of using a vpc in the first place.</p>
<ul>
<li>Why is the smtp-email endpoint not working while secretmanager and lambda endpoints do work?</li>
<li>How can I call cognito admin functions from a vpc-based lambda if there is no cognito endpoint type?</li>
<li>If there is no available endpoint type for a specific aws service (such as cognito), does that mean a NAT gateway is required?</li>
<li>Are lambda functions without an apig interface safe from being invoked by hackers over the internet?</li>
<li>Is using a non-vpc lambda to access aws services from a vpc lambda actually a good idea?</li>
<li>Should I just use a NAT gateway?</li>
</ul>
",10583845.0,,174777.0,,2021-08-31 12:51:28,2021-08-31 12:51:28,AWS Lambda in VPC calling other AWS services with no corresponding endpoint type,<amazon-web-services><aws-lambda><amazon-vpc><endpoint>,0,7,1.0,,,CC BY-SA 4.0,I have a vue js/aws-nodejs/mongodb-atlas website  To lock things down better  Im switching the mongodb-atlas database to VPC peering with lambda   That works just fine   But the other aws services now are giving me problems   They tend to just hang and never return  I understand that I should use vpc endpoints specific to the aws services to make them work  but they are not consistently working or do not exist   Heres what I have:  lambda -&gt; aws secret manager using secretmanager endpoint: works fine lambda -&gt; invoking other lamdas using lambda endpoint: works fine lambda -&gt; s3 does not work with interface endpoint  but does work with gateway endpoint  lambda -&gt; aws ses using smtp-email endpoint: hangs lambda -&gt; aws cognito admin functions (such as adminCreateUser)  cannot find a cognito endpoint type:  hangs  I have created a separate  non-vpc lambda that calls the SES api   My vpc lambda invokes this non-vpc lambda with parameters to send email  This does work  but seems kludgey    My old non-vpc code worked fine    Before calling ses or doing anything dangerous  I checked custom permissions in my database   But this new non-vpc lambda does not have access to the database and therefore is missing this check close to the api call   This non-vpc lambda feels like potential weapon of mass destruction  Apparently  I could use a NAT gateway  But a NAT gateway is expensive  especially if I want redundancy   And using the public internet defeats the purpose of using a vpc in the first place   Why is the smtp-email endpoint not working while secretmanager and lambda endpoints do work  How can I call cognito admin functions from a vpc-based lambda if there is no cognito endpoint type  If there is no available endpoint type for a specific aws service (such as cognito)  does that mean a NAT gateway is required  Are lambda functions without an apig interface safe from being invoked by hackers over the internet  Is using a non-vpc lambda to access aws services from a vpc lambda actually a good idea  Should I just use a NAT gateway   
69017065,1,,,2021-09-01 15:53:38,,0,304,"<p>Is there any way to manually kick off an Amazon S3 Inventory report job?</p>
<p>I'm working on a project that creates daily inventory reports to another account but I can't seem to find a way to manually kick off the run. We're in the design / development phase of a data telemetry project and are tweaking our inventory configurations but having to wait for the daily job to run to see if the configuration satisfies our requirements is really inconvenient and slowing us down.</p>
<p>Is there a way to manually kick off an inventory report run after making a configuration change? I've tried looking in the api documentation as well as the boto3 documentation and all I have found is a call to create a bucket inventory configuration but nothing to actually perform a run.</p>
<p>Thanks,
Bill</p>
",5823586.0,,1435543.0,,2021-09-02 16:28:22,2021-09-02 16:28:22,Manually trigger an Amazon S3 Inventory Report,<amazon-web-services><amazon-s3><aws-lambda>,1,0,,,,CC BY-SA 4.0,Is there any way to manually kick off an Amazon S3 Inventory report job  Im working on a project that creates daily inventory reports to another account but I cant seem to find a way to manually kick off the run  Were in the design / development phase of a data telemetry project and are tweaking our inventory configurations but having to wait for the daily job to run to see if the configuration satisfies our requirements is really inconvenient and slowing us down  Is there a way to manually kick off an inventory report run after making a configuration change  Ive tried looking in the api documentation as well as the boto3 documentation and all I have found is a call to create a bucket inventory configuration but nothing to actually perform a run  Thanks  Bill 
69193696,1,69323202.0,,2021-09-15 13:03:07,,0,329,"<p><strong>edit: Turns out the solution is in the docs. I had bog standard normal 'sam' installed but I needed what they call the 'public preview version' AKA 'sam-beta-cdk'. With this installed the API can be started locally with <code>sam-betacdk start-api</code> and works well. While I appreciate the answers which suggest that development should be done using purely TDD I feel there is also value in this more interactive, manual mode as it permits quicker exploration of the problem space.</strong></p>
<p>I'm trying to build my first app with CDK + Typescript using API Gateway, Lambdas and DynamoDB. I have built a couple of Lambdas and deployed them and they work fine live on the web. However I don't want a minute long deploy cycle and various associated AWS costs as part of my workflow. What I want is to be able to test my API locally.</p>
<p>I have struggled to find docs on how to do this. Amazon seem to recommend using the SAM CLI <a href=""https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-cdk-testing.html"" rel=""nofollow noreferrer"">here</a> so that is what I've been trying.</p>
<p>The docs claim running <code>sam local xyz</code> runs <code>cdk synth</code> to make a &quot;could assembly&quot; in <code>./aws-sam/build</code> but I see no evidence of this. Instead what I get is a complaint that sam could not find a 'template.yml'. So I manually run <code>cdk synth &gt; template.yml</code> which creates one in the root folder. Then I run <code>sam local start-api</code> and it seems happy to start up.</p>
<p>Then I try and hit my test lambda using CURL: <code>curl 'http://127.0.0.1:3000/test'</code> I get <code>{&quot;message&quot;:&quot;Internal server error&quot;}</code> and a huge ugly stack trace in the console that is running <code>sam local start-api</code></p>
<p>The lambda is this...</p>
<pre><code>exports.handler = async function() {
    console.log(&quot;WooHoo! Test handler ran&quot;)
    return {statusCode: 200, headers: {&quot;Content-Type&quot;: &quot;application/json&quot;}, body: &quot;Test handler ran!&quot;}
}
</code></pre>
<p>Start of the huge ugly stack trace...</p>
<pre><code>Mounting /home/user/code/image-cache/asset.beeaa749e012b5921018077f0a5e4fc3ab271ef1c191bd12a82aa9a92148782e as /var/task:ro,delegated inside runtime container
START RequestId: 99f53642-b294-4ce5-a1b4-8c967db80ce1 Version: $LATEST
2021-09-15T12:33:37.086Z    undefined   ERROR   Uncaught Exception  {&quot;errorType&quot;:&quot;Runtime.ImportModuleError&quot;,&quot;errorMessage&quot;:&quot;Error: Cannot find module 'test'\nRequire stack:\n- /var/runtime/UserFunction.js\n- /var/runtime/index.js&quot;,&quot;stack&quot;:[&quot;Runtime.ImportModuleError: Error: Cannot find module 'test'&quot;,&quot;Require stack:&quot;,&quot;- /var/runtime/UserFunction.js&quot;,&quot;- /var/runtime/index.js&quot;,&quot;    at _loadUserApp (/var/runtime/UserFunction.js:100:13)&quot;,&quot;    at Object.module.exports.load (/var/runtime/UserFunction.js:140:17)&quot;,
</code></pre>
<p>The end of the huge ugly stack trace...</p>
<pre><code>Invalid lambda response received: Lambda response must be valid json
</code></pre>
<p>So it would seem <code>sam local start-api</code> can't find <code>test</code> and throws and error which means the API gateway doesn't get a valid 'lambda response'. So far this has not helped me chase down the problem :/ It certainly seems aware that test is a route, as trying to hit other endpoints gives the classic <code>{&quot;message&quot;:&quot;Missing Authentication Token&quot;}</code> but it chokes hard trying to fulfill it despite me having both <code>functions/test.ts</code> and the compiled <code>functions/test.js</code> present.</p>
<p>I have the test route and handler defined in my CDK stack definition like so...</p>
<pre><code>    const testLambda = new lambda.Function(this, &quot;testLambdaHandler&quot;, {
      runtime: lambda.Runtime.NODEJS_14_X,
      code: lambda.Code.fromAsset(&quot;functions&quot;),
      handler: &quot;test.handler&quot;
    })

    api.root
      .resourceForPath(&quot;test&quot;)
      .addMethod(&quot;GET&quot;, new apigateway.LambdaIntegration(testLambda))

</code></pre>
<p>I considered posting my template.yml but that is even longer than the big ugly error message so I haven't.</p>
<p>So I have three questions (well actually a million but I don't want to be too cheeky!)</p>
<ol>
<li>Is this actually the canonical way of locally testing apps made with CDK</li>
<li>If so, where am I going wrong?</li>
<li>If not, what is the better/proper way?</li>
</ol>
",201436.0,,201436.0,,2021-10-04 17:35:26,2021-10-04 17:35:26,AWS CDK - How to run API and Lambdas locally?,<amazon-web-services><aws-lambda><aws-cdk><aws-sam-cli>,2,5,,,,CC BY-SA 4.0,edit: Turns out the solution is in the docs  I had bog standard normal sam installed but I needed what they call the public preview version AKA sam-beta-cdk  With this installed the API can be started locally with  and works well  While I appreciate the answers which suggest that development should be done using purely TDD I feel there is also value in this more interactive  manual mode as it permits quicker exploration of the problem space  Im trying to build my first app with CDK + Typescript using API Gateway  Lambdas and DynamoDB  I have built a couple of Lambdas and deployed them and they work fine live on the web  However I dont want a minute long deploy cycle and various associated AWS costs as part of my workflow  What I want is to be able to test my API locally  I have struggled to find docs on how to do this  Amazon seem to recommend using the SAM CLI  so that is what Ive been trying  The docs claim running  runs  to make a could assembly in  but I see no evidence of this  Instead what I get is a complaint that sam could not find a template yml  So I manually run  which creates one in the root folder  Then I run  and it seems happy to start up  Then I try and hit my test lambda using CURL:  I get  and a huge ugly stack trace in the console that is running  The lambda is this     Start of the huge ugly stack trace     The end of the huge ugly stack trace     So it would seem  cant find  and throws and error which means the API gateway doesnt get a valid lambda response  So far this has not helped me chase down the problem :/ It certainly seems aware that test is a route  as trying to hit other endpoints gives the classic  but it chokes hard trying to fulfill it despite me having both  and the compiled  present  I have the test route and handler defined in my CDK stack definition like so     I considered posting my template yml but that is even longer than the big ugly error message so I havent  So I have three questions (well actually a million but I dont want to be too cheeky )  Is this actually the canonical way of locally testing apps made with CDK If so  where am I going wrong  If not  what is the better/proper way   
52298332,1,52300162.0,,2018-09-12 15:07:21,,0,156,"<p>I'm a developer on a startup and right now we are using around 30 cronjobs, some of them run each minute, others run once per day while other run on specific days. The problem are the ones that run every minute, when most of the time is not necessary.</p>

<p>This somewhat increases our expenses because during the night, they still run when most of the times our services have nobody online (and don't require to be run).</p>

<p>We have been talking about using AWS to replace those cronjobs into something like event based. Yet, I cannot find a solution. Here's an example of one of our cronjobs:</p>

<ul>
<li>One costumer starts to make a registration and has 8 minutes to complete it. Right now, we have a cronjob that runs every minute to validate if he completed, and if not, to ""delete"" it.</li>
</ul>

<p>I though I could replace this with a SNS + Lambda event. Basically, when an user starts registration, send an message to SNS, that would triger a lambda function. Yet, it could only run after 8 minutes, and not instantly.</p>

<p>I've seen on SNS that we can delay up to 15 minutes, but we got some other service that sends an email after few hours, which would not work</p>

<p>Anyone have a clue on how can I do it?</p>

<p>Thanks</p>
",3458970.0,,3458970.0,,2018-09-12 15:15:54,2018-09-12 16:57:11,Alternative to Cronjob using AWS Lambda + Cloudwatch (),<amazon-web-services><cron><aws-lambda><amazon-cloudwatch>,1,0,,,,CC BY-SA 4.0,Im a developer on a startup and right now we are using around 30 cronjobs  some of them run each minute  others run once per day while other run on specific days  The problem are the ones that run every minute  when most of the time is not necessary  This somewhat increases our expenses because during the night  they still run when most of the times our services have nobody online (and dont require to be run)  We have been talking about using AWS to replace those cronjobs into something like event based  Yet  I cannot find a solution  Heres an example of one of our cronjobs:  One costumer starts to make a registration and has 8 minutes to complete it  Right now  we have a cronjob that runs every minute to validate if he completed  and if not  to delete it   I though I could replace this with a SNS + Lambda event  Basically  when an user starts registration  send an message to SNS  that would triger a lambda function  Yet  it could only run after 8 minutes  and not instantly  Ive seen on SNS that we can delay up to 15 minutes  but we got some other service that sends an email after few hours  which would not work Anyone have a clue on how can I do it  Thanks 
71469455,1,,,2022-03-14 14:33:32,,0,16,"<p>Let's say a user needs some information from a database. When the user clicks a button, a request is routed through cloudfront to lambda at edge, which puts the request on an SQS queue. The SQS sends a batch of requests over to a lambda function. For each request, the lambda pulls relevant information from a dynamodb instance and returns it back to the client.</p>
<p>Is this flow possible? What's missing to me here is how the request is actually routed back to the client session. I've worked with API-Gateways where requests are sent to methods. I assume API-Gateway handles the routing here, but it isn't clear to me what exactly is happening when a request goes through cloudfront.</p>
<p>If the flow I first shared doesn't make sense, how would you set up a flow to handle 10,000 or 20,000 of these requests a minute? It seems having lambda handle requests in batches is the only possible way to do this with serverless, individual lambda instances just screw you on the start up/spin down times and costs</p>
",9904934.0,,,,,2022-03-14 14:33:32,How does Cloudfront/SQS/Lambda handle returning requests back to users?,<amazon-web-services><aws-lambda><amazon-cloudfront><amazon-sqs>,0,4,,,,CC BY-SA 4.0,Lets say a user needs some information from a database  When the user clicks a button  a request is routed through cloudfront to lambda at edge  which puts the request on an SQS queue  The SQS sends a batch of requests over to a lambda function  For each request  the lambda pulls relevant information from a dynamodb instance and returns it back to the client  Is this flow possible  Whats missing to me here is how the request is actually routed back to the client session  Ive worked with API-Gateways where requests are sent to methods  I assume API-Gateway handles the routing here  but it isnt clear to me what exactly is happening when a request goes through cloudfront  If the flow I first shared doesnt make sense  how would you set up a flow to handle 10 000 or 20 000 of these requests a minute  It seems having lambda handle requests in batches is the only possible way to do this with serverless  individual lambda instances just screw you on the start up/spin down times and costs 
55244420,1,,,2019-03-19 15:19:39,,4,3173,"<p>I'm planning to move my lambda function configuration from environment variables to DynamoDb since my lambda functions share configurations and also I don't want to redeploy my lambda functions each time a configuration changes (my config changes frequently and once they do I have to redeploy so many lambda functions).</p>

<p>But in order to improve the performance of my lambda functions and also to reduce the cost, I'm not going to load the configuration per each execution. Instead, I will load the config into a global variable and since global variables persist across executions (as long as the lambda function is warmed up) I can reuse the same config without accessing DynamoDb. Here's a sample code:</p>

<pre><code>let config = null;
function getConfig() {
    if (config)
        return Promise.resolve(config);
    else {
        //Load config from DynamoDb and return it in form of a promise
    }
}

exports.handler = function(event, context, callback) {   
    getConfig()
        .then(config =&gt; {
            //Your code that makes use of config
        })
}
</code></pre>

<p>So far everything is fine. Now, consider the time that DynamoDb is updated with the new configuration. The warmed-up lambda functions will continue using the old config before they are brought down by AWS and face a cold-start.</p>

<p>What I want to do is to signal lambda functions and force them to flush their warmed-up lambda functions and start over each time the configuration changes. I know I can redeploy them which will do exactly what I wanted. But that's what exactly I was escaping from in the first place. So, what are my options?</p>
",866082.0,,866082.0,,2019-03-19 15:25:40,2019-03-19 15:29:59,How to force a reload for lambda function's global variables?,<amazon-web-services><configuration><aws-lambda><cold-start>,1,0,2.0,,,CC BY-SA 4.0,Im planning to move my lambda function configuration from environment variables to DynamoDb since my lambda functions share configurations and also I dont want to redeploy my lambda functions each time a configuration changes (my config changes frequently and once they do I have to redeploy so many lambda functions)  But in order to improve the performance of my lambda functions and also to reduce the cost  Im not going to load the configuration per each execution  Instead  I will load the config into a global variable and since global variables persist across executions (as long as the lambda function is warmed up) I can reuse the same config without accessing DynamoDb  Heres a sample code:  So far everything is fine  Now  consider the time that DynamoDb is updated with the new configuration  The warmed-up lambda functions will continue using the old config before they are brought down by AWS and face a cold-start  What I want to do is to signal lambda functions and force them to flush their warmed-up lambda functions and start over each time the configuration changes  I know I can redeploy them which will do exactly what I wanted  But thats what exactly I was escaping from in the first place  So  what are my options  
57566552,1,57571012.0,,2019-08-20 04:01:14,,1,106,"<p>I am having long cold start times on our Lambda functions. We have tried ""pinging"" the Lambdas to keep them warm but that can get costly and seems like a poor way to keep performance up. We also have an EC2 instance running 24/7. I could theoretically ""mirror"" all of our Lambda functions to our EC2 instance to respond with the same data for our API call. Our Lambdas are on <a href=""https://api.mysite.com"" rel=""nofollow noreferrer"">https://api.mysite.com</a> and our EC2 is <a href=""https://dev.mysite.com"" rel=""nofollow noreferrer"">https://dev.mysite.com</a>.</p>

<p>My question is could we <strong>""load balance?"" traffic between the two</strong>. (Create a new subdomain to do the following) To have our dev subdomain (EC2) respond to all requests up until a certain ""requests per minute"" is hit. Then start to route traffic to our dev subdomain (Lambda) since we have enough traffic coming in to keep the Lambdas hot. Once traffic slows down, we move the traffic back over to our EC2.. Is this possible? </p>
",270760.0,,,,,2019-08-20 11:52:35,EC2 to Lambda forwarding based on current usage,<amazon-web-services><amazon-ec2><aws-lambda><load-balancing><cold-start>,2,3,,,,CC BY-SA 4.0,I am having long cold start times on our Lambda functions  We have tried pinging the Lambdas to keep them warm but that can get costly and seems like a poor way to keep performance up  We also have an EC2 instance running 24/7  I could theoretically mirror all of our Lambda functions to our EC2 instance to respond with the same data for our API call  Our Lambdas are on  and our EC2 is   My question is could we load balance  traffic between the two  (Create a new subdomain to do the following) To have our dev subdomain (EC2) respond to all requests up until a certain requests per minute is hit  Then start to route traffic to our dev subdomain (Lambda) since we have enough traffic coming in to keep the Lambdas hot  Once traffic slows down  we move the traffic back over to our EC2   Is this possible   
57569095,1,57569202.0,,2019-08-20 07:59:47,,2,690,"<p>I have a lambda function that gets triggered every time a file is written onto an S3 bucket. My understanding is that every time a single file gets in (this is a potential scenario, rather than having a batch of files being sent), an API call is fired up and that means that I am charged. My question is: can I batch multiple files so that each API calls will only be called if, for example, I have a batch of 10 files? Is this a good practice? I should not be in the position of having a processing time greater than 15 minutes, so the use of the lambda is still fine. </p>

<p>Thank you</p>
",397940.0,,,,,2019-08-20 11:40:59,Lambda function and triggered S3 events,<amazon-s3><aws-lambda><batch-processing>,2,0,2.0,,,CC BY-SA 4.0,I have a lambda function that gets triggered every time a file is written onto an S3 bucket  My understanding is that every time a single file gets in (this is a potential scenario  rather than having a batch of files being sent)  an API call is fired up and that means that I am charged  My question is: can I batch multiple files so that each API calls will only be called if  for example  I have a batch of 10 files  Is this a good practice  I should not be in the position of having a processing time greater than 15 minutes  so the use of the lambda is still fine   Thank you 
40829263,1,,,2016-11-27 13:02:57,,2,542,"<p>I have the following situation that I try to find the best solution for. 
A device writes its GPS coordinates every second to a csv file and uploads the file every x minutes to s3 before starting a new csv.</p>

<p>Later I want to be able to get the GPS data for a specific time period e.g 2016-11-11 8am  until 2016-11-11 2pm</p>

<p>Here are two solutions that I am currently considering:</p>

<ol>
<li><p>Use a lambda function to automatically save the csv data to a dynamoDB record</p></li>
<li><p>Only save the metadata (csv gps timestamp-start, timestamp-end, s3Filename) in dynamoDB and then request the files directly from s3.</p></li>
</ol>

<p>However both solutions seem to have a major drawback:</p>

<ol>
<li><p>The gps data uses about 40 bytes per record (second). So if I use 10min chunks this will result in a 24 kB file. dynamoDB charges write capacities by item size (1 write capacity unit = 1 kB). So this would require 24 units for a single write. Reads (4kB/unit) are even worse since a user may request timeframes greater than 10 min. So for a request covering e.g. 6 hours (=864kB) it would require a read capacity of 216. This will just be too expensive considering multiple users.</p></li>
<li><p>When I read directly from S3 I face the browser limiting the number of concurrent requests. The 6 hour timespan for instance would cover 36 files. This might still be acceptable, considering a connection limit of 6. But a request for 24 hours (=144 files) would just take too long.</p></li>
</ol>

<p>Any idea how to solve the problem?</p>

<p>best regards, Chris</p>
",5661749.0,,1022807.0,,2017-03-24 15:08:59,2017-03-24 15:08:59,s3 vs dynamoDB for gps data,<amazon-web-services><amazon-s3><amazon-dynamodb><aws-lambda>,2,0,,,,CC BY-SA 3.0,I have the following situation that I try to find the best solution for   A device writes its GPS coordinates every second to a csv file and uploads the file every x minutes to s3 before starting a new csv  Later I want to be able to get the GPS data for a specific time period e g 2016-11-11 8am  until 2016-11-11 2pm Here are two solutions that I am currently considering:  Use a lambda function to automatically save the csv data to a dynamoDB record Only save the metadata (csv gps timestamp-start  timestamp-end  s3Filename) in dynamoDB and then request the files directly from s3   However both solutions seem to have a major drawback:  The gps data uses about 40 bytes per record (second)  So if I use 10min chunks this will result in a 24 kB file  dynamoDB charges write capacities by item size (1 write capacity unit = 1 kB)  So this would require 24 units for a single write  Reads (4kB/unit) are even worse since a user may request timeframes greater than 10 min  So for a request covering e g  6 hours (=864kB) it would require a read capacity of 216  This will just be too expensive considering multiple users  When I read directly from S3 I face the browser limiting the number of concurrent requests  The 6 hour timespan for instance would cover 36 files  This might still be acceptable  considering a connection limit of 6  But a request for 24 hours (=144 files) would just take too long   Any idea how to solve the problem  best regards  Chris 
59299996,1,59300317.0,,2019-12-12 07:57:34,,1,294,"<p>AWS <a href=""https://aws.amazon.com/blogs/compute/announcing-http-apis-for-amazon-api-gateway/"" rel=""nofollow noreferrer"">just announced</a> HTTP API support for Amazon API Gateway. There are some very impressive pricing and performance numbers coming in with this new release. Off the top, AWS says that the general cost of using v2 will be 70% cheaper and have 50% lower latency than v1. I'd love to try this out in my existing projects.</p>

<p>I use Serverless framework in my application. How do I convert my existing API to use this new feature? This is what my <code>serverless.yml</code> file looks like:</p>

<pre><code>service: amitsn-blog-api

# Use the serverless-webpack plugin to transpile ES6
plugins:
  - serverless-webpack
  - serverless-offline

# serverless-webpack configuration
# Enable auto-packing of external modules
custom:
  webpack:
    webpackConfig: ./webpack.config.js
    includeModules: true

provider:
  name: aws
  runtime: nodejs10.x
  stage: prod
  region: ap-south-1

  # 'iamRoleStatements' defines the permission policy for the Lambda function.
  # In this case Lambda functions are granted with permissions to access DynamoDB.
  iamRoleStatements:
    - Effect: Allow
      Action:
        - dynamodb:DescribeTable
        - dynamodb:Query
        - dynamodb:Scan
        - dynamodb:GetItem
        - dynamodb:PutItem
      Resource: ""arn:aws:dynamodb:ap-south-1:*:*""
    - Effect: Deny
      Action:
        - logs:CreateLogGroup
        - logs:CreateLogStream
        - logs:PutLogEvents
      Resource: ""*""

functions:
  # Defines an HTTP API endpoint that calls the main function in create.js
  # - path: url path is /posts
  # - method: POST request
  # - cors: enabled CORS (Cross-Origin Resource Sharing) for browser cross
  #     domain api call
  # - authorizer: authenticate using the AWS IAM role
  options:
    handler: options.main
    events:
      - http:
          path: posts
          method: options
          cors: true
  create:
    handler: create.main
    events:
      - http:
          path: posts
          method: post
          cors: true
          authorizer: aws_iam
  get:
  # Defines an HTTP API endpoint that calls the main function in get.js
  # - path: url path is /posts/{id}
  # - method: GET request
    handler: get.main
    events:
      - http:
          path: posts/{id}
          method: get
          cors: true
  list:
    # Defines an HTTP API endpoint that calls the main function in list.js
    # - path: url path is /posts
    # - method: GET request
    handler: list.main
    events:
      - http:
          path: posts
          method: get
          cors: true
          integration: lambda
          request:
            template:
              application/json: '{ ""postType"" : ""$input.params(''postType'')"" }'
  update:
    # Defines an HTTP API endpoint that calls the main function in update.js
    # - path: url path is /posts/{id}
    # - method: PUT request
    handler: update.main
    events:
      - http:
          path: posts/{id}
          method: put
          cors: true
          authorizer: aws_iam
  delete:
    # Defines an HTTP API endpoint that calls the main function in delete.js
    # - path: url path is /posts/{id}
    # - method: DELETE request
    handler: delete.main
    events:
      - http:
          path: posts/{id}
          method: delete
          cors: true
          authorizer: aws_iam

# Create our resources with separate CloudFormation templates
resources:
  # API Gateway Errors
  - ${file(resources/api-gateway-errors.yml)}
</code></pre>
",1414578.0,,1414578.0,,2019-12-12 08:03:59,2019-12-12 08:20:54,Serverless Framework - Switch existing application from REST to HTTP in AWS API Gateway,<amazon-web-services><aws-api-gateway><serverless-framework><serverless><aws-serverless>,1,0,,,,CC BY-SA 4.0,AWS  HTTP API support for Amazon API Gateway  There are some very impressive pricing and performance numbers coming in with this new release  Off the top  AWS says that the general cost of using v2 will be 70% cheaper and have 50% lower latency than v1  Id love to try this out in my existing projects  I use Serverless framework in my application  How do I convert my existing API to use this new feature  This is what my  file looks like:  
51168215,1,,,2018-07-04 07:34:54,,0,695,"<p>I created a Lambda function for deleting a given thumbnail and I set a trigger on the ObjectRemoved event in order to automatically delete a thumbnail image when the original file was deleted from a given aws-S3 bucket.</p>

<p>However, by analyzing the monthly bill I realized that for some reason that Lambda was called hundred millions of times and wouldn't stop to be triggered. I had to disable the trigger on the Lambda to disable it.</p>

<p>The problem is I have not created or deleted any file on that bucket, so I wonder how it's possible the lambda function continued to be triggered continuously.</p>

<p>Any help is appreciated.</p>

<p>Thanks.</p>

<p>Edit:</p>

<p>My AWS Lambda code</p>

<pre class=""lang-js prettyprint-override""><code>var aws = require('aws-sdk');
var s3 = new aws.S3();
exports.handler = function (event, context) {
    console.log('Received event:', JSON.stringify(event, null, 2));

    // Get the object from the event and show its content type
    const bucket = event.Records[0].s3.bucket.name;
    const key = event.Records[0].s3.object.key;
    const path = key.split('/');
    const folder = path[0];
    const fileName = path[1];
    const deleteKey = folder + '/thumbnails/' + fileName;

    s3.deleteObject({ Bucket: bucket, Key: deleteKey }, function (err, data) {
        if (err) {
            console.log('Error deleting object ' + deleteKey + ' from bucket ' + bucket + '. Make sure they exist and your bucket is in the same region as this function.');
            context.fail('Error getting file: ' + err)
        } else {
            context.succeed();
        }
    });
};
</code></pre>
",9999361.0,,3893262.0,,2018-08-31 14:27:19,2018-08-31 14:27:19,Lambda function triggered continuously by ObjectRemoved event,<amazon-web-services><amazon-s3><triggers><aws-lambda>,0,16,,,,CC BY-SA 4.0,I created a Lambda function for deleting a given thumbnail and I set a trigger on the ObjectRemoved event in order to automatically delete a thumbnail image when the original file was deleted from a given aws-S3 bucket  However  by analyzing the monthly bill I realized that for some reason that Lambda was called hundred millions of times and wouldnt stop to be triggered  I had to disable the trigger on the Lambda to disable it  The problem is I have not created or deleted any file on that bucket  so I wonder how its possible the lambda function continued to be triggered continuously  Any help is appreciated  Thanks  Edit: My AWS Lambda code  
69042070,1,,,2021-09-03 08:50:24,,1,137,"<p>I am stuck in my current situation: I have a regular Aurora MySQL RDS cluster and some apps that make use of it (some query the database constantly), now I also have a Next.js application hosted on Vercel, this moment I realised I cannot connect to the database in my serverless functions.</p>
<p>How can I deal with this situation? I figured one easy way would be to migrate to Aurora serverless but I think that will be expensive because of the apps that constantly query the database (both the serverless function and apps should access the same data)</p>
",,user15485638,,,,2021-09-03 08:50:24,How to deal with an existing Aurora MySQL database and serverless functions?,<amazon-web-services><next.js><amazon-rds><serverless><vercel>,0,5,,,,CC BY-SA 4.0,I am stuck in my current situation: I have a regular Aurora MySQL RDS cluster and some apps that make use of it (some query the database constantly)  now I also have a Next js application hosted on Vercel  this moment I realised I cannot connect to the database in my serverless functions  How can I deal with this situation  I figured one easy way would be to migrate to Aurora serverless but I think that will be expensive because of the apps that constantly query the database (both the serverless function and apps should access the same data) 
35108295,1,,,2016-01-30 23:03:00,,7,1504,"<p>I'd like to do Universal rendering of react components using serverless, react-router, lambda, API gateway and Cloudflare. How would you map the API gateway endpoint URLs to a server-side instance of react-router running inside an AWS lambda function in a way that was compatible with Cloudflare?</p>

<p>I found an interesting hack/approach to doing it with CloudFront ( <a href=""http://highscalability.com/blog/2015/12/7/the-serverless-start-up-down-with-servers.html"" rel=""nofollow noreferrer"">http://highscalability.com/blog/2015/12/7/the-serverless-start-up-down-with-servers.html</a> ), but I'd prefer using Cloudflare for cost and DDoS attack prevention reasons.</p>

<p>Thanks!</p>
",217406.0,,5891719.0,,2020-02-21 04:38:28,2020-02-21 04:38:28,"Universal rendering react components using serverless, react-router, lambda, api gateway and cloudflare",<react-router><cloudflare><aws-lambda><aws-api-gateway><serverless-framework>,0,7,1.0,,,CC BY-SA 4.0,Id like to do Universal rendering of react components using serverless  react-router  lambda  API gateway and Cloudflare  How would you map the API gateway endpoint URLs to a server-side instance of react-router running inside an AWS lambda function in a way that was compatible with Cloudflare  I found an interesting hack/approach to doing it with CloudFront (  )  but Id prefer using Cloudflare for cost and DDoS attack prevention reasons  Thanks  
51202756,1,,,2018-07-06 03:49:18,,15,6734,"<p>I have an AWS SQS queue that I'm going to setup with a Lambda function trigger to run the Lambda function for every item that gets added to the queue to do some processing work.</p>

<p>One step of processing is going to be hitting an API endpoint to get some data back for every item added to the queue, then storing that away in an DynamoDB table. In order to manage costs, and stay in control of this process I'm wanting to throttle the amount of times this Lambda function gets called in a certain period of time.</p>

<p>For example, I want this function to be run a maximum of 100 times per day, in order to not overwhelm the DynamoDB table capacity or the API endpoint. It would also be nice to throttle it, to where it will only have a maximum of 5 concurrent actions being run at once, with a 1 second delay between running again. This type of control would allow me to directly map the function to the DynamoDB table limits to ensure I'm not going over capacity, and to I comply with any API rate limits. </p>

<p>I have looked into <a href=""https://docs.aws.amazon.com/lambda/latest/dg/concurrent-executions.html#per-function-concurrency"" rel=""noreferrer"">AWS Lambda Managing Concurrency</a>. Specifically the ""Function Level Concurrent Execution Limit"" section. But this section doesn't seem to address the 100 times per day limit, or the 1 second delay between running the next function.</p>

<p>I also know that I could just limit it on the other side, by limiting the number of items I put in the queue at a time (or per day). But because of how I'm planning this system, that would add a lot of complexity that I would love to try to avoid.</p>

<p>Is there a way using AWS SQS and Lambda to achieve this and limit these Lambda functions?</p>
",894067.0,,,,,2018-07-06 07:33:34,AWS Lambda SQS Trigger Throttle/Limit,<amazon-web-services><aws-lambda><amazon-sqs>,1,4,1.0,,,CC BY-SA 4.0,I have an AWS SQS queue that Im going to setup with a Lambda function trigger to run the Lambda function for every item that gets added to the queue to do some processing work  One step of processing is going to be hitting an API endpoint to get some data back for every item added to the queue  then storing that away in an DynamoDB table  In order to manage costs  and stay in control of this process Im wanting to throttle the amount of times this Lambda function gets called in a certain period of time  For example  I want this function to be run a maximum of 100 times per day  in order to not overwhelm the DynamoDB table capacity or the API endpoint  It would also be nice to throttle it  to where it will only have a maximum of 5 concurrent actions being run at once  with a 1 second delay between running again  This type of control would allow me to directly map the function to the DynamoDB table limits to ensure Im not going over capacity  and to I comply with any API rate limits   I have looked into   Specifically the Function Level Concurrent Execution Limit section  But this section doesnt seem to address the 100 times per day limit  or the 1 second delay between running the next function  I also know that I could just limit it on the other side  by limiting the number of items I put in the queue at a time (or per day)  But because of how Im planning this system  that would add a lot of complexity that I would love to try to avoid  Is there a way using AWS SQS and Lambda to achieve this and limit these Lambda functions  
69063329,1,,,2021-09-05 12:19:35,,0,35,"<p>Let's say I have about 20 different types of resources in my dynamodb, which are premium features. Now for each resource I want to validate access of a particular user, I mean an user &quot;buys&quot; access to particular resource - how should I store its access? In. e.g. dynamodb table, and search the table for every user request to a particular resource? Or e.g. should I use Cognito and custom attributes? And store e.g. resource id and timestamp till a particular user has access? Or create a group in cognito for each resource and check if a particular user is assigned to the group? Or is there any other good, cheap way to validate user access?</p>
",4643919.0,,,,,2021-09-05 12:43:54,Api Gateway + Cognito + Lambda + DynamoDB user access validation,<amazon-web-services><aws-lambda><amazon-dynamodb><aws-api-gateway><amazon-cognito>,1,0,,,,CC BY-SA 4.0,Lets say I have about 20 different types of resources in my dynamodb  which are premium features  Now for each resource I want to validate access of a particular user  I mean an user buys access to particular resource - how should I store its access  In  e g  dynamodb table  and search the table for every user request to a particular resource  Or e g  should I use Cognito and custom attributes  And store e g  resource id and timestamp till a particular user has access  Or create a group in cognito for each resource and check if a particular user is assigned to the group  Or is there any other good  cheap way to validate user access  
51210445,1,51291486.0,,2018-07-06 12:25:12,,17,6938,"<p>On my project there is REST API which implemented on AWS API Gateway and AWS Lambda. As AWS Lambda functions are serverless and stateless while we make a call to it, AWS starts a container with code of the Lambda function which process our call. According <a href=""https://aws.amazon.com/ru/blogs/compute/container-reuse-in-lambda/"" rel=""noreferrer"">AWS documentation</a> after finishing of lambda function execution AWS don't stop the container and we are able to process next call in that container. Such approach improves performance of the service - only in time of first call AWS spend time to start container (cold start of Lambda function) and all next calls are executed faster because their use the same container (warm starts).</p>

<p>As a next step for improving the performance we created cron job which calls periodically our Lambda function (we use Cloudwatch rules for that). Such approach allow to keep Lambda function ""warm"" allowing to avoid stopping and restarting of containers. I.e. when the real user will call our REST API, Lambda will not spent time to start a new container.</p>

<p>But we faced with the issue - such approach allow to keep warm only one container of Lambda function while the actual number of parallel calls from different users can be much larger (in our case that's hundreds and sometimes even thousands of users). Is there any way to implement warm up functionality for Lambda function which could warm not only single container, but some desired number of them?</p>

<p>I understand that such approach can affect cost of Lambda function's using and possibly, at all it will be better to use good old application server, but comparison of these approaches and their costs will be the next steps, I think, and in current moment I would like just to find the way to warm desired count of Lambda function containers.</p>
",1145792.0,,,,,2019-12-05 09:58:24,How to keep desired amount of AWS Lambda function containers warm,<performance><amazon-web-services><aws-lambda><amazon-cloudwatch><cold-start>,5,8,9.0,,,CC BY-SA 4.0,On my project there is REST API which implemented on AWS API Gateway and AWS Lambda  As AWS Lambda functions are serverless and stateless while we make a call to it  AWS starts a container with code of the Lambda function which process our call  According  after finishing of lambda function execution AWS dont stop the container and we are able to process next call in that container  Such approach improves performance of the service - only in time of first call AWS spend time to start container (cold start of Lambda function) and all next calls are executed faster because their use the same container (warm starts)  As a next step for improving the performance we created cron job which calls periodically our Lambda function (we use Cloudwatch rules for that)  Such approach allow to keep Lambda function warm allowing to avoid stopping and restarting of containers  I e  when the real user will call our REST API  Lambda will not spent time to start a new container  But we faced with the issue - such approach allow to keep warm only one container of Lambda function while the actual number of parallel calls from different users can be much larger (in our case thats hundreds and sometimes even thousands of users)  Is there any way to implement warm up functionality for Lambda function which could warm not only single container  but some desired number of them  I understand that such approach can affect cost of Lambda functions using and possibly  at all it will be better to use good old application server  but comparison of these approaches and their costs will be the next steps  I think  and in current moment I would like just to find the way to warm desired count of Lambda function containers  
69076040,1,69088634.0,,2021-09-06 14:17:07,,2,84,"<p>I want to use AWS API Gateway HTTP API instead of old REST with my lambda functions, for pricing reasons.</p>
<p>Difference here: <a href=""https://docs.aws.amazon.com/apigateway/latest/developerguide/http-api-vs-rest.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/apigateway/latest/developerguide/http-api-vs-rest.html</a></p>
<p>What is the option in Zappa for this?</p>
<pre><code>prod:
  s3_bucket: mybucket
  project_name&quot;: myproject
  app_function: app.app
  aws_region: eu-west-3
  domain: my.domain.com
  memory_size: 128
  lambda_concurrency: 10
  runtime: python3.8
  timeout_seconds: 30
  exception_handler: zappa_sentry.unhandled_exceptions
  keep_warm: false
  async_resources: false
</code></pre>
<pre class=""lang-sh prettyprint-override""><code>zappa update prod -s zappa_settings.yml
</code></pre>
<p>[EDIT]
Not sure if it's linked, but I encountered <a href=""https://stackoverflow.com/questions/68391621/zappa-deploy-fails-with-attributeerror-template-object-has-no-attribute-add"">Zappa deploy fails with AttributeError: &#39;Template&#39; object has no attribute &#39;add_description&#39;</a>
and then using this as <code>requirements.txt</code> (using <code>python3.8</code>):</p>
<pre><code>flask==1.1.4
zappa==0.53.0
zappa_sentry==0.4.1
troposphere&lt;3
</code></pre>
",1488055.0,,1488055.0,,2021-09-07 09:00:30,2021-09-07 12:57:17,Force Zappa to use API Gateway HTTP API instead of REST,<aws-lambda><aws-api-gateway><zappa>,1,0,,,,CC BY-SA 4.0,I want to use AWS API Gateway HTTP API instead of old REST with my lambda functions  for pricing reasons  Difference here:  What is the option in Zappa for this    [EDIT] Not sure if its linked  but I encountered  and then using this as  (using ):  
69109082,1,,,2021-09-08 20:07:20,,0,22,"<p>I have a chatbot that I made with selenium. This chatbot should work on multiple users. At the same time, I need to pull and push data from the chatbot. And I want users to be able to command the bot whenever they want. Closing the bot, Writing a message, etc.</p>
<p>When the button is pressed from the panel, I want to raise the bot to its feet. I thought of many ways to do this. Django Celery build, Heroku, Docker Container or Aws Lambda Container . With which platform can I ensure that the bot works perfectly, that the communication takes place and that the cost is the least? Do you have a previous source?</p>
",13486448.0,,,,,2021-09-08 20:07:20,Integrating Selenium Bot into Django,<django><docker><selenium><aws-lambda><selenium-grid>,0,0,1.0,,,CC BY-SA 4.0,I have a chatbot that I made with selenium  This chatbot should work on multiple users  At the same time  I need to pull and push data from the chatbot  And I want users to be able to command the bot whenever they want  Closing the bot  Writing a message  etc  When the button is pressed from the panel  I want to raise the bot to its feet  I thought of many ways to do this  Django Celery build  Heroku  Docker Container or Aws Lambda Container   With which platform can I ensure that the bot works perfectly  that the communication takes place and that the cost is the least  Do you have a previous source  
69111239,1,,,2021-09-09 01:30:57,,2,482,"<p>Im looking for a way to transfer cloudwatch logs to s3 bucket. I found I can use subscription filter in cloudwatch. I see two options.</p>
<p>One is CW -&gt; Kinesis Firehose -&gt; S3.</p>
<p>Other one is CW -&gt; lambda -&gt; S3.</p>
<p>Do you know which one is better? I feel like Kinesis firehose is easier to set up, but is using lambda cheaper?</p>
",6938992.0,,15952974.0,,2021-09-13 04:52:43,2021-09-13 04:52:43,Transfer CloudWatch Logs to S3 using Lambda or Kinesis Firehose?,<amazon-web-services><amazon-s3><aws-lambda><amazon-cloudwatchlogs><amazon-kinesis-firehose>,2,0,,,,CC BY-SA 4.0,Im looking for a way to transfer cloudwatch logs to s3 bucket  I found I can use subscription filter in cloudwatch  I see two options  One is CW -&gt; Kinesis Firehose -&gt; S3  Other one is CW -&gt; lambda -&gt; S3  Do you know which one is better  I feel like Kinesis firehose is easier to set up  but is using lambda cheaper  
69143293,1,69143608.0,,2021-09-11 13:05:36,,0,57,"<p>My AWS Lambda function needs to access data that is updated every hour and is going to be called very often via api. What is the most efficient and least expensive way?</p>
<p>The data that is already updated every hour is configured through Lambda batch, but I don't know where to store this data.</p>
<p>How about putting the latest data in the latest bucket of Amazon S3 every time? Or, even if there is a problem with the hot partition, how about storing it in Amazon DynamoDB because it is simple access? I considered the gateway cache, which is updated every hour, but at a cost. Please advise.</p>
",4314787.0,,174777.0,,2021-09-12 02:03:44,2021-09-12 02:07:41,AWS Lambda access data refresh hourly,<amazon-web-services><amazon-s3><aws-lambda><amazon-dynamodb>,2,1,,,,CC BY-SA 4.0,My AWS Lambda function needs to access data that is updated every hour and is going to be called very often via api  What is the most efficient and least expensive way  The data that is already updated every hour is configured through Lambda batch  but I dont know where to store this data  How about putting the latest data in the latest bucket of Amazon S3 every time  Or  even if there is a problem with the hot partition  how about storing it in Amazon DynamoDB because it is simple access  I considered the gateway cache  which is updated every hour  but at a cost  Please advise  
35541780,1,35563347.0,,2016-02-21 20:44:10,,10,3033,"<p>What ways do exist for handling http(s) requests using <code>AWS lambda</code> but without using <code>API Gateway</code> or <code>Amazon Kinesis</code> ? Is it possible at all?</p>

<p>Particular I want implement my own REST API but do not pay for <code>API Gateway</code> service, using only <code>AWS lambda</code>.</p>

<p><strong>I'm not asking for tutorial or library</strong>, this is principal about <code>Amazon services</code> architecture.</p>

<p>This all is about <code>Java 8</code> runtime.</p>
",1479414.0,,,,,2018-08-03 13:54:17,Handling https requests without API Gateway,<java><amazon-web-services><aws-lambda>,1,2,2.0,,,CC BY-SA 3.0,What ways do exist for handling http(s) requests using  but without using  or    Is it possible at all  Particular I want implement my own REST API but do not pay for  service  using only   Im not asking for tutorial or library  this is principal about  architecture  This all is about  runtime  
69182279,1,,,2021-09-14 17:38:58,,0,57,"<p>I have developed multiple microServices in my previous projects, So am well versed on it. Currently we are on the edge to start developing a SaaS webApp using java or springboot &amp; we want to leverage AWS lambda for pricing sake but couple things I couldn't understand and couldn't find good example are:</p>
<ol>
<li>Is it feasible to develop web app fully on AWS lambda using JAVA or springboot?(I
know we can write java function on lambda).</li>
<li>How do we tackle code reusability on AWS Lambda?</li>
<li>If we end up having 100 lambda functions, does it mean we need to build
100 jar's for each function?</li>
<li>Does each end point need to be represent by single lambda function?</li>
</ol>
<p>I would really appreciate it if you can attache a link for similar scenario!!!</p>
",8150201.0,,,,,2021-09-14 17:38:58,Architecture of SaaS Web Application With AWS lambda Using Java or Springboot?,<java><spring-boot><aws-lambda>,0,2,,,,CC BY-SA 4.0,I have developed multiple microServices in my previous projects  So am well versed on it  Currently we are on the edge to start developing a SaaS webApp using java or springboot &amp; we want to leverage AWS lambda for pricing sake but couple things I couldnt understand and couldnt find good example are:  Is it feasible to develop web app fully on AWS lambda using JAVA or springboot (I know we can write java function on lambda)  How do we tackle code reusability on AWS Lambda  If we end up having 100 lambda functions  does it mean we need to build 100 jars for each function  Does each end point need to be represent by single lambda function   I would really appreciate it if you can attache a link for similar scenario    
35878619,1,,,2016-03-08 21:44:16,,17,15176,"<p>We are trying to develop a true lambda-based application in which certain tasks need to be performed at schedules of variable frequencies. They are actually polling for data, and at certain times of the day, this polling can be as slow as once every hour, while at other times, it has to be once every second. I have looked at the options for scheduling (e.g. <a href=""https://docs.aws.amazon.com/lambda/latest/dg/services-cloudwatchevents.html"" rel=""nofollow noreferrer"">Using AWS Lambda with Scheduled Events</a> and <a href=""https://www.youtube.com/watch?v=FhJxTIq81AU"" rel=""nofollow noreferrer"">AWS re:Invent 2015 | (CMP407) Lambda as Cron: Scheduling Invocations in AWS Lambda</a>), but it seems that short of spinning up an EC2 instance or a long-running lambda, there's no built-in way of firing up lambdas at a frequency of less than one minute. The lambda rate expression doesn't have a place for seconds. Is there a way to do this without an EC2 instance or long-running lambda? Ideally, something that can be done without incurring additional cost for scheduling.</p>
",2314001.0,,6270590.0,,2021-03-15 11:17:28,2021-08-27 17:02:43,Scheduled AWS Lambda Task at less than 1 minute frequency,<amazon-web-services><amazon-ec2><scheduled-tasks><aws-lambda>,5,8,,,,CC BY-SA 4.0,We are trying to develop a true lambda-based application in which certain tasks need to be performed at schedules of variable frequencies  They are actually polling for data  and at certain times of the day  this polling can be as slow as once every hour  while at other times  it has to be once every second  I have looked at the options for scheduling (e g   and )  but it seems that short of spinning up an EC2 instance or a long-running lambda  theres no built-in way of firing up lambdas at a frequency of less than one minute  The lambda rate expression doesnt have a place for seconds  Is there a way to do this without an EC2 instance or long-running lambda  Ideally  something that can be done without incurring additional cost for scheduling  
69234620,1,69235751.0,,2021-09-18 12:25:22,,0,43,"<p>After paying many thousands to AWS due to small programmer's mistake I have a question.</p>
<p>How can I set an action which will suspend the whole account activity after budget alarm happened? For me it seems <strong>insane</strong> that customer isn't allowed to do such thing easily. We had alarm set up but it was weekend and nobody reacted on it.</p>
<p>We were compensated only 5k(it's a small part of the whole bill).</p>
",7248743.0,,,,,2021-09-18 15:08:50,Automatically suspend whole AWS account after reaching limits,<amazon-web-services><aws-lambda><cloud><serverless><aws-serverless>,1,1,,,,CC BY-SA 4.0,After paying many thousands to AWS due to small programmers mistake I have a question  How can I set an action which will suspend the whole account activity after budget alarm happened  For me it seems insane that customer isnt allowed to do such thing easily  We had alarm set up but it was weekend and nobody reacted on it  We were compensated only 5k(its a small part of the whole bill)  
69246714,1,69259271.0,,2021-09-19 20:05:15,,0,474,"<p>I have two AWS accounts with VPCs connected with peer connections. I have RDS Proxy on account 1 and Lambda in the private, isolated subnet on account 2.</p>
<p>I cannot figure out how to connect to RDS Proxy. I was trying all possible VPC endpoints and interfaces and whatnot.</p>
<p>The only way I managed to connect was through NAT Gateway, but it's expensive. And to be honest, weird, if I want to keep a private network.</p>
<p>Is it possible to have a PrivateLink or something?
How should I connect to RDS Proxy from Lambda?</p>
<p>I have attached the CF template (I hope I cleaned it from all sensitive data ):</p>
<pre><code>{
  &quot;Resources&quot;: {
    &quot;vpcA2121C38&quot;: {
      &quot;Type&quot;: &quot;AWS::EC2::VPC&quot;,
      &quot;Properties&quot;: {
        &quot;CidrBlock&quot;: &quot;10.0.0.0/16&quot;,
        &quot;EnableDnsHostnames&quot;: true,
        &quot;EnableDnsSupport&quot;: true,
        &quot;InstanceTenancy&quot;: &quot;default&quot;
      },
      &quot;Metadata&quot;: {
        &quot;aws:cdk:path&quot;: &quot;Mws/vpc/Resource&quot;
      }
    },
    &quot;vpcPrivateSubnet1Subnet934893E8&quot;: {
      &quot;Type&quot;: &quot;AWS::EC2::Subnet&quot;,
      &quot;Properties&quot;: {
        &quot;CidrBlock&quot;: &quot;10.0.0.0/26&quot;,
        &quot;VpcId&quot;: {
          &quot;Ref&quot;: &quot;vpcA2121C38&quot;
        },
        &quot;AvailabilityZone&quot;: {
          &quot;Fn::Select&quot;: [
            0,
            {
              &quot;Fn::GetAZs&quot;: &quot;&quot;
            }
          ]
        },
        &quot;MapPublicIpOnLaunch&quot;: false
      },
      &quot;Metadata&quot;: {
        &quot;aws:cdk:path&quot;: &quot;Mws/vpc/PrivateSubnet1/Subnet&quot;
      }
    },
    &quot;vpcPrivateSubnet1RouteTableB41A48CC&quot;: {
      &quot;Type&quot;: &quot;AWS::EC2::RouteTable&quot;,
      &quot;Properties&quot;: {
        &quot;VpcId&quot;: {
          &quot;Ref&quot;: &quot;vpcA2121C38&quot;
        }
      },
      &quot;Metadata&quot;: {
        &quot;aws:cdk:path&quot;: &quot;Mws/vpc/PrivateSubnet1/RouteTable&quot;
      }
    },
    &quot;vpcPrivateSubnet1RouteTableAssociation67945127&quot;: {
      &quot;Type&quot;: &quot;AWS::EC2::SubnetRouteTableAssociation&quot;,
      &quot;Properties&quot;: {
        &quot;RouteTableId&quot;: {
          &quot;Ref&quot;: &quot;vpcPrivateSubnet1RouteTableB41A48CC&quot;
        },
        &quot;SubnetId&quot;: {
          &quot;Ref&quot;: &quot;vpcPrivateSubnet1Subnet934893E8&quot;
        }
      },
      &quot;Metadata&quot;: {
        &quot;aws:cdk:path&quot;: &quot;Mws/vpc/PrivateSubnet1/RouteTableAssociation&quot;
      }
    },
    &quot;vpcS3CB758969&quot;: {
      &quot;Type&quot;: &quot;AWS::EC2::VPCEndpoint&quot;,
      &quot;Properties&quot;: {
        &quot;ServiceName&quot;: {
          &quot;Fn::Join&quot;: [
            &quot;&quot;,
            [
              &quot;com.amazonaws.&quot;,
              {
                &quot;Ref&quot;: &quot;AWS::Region&quot;
              },
              &quot;.s3&quot;
            ]
          ]
        },
        &quot;VpcId&quot;: {
          &quot;Ref&quot;: &quot;vpcA2121C38&quot;
        },
        &quot;RouteTableIds&quot;: [
          {
            &quot;Ref&quot;: &quot;vpcPrivateSubnet1RouteTableB41A48CC&quot;
          }
        ],
        &quot;VpcEndpointType&quot;: &quot;Gateway&quot;
      },
      &quot;Metadata&quot;: {
        &quot;aws:cdk:path&quot;: &quot;Mws/vpc/S3/Resource&quot;
      }
    },
    &quot;vpcPeertomainaccount833D3E2C&quot;: {
      &quot;Type&quot;: &quot;AWS::EC2::VPCPeeringConnection&quot;,
      &quot;Properties&quot;: {
        &quot;PeerVpcId&quot;: &quot;vpc-f04b939b&quot;,
        &quot;VpcId&quot;: {
          &quot;Ref&quot;: &quot;vpcA2121C38&quot;
        },
        &quot;PeerOwnerId&quot;: &quot;XXXX__ACCOINT_1__XXXXXX&quot;,
        &quot;PeerRoleArn&quot;: &quot;arn:aws:iam::XXXX__ACCOINT_1__XXXXXX:role/VPCPeerConnection&quot;
        
      },
      &quot;Metadata&quot;: {
        &quot;aws:cdk:path&quot;: &quot;Mws/vpc/Peer to main account&quot;
      }
    },
    &quot;producerServiceRoleEBCB54D0&quot;: {
      &quot;Type&quot;: &quot;AWS::IAM::Role&quot;,
      &quot;Properties&quot;: {
        &quot;AssumeRolePolicyDocument&quot;: {
          &quot;Statement&quot;: [
            {
              &quot;Action&quot;: &quot;sts:AssumeRole&quot;,
              &quot;Effect&quot;: &quot;Allow&quot;,
              &quot;Principal&quot;: {
                &quot;Service&quot;: &quot;lambda.amazonaws.com&quot;
              }
            }
          ],
          &quot;Version&quot;: &quot;2012-10-17&quot;
        },
        &quot;ManagedPolicyArns&quot;: [
          {
            &quot;Fn::Join&quot;: [
              &quot;&quot;,
              [
                &quot;arn:&quot;,
                {
                  &quot;Ref&quot;: &quot;AWS::Partition&quot;
                },
                &quot;:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole&quot;
              ]
            ]
          },
          {
            &quot;Fn::Join&quot;: [
              &quot;&quot;,
              [
                &quot;arn:&quot;,
                {
                  &quot;Ref&quot;: &quot;AWS::Partition&quot;
                },
                &quot;:iam::aws:policy/service-role/AWSLambdaVPCAccessExecutionRole&quot;
              ]
            ]
          }
        ]
      },
      &quot;Metadata&quot;: {
        &quot;aws:cdk:path&quot;: &quot;Mws/producer/producer/ServiceRole/Resource&quot;
      }
    },
    &quot;producerServiceRoleDefaultPolicyEA5B80A1&quot;: {
      &quot;Type&quot;: &quot;AWS::IAM::Policy&quot;,
      &quot;Properties&quot;: {
        &quot;PolicyDocument&quot;: {
          &quot;Statement&quot;: [
            {
              &quot;Action&quot;: [
                &quot;xray:PutTraceSegments&quot;,
                &quot;xray:PutTelemetryRecords&quot;
              ],
              &quot;Effect&quot;: &quot;Allow&quot;,
              &quot;Resource&quot;: &quot;*&quot;
            },
            {
              &quot;Action&quot;: [
                &quot;s3:GetObject*&quot;,
                &quot;s3:GetBucket*&quot;,
                &quot;s3:List*&quot;
              ],
              &quot;Effect&quot;: &quot;Allow&quot;,
              &quot;Resource&quot;: [
                {
                  &quot;Fn::Join&quot;: [
                    &quot;&quot;,
                    [
                      &quot;arn:&quot;,
                      {
                        &quot;Ref&quot;: &quot;AWS::Partition&quot;
                      },
                      &quot;:s3:::&quot;,
                      {
                        &quot;Fn::Sub&quot;: &quot;cdk-hnb659fds-assets-${AWS::AccountId}-${AWS::Region}&quot;
                      }
                    ]
                  ]
                },
                {
                  &quot;Fn::Join&quot;: [
                    &quot;&quot;,
                    [
                      &quot;arn:&quot;,
                      {
                        &quot;Ref&quot;: &quot;AWS::Partition&quot;
                      },
                      &quot;:s3:::&quot;,
                      {
                        &quot;Fn::Sub&quot;: &quot;cdk-hnb659fds-assets-${AWS::AccountId}-${AWS::Region}&quot;
                      },
                      &quot;/*&quot;
                    ]
                  ]
                }
              ]
            }
          ],
          &quot;Version&quot;: &quot;2012-10-17&quot;
        },
        &quot;PolicyName&quot;: &quot;producerServiceRoleDefaultPolicyEA5B80A1&quot;,
        &quot;Roles&quot;: [
          {
            &quot;Ref&quot;: &quot;producerServiceRoleEBCB54D0&quot;
          }
        ]
      },
      &quot;Metadata&quot;: {
        &quot;aws:cdk:path&quot;: &quot;Mws/producer/producer/ServiceRole/DefaultPolicy/Resource&quot;
      }
    },
    &quot;producerSecurityGroup9AA1BE28&quot;: {
      &quot;Type&quot;: &quot;AWS::EC2::SecurityGroup&quot;,
      &quot;Properties&quot;: {
        &quot;GroupDescription&quot;: &quot;Automatic security group for Lambda Function Mwsproducer416E938A&quot;,
        &quot;SecurityGroupEgress&quot;: [
          {
            &quot;CidrIp&quot;: &quot;0.0.0.0/0&quot;,
            &quot;Description&quot;: &quot;Allow all outbound traffic by default&quot;,
            &quot;IpProtocol&quot;: &quot;-1&quot;
          }
        ],
        
        &quot;VpcId&quot;: {
          &quot;Ref&quot;: &quot;vpcA2121C38&quot;
        }
      },
      &quot;Metadata&quot;: {
        &quot;aws:cdk:path&quot;: &quot;Mws/producer/producer/SecurityGroup/Resource&quot;
      }
    },
    &quot;producerAD962441&quot;: {
      &quot;Type&quot;: &quot;AWS::Lambda::Function&quot;,
      &quot;Properties&quot;: {
        &quot;Code&quot;: {
          &quot;S3Bucket&quot;: {
            &quot;Fn::Sub&quot;: &quot;cdk-hnb659fds-assets-${AWS::AccountId}-${AWS::Region}&quot;
          },
          &quot;S3Key&quot;: &quot;dc05df325f7034421a587e7ee47aed301c7472d001152e686053dd9d5c45c164.zip&quot;
        },
        &quot;Role&quot;: {
          &quot;Fn::GetAtt&quot;: [
            &quot;producerServiceRoleEBCB54D0&quot;,
            &quot;Arn&quot;
          ]
        },
        &quot;Description&quot;: &quot;Produces MWS customer orders messages&quot;,
        &quot;Environment&quot;: {
          &quot;Variables&quot;: {
            &quot;DB_HOST&quot;: &quot;mws-rds-proxy.proxy-XXXXXXXXX.eu-central-1.rds.amazonaws.com&quot;,
            &quot;DB_NAME&quot;: &quot;dev&quot;
          }
        },
        &quot;Handler&quot;: &quot;lambda/mws_producer.php&quot;,
        &quot;Layers&quot;: [
          &quot;arn:aws:lambda:eu-central-1:209497400698:layer:php-80:18&quot;
        ],
        &quot;MemorySize&quot;: 1024,
        &quot;Runtime&quot;: &quot;provided.al2&quot;,
        
        &quot;Timeout&quot;: 900,
        &quot;TracingConfig&quot;: {
          &quot;Mode&quot;: &quot;Active&quot;
        },
        &quot;VpcConfig&quot;: {
          &quot;SecurityGroupIds&quot;: [
            {
              &quot;Fn::GetAtt&quot;: [
                &quot;producerSecurityGroup9AA1BE28&quot;,
                &quot;GroupId&quot;
              ]
            }
          ],
          &quot;SubnetIds&quot;: [
            {
              &quot;Ref&quot;: &quot;vpcPrivateSubnet1Subnet934893E8&quot;
            }
          ]
        }
      },
      &quot;DependsOn&quot;: [
        &quot;producerServiceRoleDefaultPolicyEA5B80A1&quot;,
        &quot;producerServiceRoleEBCB54D0&quot;
      ]
    }
  }
}
</code></pre>
",1031304.0,,582320.0,,2021-09-29 17:47:56,2021-09-29 17:47:56,Lambda RDS Proxy connection from different VPC,<amazon-web-services><aws-lambda><amazon-rds><amazon-vpc><amazon-rds-proxy>,1,1,,,,CC BY-SA 4.0,I have two AWS accounts with VPCs connected with peer connections  I have RDS Proxy on account 1 and Lambda in the private  isolated subnet on account 2  I cannot figure out how to connect to RDS Proxy  I was trying all possible VPC endpoints and interfaces and whatnot  The only way I managed to connect was through NAT Gateway  but its expensive  And to be honest  weird  if I want to keep a private network  Is it possible to have a PrivateLink or something  How should I connect to RDS Proxy from Lambda  I have attached the CF template (I hope I cleaned it from all sensitive data ):  
69259019,1,,,2021-09-20 18:06:50,,0,407,"<p>I am new to AWS and cloud technology in general. So, please bear with me if the use case below is a trivial one.</p>
<p>Well, I have a table in Amazon DynamoDB which I am exporting to Amazon S3 using exportTableToPointInTime API (ExportsToS3) on a scheduled basis everyday at 6 AM. It is being done using an AWS Lambda function in this way -</p>
<pre><code>const AWS = require(&quot;aws-sdk&quot;);

exports.handler = async (event) =&gt; {

    const dynamodb = new AWS.DynamoDB({ apiVersion: '2012-08-10' });
    const tableParams = {
        S3Bucket: '&lt;s3-bucket-name&gt;',
        TableArn: '&lt;DynamoDB-Table-ARN&gt;',
        ExportFormat: 'DYNAMODB_JSON'
    };

    await dynamodb.exportTableToPointInTime(tableParams).promise();
};
</code></pre>
<p>The CFT template of the AWS Lambda function takes care of creating lambda roles and policies, etc. along with scheduling using Cloudwatch events. This setup works and the table is exported to the target Amazon S3 bucket everyday at the scheduled time.</p>
<p>Now, the next thing I want is that after the export to Amazon S3 is complete, I should be able to invoke an another lambda function and pass the export status to that lambda function which does some processing with it.</p>
<p>The problem I am facing is that the above lambda function finishes execution almost immediately with the exportTableToPointInTime call returning status as IN_PROGRESS.</p>
<p>I tried capturing the response of the above call like -</p>
<pre><code>const exportResponse = await dynamodb.exportTableToPointInTime(tableParams).promise();
console.log(exportResponse);
</code></pre>
<p>Output of this is -</p>
<pre><code>{
    &quot;ExportDescription&quot;: {
        &quot;ExportArn&quot;: &quot;****&quot;,
        &quot;ExportStatus&quot;: &quot;IN_PROGRESS&quot;,
        &quot;StartTime&quot;: &quot;2021-09-20T16:51:52.147000+05:30&quot;,
        &quot;TableArn&quot;: &quot;****&quot;,
        &quot;TableId&quot;: &quot;****&quot;,
        &quot;ExportTime&quot;: &quot;2021-09-20T16:51:52.147000+05:30&quot;,
        &quot;ClientToken&quot;: &quot;****&quot;,
        &quot;S3Bucket&quot;: &quot;****&quot;,
        &quot;S3SseAlgorithm&quot;: &quot;AES256&quot;,
        &quot;ExportFormat&quot;: &quot;DYNAMODB_JSON&quot;
    }
}
</code></pre>
<p>I am just obfuscating some values in the log with ****</p>
<p>As can be seen, the exportTableToPointInTime API call does not wait for the table to be exported completely. If it would have, it would have returned ExportStatus as either COMPLETED or FAILED.</p>
<p>Is there a way I can design the above use case to achieve my requirement - invoking an another lambda function <strong>only</strong> when the export is <strong>actually complete</strong>?</p>
<p>As of now, I have tried a brute force way to do it and which works but it definitely seems to be inefficient as it puts in a sleep there and also the lambda function is running for the entire duration of the export leading to cost impacts.</p>
<pre><code>exports.handler = async (event) =&gt; {

    const dynamodb = new AWS.DynamoDB({ apiVersion: '2012-08-10' });
    const tableParams = {
        S3Bucket: '&lt;s3-bucket-name&gt;',
        TableArn: '&lt;DynamoDB-Table-ARN&gt;',
        ExportFormat: 'DYNAMODB_JSON'
    };

    const exportResponse = await dynamodb.exportTableToPointInTime(tableParams).promise();

    const exportArn = exportResponse.ExportDescription.ExportArn;
    let exportStatus = exportResponse.ExportDescription.ExportStatus;
    
    const sleep = (waitTimeInMs) =&gt; new Promise(resolve =&gt; setTimeout(resolve, waitTimeInMs));
    
    do {
        await sleep(60000); //waiting every 1 min and then calling listExports API
        const listExports = await dynamodb.listExports().promise();
        const filteredExports = listExports.ExportSummaries.filter(e =&gt; e.ExportArn == exportArn);
        const currentExport = filteredExports[0];
        exportStatus = currentExport.ExportStatus;
    }
    while (exportStatus == 'IN_PROGRESS');

    var lambda = new AWS.Lambda();
    var paramsForInvocation = {
        FunctionName: 'another-lambda-function',
        InvocationType: 'Event',
        Payload: JSON.stringify({ 'ExportStatus': exportStatus })
    };

    await lambda.invoke(paramsForInvocation).promise();
};
</code></pre>
<p>What can be done to better it or the above solution is okay?</p>
<p>Thanks!!</p>
",3148811.0,,3148811.0,,2021-09-21 04:57:32,2022-01-19 12:17:55,Invoke an AWS Lambda function only after an Amazon DynamoDB export to Amazon S3 is totally complete,<amazon-web-services><amazon-s3><aws-lambda><amazon-dynamodb>,3,2,,,,CC BY-SA 4.0,I am new to AWS and cloud technology in general  So  please bear with me if the use case below is a trivial one  Well  I have a table in Amazon DynamoDB which I am exporting to Amazon S3 using exportTableToPointInTime API (ExportsToS3) on a scheduled basis everyday at 6 AM  It is being done using an AWS Lambda function in this way -  The CFT template of the AWS Lambda function takes care of creating lambda roles and policies  etc  along with scheduling using Cloudwatch events  This setup works and the table is exported to the target Amazon S3 bucket everyday at the scheduled time  Now  the next thing I want is that after the export to Amazon S3 is complete  I should be able to invoke an another lambda function and pass the export status to that lambda function which does some processing with it  The problem I am facing is that the above lambda function finishes execution almost immediately with the exportTableToPointInTime call returning status as IN_PROGRESS  I tried capturing the response of the above call like -  Output of this is -  I am just obfuscating some values in the log with **** As can be seen  the exportTableToPointInTime API call does not wait for the table to be exported completely  If it would have  it would have returned ExportStatus as either COMPLETED or FAILED  Is there a way I can design the above use case to achieve my requirement - invoking an another lambda function only when the export is actually complete  As of now  I have tried a brute force way to do it and which works but it definitely seems to be inefficient as it puts in a sleep there and also the lambda function is running for the entire duration of the export leading to cost impacts   What can be done to better it or the above solution is okay  Thanks   
69285126,1,69287973.0,,2021-09-22 13:23:34,,0,46,"<p>mouthful title but the point is this, I have some data science pipelines with these requirements (<code>python</code> based):</p>
<ol>
<li>are orchestrated with an &quot;internal&quot; orchestrator based off on a server</li>
<li>are run across a number of users/products /etc where N could be relatively high</li>
<li>the &quot;load&quot; of this jobs I want to distribute and not be tethered by the orchestrator server</li>
<li>these jobs are backed by a docker image</li>
<li>these jobs are relatively fast to run (from 1 second to 20 seconds, post data load)</li>
<li>these jobs most often require considerable I/O both coming in and out.</li>
<li>no <code>spark</code> required</li>
<li>I want minimal hassle with scaling/provisioning/etc</li>
<li>data (in/out) would be stored in either a <code>HDFS</code> space in a cluster or <code>AWS S3</code></li>
<li><code>docker</code> image would be relatively large (encompasses data science stack)</li>
</ol>
<p>I was trying to understand the most (a) cost-efficient but also (b) fast solution to parallelize this thing. candidates so far:</p>
<ol>
<li><code>AWS ECS</code></li>
<li><code>AWS lambda</code> with Container Image Support</li>
</ol>
<p>please note for all intents and purposes scaling/computing within the cluster is not feasible</p>
<p>my issue is that I worry about the tradeoffs about huge data transfers (in aggregate terms), huge costs in calling <code>docker</code> images a bunch of times, time you would spend setting up containers in servers but very low time doing anything else, serverless management and debugging when things go wrong in case for lambda functions.</p>
<p>how generally are handled these kind of cases?</p>
",4566565.0,,,,,2021-09-22 16:26:50,Scale out jobs with high memory consumption but low computing power within AWS and using Docker: finding best solution,<docker><aws-lambda><pipeline><amazon-ecs>,1,1,,,,CC BY-SA 4.0,mouthful title but the point is this  I have some data science pipelines with these requirements ( based):  are orchestrated with an internal orchestrator based off on a server are run across a number of users/products /etc where N could be relatively high the load of this jobs I want to distribute and not be tethered by the orchestrator server these jobs are backed by a docker image these jobs are relatively fast to run (from 1 second to 20 seconds  post data load) these jobs most often require considerable I/O both coming in and out  no  required I want minimal hassle with scaling/provisioning/etc data (in/out) would be stored in either a  space in a cluster or   image would be relatively large (encompasses data science stack)  I was trying to understand the most (a) cost-efficient but also (b) fast solution to parallelize this thing  candidates so far:    with Container Image Support  please note for all intents and purposes scaling/computing within the cluster is not feasible my issue is that I worry about the tradeoffs about huge data transfers (in aggregate terms)  huge costs in calling  images a bunch of times  time you would spend setting up containers in servers but very low time doing anything else  serverless management and debugging when things go wrong in case for lambda functions  how generally are handled these kind of cases  
69284722,1,,,2021-09-22 12:57:27,,0,239,"<p>Currently i have a state machine that receives two parameters as input.</p>
<pre class=""lang-json prettyprint-override""><code>{
  &quot;tte&quot;: &quot;2021-09-22T12:53:56.343571Z&quot;,
  &quot;message&quot;: &quot;Hello world&quot;
}
</code></pre>
<p>When starting, the first step is to wait for the time determined by the <code>tte</code> (time to execute) attribute. Finally send the content of the <code>message</code> attribute to a queue in sqs.</p>
<p>I understand that aws charges me for changes in the status of my machine, but I have no idea if a long waiting time, for example 1 week, could incur additional costs?</p>
",11927896.0,,13070.0,,2021-09-22 13:07:59,2021-09-22 15:26:06,Will AWS charge me for the wait time on a step function?,<amazon-web-services><aws-lambda><aws-step-functions>,1,0,,,,CC BY-SA 4.0,Currently i have a state machine that receives two parameters as input   When starting  the first step is to wait for the time determined by the  (time to execute) attribute  Finally send the content of the  attribute to a queue in sqs  I understand that aws charges me for changes in the status of my machine  but I have no idea if a long waiting time  for example 1 week  could incur additional costs  
69298298,1,,,2021-09-23 10:29:58,,0,1075,"<p>I am trying to reduce time on installing python packages on AWS Lambda.</p>
<p>For now, my codes are</p>
<pre><code># pip install custom package to /tmp/ and add to path
for package in ['requests==2.25.1', 'xlrd==2.0.1', 'pandas==1.2.4', 'numpy==1.19.5']:
    t1 = time.time()
    subprocess.call(f'pip install {package} -t /tmp/ --no-cache-dir'.split(), stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
    t2 = time.time()
    print(f'Install {package} in {round(t2-t1,2)} seconds')
sys.path.insert(1, '/tmp/')
</code></pre>
<p>but numpy and pandas take too long to install (~120s for pandas and ~50s for numpy).</p>
<p>I think this is a big charge if my function is called too frequently.</p>
<p>So I think that if I can install them into <strong>temp</strong> folder and push to S3, I can download and add to Lambda layer to call them without installing again.</p>
<p>I try zipping all <strong>temp</strong> directory but Lambda prevents me to writing files. Is there any way to overcome this?</p>
<p>Thank you for your reading!</p>
",8143272.0,,,,,2021-09-23 14:25:15,Pip install Packages on AWS Lambda and Store them on AWS S3,<python-3.x><amazon-s3><aws-lambda><shutil>,2,0,,,,CC BY-SA 4.0,I am trying to reduce time on installing python packages on AWS Lambda  For now  my codes are  but numpy and pandas take too long to install (~120s for pandas and ~50s for numpy)  I think this is a big charge if my function is called too frequently  So I think that if I can install them into temp folder and push to S3  I can download and add to Lambda layer to call them without installing again  I try zipping all temp directory but Lambda prevents me to writing files  Is there any way to overcome this  Thank you for your reading  
69310547,1,,,2021-09-24 06:34:21,,2,20,"<p>I'm trying to transfer files from an onsite Drobo to S3 Deep Archive.  Because of the way S3 stores things in Deep Storage, it never makes sense to archive objects which are 8KB or smaller (because you will pay for 8KB of Standard anyway).  Lifecycle rules are not smart enough to handle this logic, so I wrote a <a href=""https://github.com/fsiler/s3-archive/blob/main/working-lambda.py"" rel=""nofollow noreferrer"">lambda</a>.  However, I'm not sure what trigger to use.  Right now, this lambda only responds to <code>ObjectCreated:Put</code> events- which works fine for my simple online testing, but I suspect may not work when I'm doing the transfer with a Snowcone or Snowball.  The lambda itself then causes an <code>ObjectCreated:Copy</code> event if it archives the file.</p>
<p>So in order to get this to work with Snowcone/Snowball, it'd be nice to know: what event is generated when the files are transferred off those devices into S3?  I've contemplated just using DynamoDB and pushing archived filenames into a table so I have a reference, but that seems unnecessary if I can get firm guidance.  Another option is to be brutish about it and simply force-archive on every event received, because as far as I can tell, it is as expensive to query the current storage class of the object as it is to attempt the change in storage class.</p>
<p>Checked all the docs, including the 184 page Snowcone User guide PDF.  <a href=""https://docs.aws.amazon.com/AmazonS3/latest/userguide/notification-how-to-event-types-and-destinations.html"" rel=""nofollow noreferrer"">This blog article</a> suggests that the <code>Put</code> and <code>Post</code> events refer back to HTTP, but I don't think the Snow family existed at the time.  I tweeted at Jeff Barr and haven't heard back yet.  Anyone have actual experience with these devices?</p>
",6796731.0,,6796731.0,,2021-09-24 06:41:37,2021-09-24 06:41:37,AWS Snow devices- what event is generated when files are transferred to S3?,<amazon-web-services><amazon-s3><aws-lambda><aws-snowball>,0,1,0.0,,,CC BY-SA 4.0,Im trying to transfer files from an onsite Drobo to S3 Deep Archive   Because of the way S3 stores things in Deep Storage  it never makes sense to archive objects which are 8KB or smaller (because you will pay for 8KB of Standard anyway)   Lifecycle rules are not smart enough to handle this logic  so I wrote a    However  Im not sure what trigger to use   Right now  this lambda only responds to  events- which works fine for my simple online testing  but I suspect may not work when Im doing the transfer with a Snowcone or Snowball   The lambda itself then causes an  event if it archives the file  So in order to get this to work with Snowcone/Snowball  itd be nice to know: what event is generated when the files are transferred off those devices into S3   Ive contemplated just using DynamoDB and pushing archived filenames into a table so I have a reference  but that seems unnecessary if I can get firm guidance   Another option is to be brutish about it and simply force-archive on every event received  because as far as I can tell  it is as expensive to query the current storage class of the object as it is to attempt the change in storage class  Checked all the docs  including the 184 page Snowcone User guide PDF    suggests that the  and  events refer back to HTTP  but I dont think the Snow family existed at the time   I tweeted at Jeff Barr and havent heard back yet   Anyone have actual experience with these devices  
69368331,1,69368533.0,,2021-09-28 20:51:15,,1,42,"<p>I'm using <code>sls</code> with AWS Fargate. I'm trying use <code>sls deploy -v</code> but am getting an error in my <code>fargate-template.yml</code> when I try to set the <code>Memory</code> property under <code>FargateECSTaskDefinition</code>. Here's the relevant <code>yml</code> section:</p>
<pre><code>FargateECSTaskDefinition:
    Type: &quot;AWS::ECS::TaskDefinition&quot;
    Properties:
      Cpu: 256
      Memory: 512 MB
</code></pre>
<p>And here's the error I get:</p>
<pre><code>An error occurred: FargateECSTaskDefinition - Resource handler returned message: &quot;Invalid request provided: Create TaskDefinition: Invalid 'memory' setting for task. (Service: AmazonECS; Status Code: 400; Error Code: InvalidParameterException; Request ID: f4d1c13d-1885-4157-bf21-4299b06cf3ff; Proxy: null)&quot; (RequestToken: b872e59f-4329-35ed-5191-cfb0c486f49f, HandlerErrorCode: InvalidRequest).
</code></pre>
<p>Setting <code>Memory: 1GB</code> works without a problem. I tried both <code>Memory: 512 MB</code> and <code>Memory: 512MB</code>, thinking that the space would make a difference. Both returned an error though. I'm following <a href=""https://docs.aws.amazon.com/AmazonECS/latest/developerguide/create-task-definition.html"" rel=""nofollow noreferrer"">this</a> documentation, which suggests that a combination of 256 CPU units and <code>512 MB</code> memory is compatible, so I'm not sure what the issue could be.</p>
<p>The difference between 512MB and 1GB is trivial from a cost perspective, but at this point I want to solve this on principle.</p>
",14908234.0,,248823.0,,2021-09-28 21:12:22,2021-09-28 21:12:22,Error in Fargate configuration file: FargateECSTaskDefinition,<amazon-web-services><amazon-ec2><aws-lambda><amazon-cloudformation><aws-fargate>,1,0,,,,CC BY-SA 4.0,Im using  with AWS Fargate  Im trying use  but am getting an error in my  when I try to set the  property under   Heres the relevant  section:  And heres the error I get:  Setting  works without a problem  I tried both  and   thinking that the space would make a difference  Both returned an error though  Im following  documentation  which suggests that a combination of 256 CPU units and  memory is compatible  so Im not sure what the issue could be  The difference between 512MB and 1GB is trivial from a cost perspective  but at this point I want to solve this on principle  
69395747,1,,,2021-09-30 16:24:17,,0,86,"<p>Suppose I have some resources that I want to share between different requests in an aws lambda written in python. How should I implement this?</p>
<p>Are there hooks for &quot;post startup&quot;, or should I lazily create resources on the first call? The disadvantage of &quot;lazy inititializing&quot; is that it means some requests will be randomly slow because you pick a consumer to incur the startup cost.</p>
<p>Also... will those resources survive a lambda executable being &quot;frozen&quot;?</p>
<p>This page <a href=""https://docs.aws.amazon.com/lambda/latest/dg/runtimes-context.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/lambda/latest/dg/runtimes-context.html</a> talks about an &quot;Init&quot; stage. How do I execute things in the init stage? This seems to suggest that Init includes both an &quot;unfreeze&quot; operation and a freeze operation.</p>
<h1>Research</h1>
<p><a href=""https://stackoverflow.com/questions/51074990/connection-pooling-in-aws-across-lambdas"">Connection pooling in AWS across lambdas</a></p>
",1892584.0,,174777.0,,2021-10-01 01:04:41,2021-10-01 08:27:46,Python AWS lambda python startup hook?,<python><amazon-web-services><aws-lambda>,1,1,,,,CC BY-SA 4.0,Suppose I have some resources that I want to share between different requests in an aws lambda written in python  How should I implement this  Are there hooks for post startup  or should I lazily create resources on the first call  The disadvantage of lazy inititializing is that it means some requests will be randomly slow because you pick a consumer to incur the startup cost  Also    will those resources survive a lambda executable being frozen  This page  talks about an Init stage  How do I execute things in the init stage  This seems to suggest that Init includes both an unfreeze operation and a freeze operation  Research  
69420211,1,,,2021-10-02 20:56:44,,1,71,"<p>According to the AWS Solutions Architect labs that I am currently following, AWS Lambdas are ideal for IoT applications.</p>
<p>I'm unclear on why this is.</p>
<p>Is it because the compute capacity of such devices are typically very limited so it makes sense to offload processing to the cloud?</p>
<p>If this is the case, why is Lambda more effective for the purpose than a more typical server running on EC2 or EKS?</p>
<p>Is this assessment based purely on the costing model?</p>
",12075926.0,,4800344.0,,2021-10-13 09:31:22,2021-10-13 09:31:22,Why are AWS Lambda functions well suited to IOT applications?,<amazon-web-services><aws-lambda><iot><aws-iot>,2,0,,,,CC BY-SA 4.0,According to the AWS Solutions Architect labs that I am currently following  AWS Lambdas are ideal for IoT applications  Im unclear on why this is  Is it because the compute capacity of such devices are typically very limited so it makes sense to offload processing to the cloud  If this is the case  why is Lambda more effective for the purpose than a more typical server running on EC2 or EKS  Is this assessment based purely on the costing model  
69443194,1,,,2021-10-04 22:58:30,,0,50,"<p>I set up ALB + Lambda target group and while testing it, I noticed there was a constant flow of requests seemingly from crawlers / bots. The endpoint was not used in any webpages or backend services.</p>
<p>Questions</p>
<ol>
<li>Is that expected / normal?</li>
<li>What is the recommended way to filter them out?</li>
<li>Since these requests are triggering ALB and also Lambda, it incurs extra cost to us. Is there anything we can do about it?</li>
</ol>
<p>This histogram shows the number of requests I got.
<a href=""https://i.stack.imgur.com/k18qv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/k18qv.png"" alt=""Number of requests I get"" /></a></p>
<p>For example,<br />
1</p>
<pre><code>{'requestContext': {'elb': {'targetGroupArn': 'targetGroupARN'}}, '
httpMethod': 'GET', 'path': '/config/getuser', 
'queryStringParameters': {'index': '0'}, 'headers': {'accept': 'text/html,
application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8', 
'accept-encoding': 'gzip, deflate', 'accept-language': 'en-GB,en;q=0.5',
 'connection': 'close', 'host': '107.23.114.79:80', 
'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:76.0) 
Gecko/20100101 Firefox/76.0', 
'x-amzn-trace-id': 'traceId', 
'x-forwarded-for': '198.12.85.85', 
'x-forwarded-port': '80', 'x-forwarded-proto': 'http'}, 
'body': '', 'isBase64Encoded': False}
</code></pre>
<p>2</p>
<pre><code>{'requestContext': {'elb': {'targetGroupArn': 'targetGroupARN'}}, 
'httpMethod': 'GET', 'path': '/', 'queryStringParameters': {}, 
'headers': {'host': '18.214.78.215:80', 
'user-agent': 'https://gdnplus.com:Gather Analyze Provide.', 
'x-amzn-trace-id': 'traceId', 
'x-forwarded-for': '104.206.128.50', 'x-forwarded-port': '80', 
'x-forwarded-proto': 'http'}, 'body': '', 'isBase64Encoded': False}
</code></pre>
<p>3</p>
<pre><code>{'requestContext': {'elb': {'targetGroupArn': 'targetGroupARN'}}, 
'httpMethod': 'GET', 'path': '/', 'queryStringParameters': {}, 
'headers': {'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8', 
'accept-encoding': 'gzip, deflate', 'accept-language': 'en-GB,en;q=0.5', 
'connection': 'close', 'host': '107.23.114.79:80', 'upgrade-insecure-requests': '1',
 'user-agent': 'Linux Gnu (cow)', 
'x-amzn-trace-id': 'traceId', 
'x-forwarded-for': '209.141.56.209', 'x-forwarded-port': '80', 
'x-forwarded-proto': 'http'}, 'body': '', 'isBase64Encoded': False}
</code></pre>
",5042169.0,,,,,2021-10-04 22:58:30,How to filter out AWS Application Load Balancer Unknown Requests(Possibly from Bots / Crawlers)?,<amazon-ec2><aws-lambda><aws-application-load-balancer>,0,0,,,,CC BY-SA 4.0,I set up ALB + Lambda target group and while testing it  I noticed there was a constant flow of requests seemingly from crawlers / bots  The endpoint was not used in any webpages or backend services  Questions  Is that expected / normal  What is the recommended way to filter them out  Since these requests are triggering ALB and also Lambda  it incurs extra cost to us  Is there anything we can do about it   This histogram shows the number of requests I got   For example  1  2  3  
69497476,1,,,2021-10-08 14:26:21,,1,64,"<p>I'm using S3 Batch Operations to invoke a lambda to load a bunch of data into elasticsearch. Reserved concurrency is set to 1 on the lambda while experimenting with the right amount of concurrency.</p>
<p>Strangely when testing I see S3 calls the lambda a few times back-to-back, and then makes no more invocations for about 5 minutes. For example:</p>
<p><a href=""https://i.stack.imgur.com/xFkVp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xFkVp.png"" alt=""Lambda invocations"" /></a></p>
<p>Any ideas how to avoid this delay? There are no other batch operations happening on the account and priority is set to &quot;10&quot;. Maybe it has something to do with the reserved concurrency value? Although according to the <a href=""https://docs.aws.amazon.com/lambda/latest/dg/services-s3-batch.html"" rel=""nofollow noreferrer"">documentation</a>:</p>
<blockquote>
<p>When the job runs, Amazon S3 starts multiple function instances to process the Amazon S3 objects in parallel, up to the concurrency limit of the function. Amazon S3 limits the initial ramp-up of instances to avoid excess cost for smaller jobs.</p>
</blockquote>
<p>Thank you</p>
",2923125.0,,,,,2021-10-08 19:08:15,S3 Batch Operation - possible to avoid long delay between lambda invocations?,<amazon-web-services><amazon-s3><aws-lambda><s3-batch>,1,3,,,,CC BY-SA 4.0,Im using S3 Batch Operations to invoke a lambda to load a bunch of data into elasticsearch  Reserved concurrency is set to 1 on the lambda while experimenting with the right amount of concurrency  Strangely when testing I see S3 calls the lambda a few times back-to-back  and then makes no more invocations for about 5 minutes  For example:  Any ideas how to avoid this delay  There are no other batch operations happening on the account and priority is set to 10  Maybe it has something to do with the reserved concurrency value  Although according to the :  When the job runs  Amazon S3 starts multiple function instances to process the Amazon S3 objects in parallel  up to the concurrency limit of the function  Amazon S3 limits the initial ramp-up of instances to avoid excess cost for smaller jobs   Thank you 
69556747,1,,,2021-10-13 13:47:10,,0,22,"<p>I want to fetch storage capacity, storage type , Throughput capacity and backup storage for FSX and calculate the cost using aws price calculator.
I am new to this and don't have any idea how to do it.
Also let me know if i am going wrong
Thanks in advance</p>
",15571029.0,,174777.0,,2021-10-13 21:17:25,2021-10-13 21:17:25,How to calculate price for AWS FSx for lustre using aws price list api in aws lambda(python),<amazon-web-services><aws-lambda><fsx>,0,1,,,,CC BY-SA 4.0,I want to fetch storage capacity  storage type   Throughput capacity and backup storage for FSX and calculate the cost using aws price calculator  I am new to this and dont have any idea how to do it  Also let me know if i am going wrong Thanks in advance 
51407771,1,53974881.0,,2018-07-18 17:23:25,,3,1442,"<p>I am using Hashicorp Terraform to define an AWS API Gateway to hit a Lambda function. I have a requirement that I need to tag my AWS resources with a particular tag so that costs can be tracked. Terraform seems to allow this for most resources. However, when creating an API Gateway stage using <a href=""https://www.terraform.io/docs/providers/aws/r/api_gateway_deployment.html"" rel=""nofollow noreferrer"">aws_api_gateway_deployment</a> I do not have the option to specify tags.</p>

<p>I see that Terraform recently added the resource <a href=""https://www.terraform.io/docs/providers/aws/r/api_gateway_stage.html"" rel=""nofollow noreferrer"">aws_api_gateway_stage</a>. This one does allow tags to be specified. But, <strong>aws_api_gateway_stage</strong> requires an <strong>aws_api_gateway_deployment</strong>. If I give them the same ""stage_name"" as so:</p>

<pre><code>resource ""aws_api_gateway_stage"" ""PlayLambdaApiGatewayStage"" {
  stage_name = ""${environment}""
  rest_api_id = ""${aws_api_gateway_rest_api.PlayLambdaApiGateway.id}""
  deployment_id = ""${aws_api_gateway_deployment.PlayLambdaApiGatewayDeployment.id}""
  tags = {
    cost-allocation = ""play-${var.environment}""
  }
}

resource ""aws_api_gateway_deployment"" ""PlayLambdaApiGatewayDeployment"" {
  depends_on = [
    ""aws_api_gateway_integration.PlayLambdaApiLambdaIntegration"",
    ""aws_api_gateway_integration.PlayLambdaApiLambdaIntegrationRoot""
  ]

  rest_api_id = ""${aws_api_gateway_rest_api.PlayLambdaApiGateway.id}""
  stage_name  = ""${var.environment}""
}
</code></pre>

<p>Then they both resources try to create the stage and I get an error:</p>

<p><strong>aws_api_gateway_stage.PlayLambdaApiGatewayStage: Error creating API Gateway Stage: ConflictException: Stage already exists
    status code: 409, request id: f67a10c4-8aad-11e8-b486-c337ea2d214f</strong></p>

<p>Here it would seem that the <strong>aws_api_gateway_deployment</strong> already created the stage, so the <strong>aws_api_gateway_stage</strong> resource failed to create it also. If I add the stage to the deployment's ""depends_on"" so that the stage gets created first, it complains about there being a cycle between the two. </p>

<p>So, it seems like:</p>

<ul>
<li><strong>aws_api_gateway_stage</strong> is only intended to add additional stages to a deployment, rather than creating a stage to use for the deployment</li>
<li><strong>aws_api_gateway_deployment</strong> does not allow tags to be specified when it creates the stage.</li>
</ul>

<p>Any ideas? What am I missing?</p>
",1904246.0,,,,,2018-12-30 02:34:46,How to assign tags on an AWS API Gateway deployment stage using Terraform,<amazon-web-services><aws-lambda><terraform><aws-serverless>,1,1,1.0,,,CC BY-SA 4.0,I am using Hashicorp Terraform to define an AWS API Gateway to hit a Lambda function  I have a requirement that I need to tag my AWS resources with a particular tag so that costs can be tracked  Terraform seems to allow this for most resources  However  when creating an API Gateway stage using  I do not have the option to specify tags  I see that Terraform recently added the resource   This one does allow tags to be specified  But  aws_api_gateway_stage requires an aws_api_gateway_deployment  If I give them the same stage_name as so:  Then they both resources try to create the stage and I get an error: aws_api_gateway_stage PlayLambdaApiGatewayStage: Error creating API Gateway Stage: ConflictException: Stage already exists     status code: 409  request id: f67a10c4-8aad-11e8-b486-c337ea2d214f Here it would seem that the aws_api_gateway_deployment already created the stage  so the aws_api_gateway_stage resource failed to create it also  If I add the stage to the deployments depends_on so that the stage gets created first  it complains about there being a cycle between the two   So  it seems like:  aws_api_gateway_stage is only intended to add additional stages to a deployment  rather than creating a stage to use for the deployment aws_api_gateway_deployment does not allow tags to be specified when it creates the stage   Any ideas  What am I missing  
71486476,1,,,2022-03-15 17:25:03,,-2,45,"<p>I have 8 TB of on premise data at present. I need to transfer it to AWS S3. Going forward every month 800gb of data will be required to update.  What will be the cost of the different approaches?</p>
<ol>
<li>Run a python script in ec2 instance.</li>
<li>Use AWS Lambda for the transfer.</li>
<li>Use AWS DMS to transfer the data.</li>
</ol>
",12348767.0,,174777.0,,2022-03-15 21:29:47,2022-03-16 09:53:16,I have 6 TB of on premise data. I need to transfer it to AWS S3. What will be the cost of the different approaches,<amazon-web-services><amazon-s3><amazon-ec2><aws-lambda><aws-dms>,2,4,,,,CC BY-SA 4.0,I have 8 TB of on premise data at present  I need to transfer it to AWS S3  Going forward every month 800gb of data will be required to update   What will be the cost of the different approaches   Run a python script in ec2 instance  Use AWS Lambda for the transfer  Use AWS DMS to transfer the data   
71513209,1,,,2022-03-17 13:33:19,,0,12,"<p>I've been planning out an architecture that gets a request from a user, basically the user passes a piece of information to the system, the system performs some lookups, and updates a value in a database reflecting the new information. The application is latency sensitive, faster is better.</p>
<p>At first glance it seemed like SQS was a good fit. Basically, the request is picked up by lambda at edge, which performs some basic tasks and shoots a message off to SQS. The SQS message is picked up by a lambda, which processes it. Boom, good to go.</p>
<p>Here's what I think I might be missing. It sounds like if you're using long polling there's a minimum 1 second wait between when the message is sent and when its picked up. Is this correct? My assumption was that SQS could be used with latency-sensitive applications, but 1-2 seconds is a pretty big cost. If I'm understanding this right, what's the correct service to use here?</p>
",9904934.0,,,,,2022-03-17 13:33:19,SQS with low latency applications,<amazon-web-services><aws-lambda><amazon-sqs>,0,1,,,,CC BY-SA 4.0,Ive been planning out an architecture that gets a request from a user  basically the user passes a piece of information to the system  the system performs some lookups  and updates a value in a database reflecting the new information  The application is latency sensitive  faster is better  At first glance it seemed like SQS was a good fit  Basically  the request is picked up by lambda at edge  which performs some basic tasks and shoots a message off to SQS  The SQS message is picked up by a lambda  which processes it  Boom  good to go  Heres what I think I might be missing  It sounds like if youre using long polling theres a minimum 1 second wait between when the message is sent and when its picked up  Is this correct  My assumption was that SQS could be used with latency-sensitive applications  but 1-2 seconds is a pretty big cost  If Im understanding this right  whats the correct service to use here  
71517855,1,,,2022-03-17 19:13:59,,1,22,"<p>I have an RDS in one AWS Account - say Acct-1.
The RDS is public (i know it's not a good idea and there are other solutions for that)</p>
<p>I have a lambda in another AWS Account - say Acct-2 which runs in a VPC.</p>
<p>I have setup VPC peering between the 2 accounts, the route table entries are in place as well as the security groups IN/OUT bound policies in place.</p>
<p>In Acct-2 I can verify that I can connect to the RDS instance in Acct-1 using a mysql cient from an EC2 instance. The EC2 instance is in the same subnet as the Lambda and they both have the same security group.
But the Lambda gets a timeout connection. The Lambda has the typical Lambda execution role that Allows logs, and network interfaces.</p>
<p>Thoughts on what could be missing ? Does the RDS need to grant specific access to the Lambda service even if it's running in a VPC ?</p>
<p>Clarification: There is no route to the RDS instance from the internet. Clearly, the ec2 host is able to resolve the Private IP for the RDS instance from the DNS name and connect.
Lambda is unable to resolve the private IP for the RDS instance.
I'm trying to keep the traffic within AWS so as to not pay egress costs.</p>
",9866806.0,,13070.0,,2022-03-17 19:44:18,2022-03-17 19:44:18,Connection from Lambda to RDS in a different account,<amazon-web-services><aws-lambda><amazon-rds><amazon-vpc><aws-vpc-peering>,0,6,,,,CC BY-SA 4.0,I have an RDS in one AWS Account - say Acct-1  The RDS is public (i know its not a good idea and there are other solutions for that) I have a lambda in another AWS Account - say Acct-2 which runs in a VPC  I have setup VPC peering between the 2 accounts  the route table entries are in place as well as the security groups IN/OUT bound policies in place  In Acct-2 I can verify that I can connect to the RDS instance in Acct-1 using a mysql cient from an EC2 instance  The EC2 instance is in the same subnet as the Lambda and they both have the same security group  But the Lambda gets a timeout connection  The Lambda has the typical Lambda execution role that Allows logs  and network interfaces  Thoughts on what could be missing   Does the RDS need to grant specific access to the Lambda service even if its running in a VPC   Clarification: There is no route to the RDS instance from the internet  Clearly  the ec2 host is able to resolve the Private IP for the RDS instance from the DNS name and connect  Lambda is unable to resolve the private IP for the RDS instance  Im trying to keep the traffic within AWS so as to not pay egress costs  
71521903,1,,,2022-03-18 02:39:16,,0,37,"<p>I know that cloud functions charge money based on execution time and network costs.</p>
<p>I am writing a cloud function that calls 2 external APIs to authenticate a user. Both external APIs just return a single boolean and therefore don't send much info.</p>
<p>Which technique is better overall?</p>
<ol>
<li><p>Make both external API calls using <code>fetch()</code> at the <strong>same time</strong> knowing that 1 may be useless half the time.</p>
</li>
<li><p>Make the external API call <strong>one after the other</strong>, and not make the second one if not needed.</p>
</li>
</ol>
<p>I'm thinking doing #1 will shorten the &quot;wall time&quot; of my cloud function and therefore may be cheaper? But I make more external calls which may be expensive?</p>
<p>But doing #2 will make the cloud function stay in memory for longer and therefore more &quot;wall time&quot;.</p>
",1203580.0,,174777.0,,2022-03-18 03:42:07,2022-03-18 03:56:12,"Is it better to make 2 API calls and discard 1, or call one at a time when inside cloud functions?",<javascript><amazon-web-services><google-cloud-platform><aws-lambda>,1,0,,,,CC BY-SA 4.0,I know that cloud functions charge money based on execution time and network costs  I am writing a cloud function that calls 2 external APIs to authenticate a user  Both external APIs just return a single boolean and therefore dont send much info  Which technique is better overall   Make both external API calls using  at the same time knowing that 1 may be useless half the time   Make the external API call one after the other  and not make the second one if not needed    Im thinking doing #1 will shorten the wall time of my cloud function and therefore may be cheaper  But I make more external calls which may be expensive  But doing #2 will make the cloud function stay in memory for longer and therefore more wall time  
69641677,1,,,2021-10-20 07:06:30,,0,21,"<p>have a good day.
I'm new to AWS. I have created a web application using angular, a backend using NodeJS and Express framework. I copied the 'dist' folder(output of angular built) into the backend root and served the frontend from the app.js file of backend.
Deployed this backend to EC2 instance with elastic beanstalk. It was working great. The backend had a 'public' folder containing all the required '.png' icons to be served to the frontend UI on load in the browser.</p>
<p>But now the requirement is to deploy this solution in a serverless architecture since it is cost effective and zero server maintenance. I have deployed the angular dist folder to AWS amplify, working great. And deployed the NodeJS Express backend to AWS Lambda as a function using serverless framework available as a NodeJS library, got an endpoint link too that I already added in the angular frontend using proxy, so that all the requests for icons from browser will be redirected to the lambda based function endpoint.</p>
<p>The issue is that the front end is served to the browser from amplify but the icons are not getting loaded from the lambda based function backend. The icons are still in the 'public' folder of the backend which is deployed on the lambda using serverless framwork. The web app successfully connects with the backend in lambda, authorizes users successfully. The only thing it doesn't server those static images.</p>
<p>Is there any way that AWS lambda backend can serve the .png icons to the frontend in browser as the backend serves if deployed in AWS EC2 server.</p>
<p>Regards</p>
",12890582.0,,174777.0,,2021-10-20 08:19:22,2021-10-20 08:19:22,Can AWS Lambda serve static .png icons to front end UI from its /tmp folder located in the lambda function's source code,<node.js><angular><amazon-web-services><aws-lambda>,0,1,,,,CC BY-SA 4.0,have a good day  Im new to AWS  I have created a web application using angular  a backend using NodeJS and Express framework  I copied the dist folder(output of angular built) into the backend root and served the frontend from the app js file of backend  Deployed this backend to EC2 instance with elastic beanstalk  It was working great  The backend had a public folder containing all the required  png icons to be served to the frontend UI on load in the browser  But now the requirement is to deploy this solution in a serverless architecture since it is cost effective and zero server maintenance  I have deployed the angular dist folder to AWS amplify  working great  And deployed the NodeJS Express backend to AWS Lambda as a function using serverless framework available as a NodeJS library  got an endpoint link too that I already added in the angular frontend using proxy  so that all the requests for icons from browser will be redirected to the lambda based function endpoint  The issue is that the front end is served to the browser from amplify but the icons are not getting loaded from the lambda based function backend  The icons are still in the public folder of the backend which is deployed on the lambda using serverless framwork  The web app successfully connects with the backend in lambda  authorizes users successfully  The only thing it doesnt server those static images  Is there any way that AWS lambda backend can serve the  png icons to the frontend in browser as the backend serves if deployed in AWS EC2 server  Regards 
69656762,1,,,2021-10-21 06:16:19,,0,59,"<p>SQS <a href=""https://aws.amazon.com/sqs/pricing/"" rel=""nofollow noreferrer"">pricing page</a> mentioned</p>
<blockquote>
<p>Every Amazon SQS action counts as a request</p>
</blockquote>
<p>SQS pricing strategy has some pricing buckets, like <em>First 1 Million Requests/Month is Free</em>, <em>From 1 Million to 100 Billion Requests/Month for Standard Queue is $0.40</em>, etc.</p>
<p>As per <a href=""https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html"" rel=""nofollow noreferrer"">AWS doc</a>, to trigger lambda from SQS, three SQS actions are required as given below.</p>
<ol>
<li><code>sqs:ReceiveMessage</code></li>
<li><code>sqs:DeleteMessage</code> and</li>
<li><code>sqs:GetQueueAttributes</code>.</li>
</ol>
<p>My questions are</p>
<ol>
<li>For every lambda invocation (triggering) do all these three actions are execute?</li>
<li>If yes, do all these actions are treated as a request?</li>
</ol>
<p>In that case, every trigger for each 64KB payload chunk consumes 3 requests. If this understanding is wrong then what is the request count calculation for every trigger execution?</p>
<p>Thanks in advance</p>
",2569182.0,,484041.0,,2022-01-29 20:19:55,2022-01-29 20:19:55,SQS Billing when triggering lambda,<amazon-web-services><aws-lambda><amazon-sqs>,0,3,,,,CC BY-SA 4.0,SQS  mentioned  Every Amazon SQS action counts as a request  SQS pricing strategy has some pricing buckets  like First 1 Million Requests/Month is Free  From 1 Million to 100 Billion Requests/Month for Standard Queue is $0 40  etc  As per   to trigger lambda from SQS  three SQS actions are required as given below     and    My questions are  For every lambda invocation (triggering) do all these three actions are execute  If yes  do all these actions are treated as a request   In that case  every trigger for each 64KB payload chunk consumes 3 requests  If this understanding is wrong then what is the request count calculation for every trigger execution  Thanks in advance 
69674140,1,69836084.0,,2021-10-22 08:59:57,,0,48,"<p>I'm using axios in a lambda function to download a file from a user provided url. Obviously that file could be any size, and might be served at any speed. I am concerned that might create Denial of Service and Denial of Wallet risks.</p>
<p>I don't know if aws have any charges for lambda ingress, I haven't been able to find a definitive answer yet. Even if they don't though, large uploads could still force my lambdas to run for longer (costing me money) and potentially pushing me up against the rate limits I have set, in part, to mitigate flooding attack risk (denying people service).</p>
<p>Likewise, very slow downloads might cause my lambdas to run til they time out. My timeouts are set fairly high because there is processing to do once the file is downloaded. I'd rather bale after a small handful of seconds as the input data should always be small and fast.</p>
<p>So what I want is for downloads to abort if they hit a preset maximum size in bytes OR a maximum download time.</p>
<p>If adding these limits isn't possible with Axios then I'm open to using different libraries like node-fetch.</p>
",201436.0,,174777.0,,2021-10-23 03:35:26,2021-11-04 08:13:07,How can I cancel a download that's too big or slow in a lambda?,<amazon-web-services><aws-lambda><axios><fetch>,1,5,1.0,,,CC BY-SA 4.0,Im using axios in a lambda function to download a file from a user provided url  Obviously that file could be any size  and might be served at any speed  I am concerned that might create Denial of Service and Denial of Wallet risks  I dont know if aws have any charges for lambda ingress  I havent been able to find a definitive answer yet  Even if they dont though  large uploads could still force my lambdas to run for longer (costing me money) and potentially pushing me up against the rate limits I have set  in part  to mitigate flooding attack risk (denying people service)  Likewise  very slow downloads might cause my lambdas to run til they time out  My timeouts are set fairly high because there is processing to do once the file is downloaded  Id rather bale after a small handful of seconds as the input data should always be small and fast  So what I want is for downloads to abort if they hit a preset maximum size in bytes OR a maximum download time  If adding these limits isnt possible with Axios then Im open to using different libraries like node-fetch  
52473321,1,,,2018-09-24 05:47:50,,2,76,"<p>I'm new in AWS. For one project we require to purchase server on AWS. I don't know what configuration is required for the server. Our website will be like <a href=""https://www.justdial.com/"" rel=""nofollow noreferrer"">https://www.justdial.com/</a> and minimum 1000 users every time will be online on the website. Please, what configuration will be best with minimum pricing. I'm mentioning details below, what we want;</p>

<pre><code>&gt;      1 - Elastic IP 
&gt;      1 - Load Balancer
&gt;      2 - Webserver + autoscaling
&gt;      1 - Database SQL
&gt;      1 - S3 storage backup 
&gt;      CDN
</code></pre>

<p>if anything else is missing please guide me.</p>
",10406232.0,,6740036.0,,2018-09-24 07:27:26,2018-09-24 07:33:25,AWS Server Configuration,<amazon-web-services><amazon-ec2><aws-lambda>,2,0,,,,CC BY-SA 4.0,Im new in AWS  For one project we require to purchase server on AWS  I dont know what configuration is required for the server  Our website will be like  and minimum 1000 users every time will be online on the website  Please  what configuration will be best with minimum pricing  Im mentioning details below  what we want;  if anything else is missing please guide me  
52617888,1,,,2018-10-02 23:48:28,,0,136,"<h1>What I imagine</h1>

<p>On my website, I have a form to buy a product. Information needed are email and domain name.
Payment are managed by stripe and I use Lambda and API gateway to verify payment information.
If payment is successful, I'm sending email and domain name to SQS.</p>

<h1>What I've got</h1>

<p>The data is not sent to SQS right after the validation.
I don't know why but the first use of the form, nothing is set to SQS ; 
the second time the form is used, data form the first form are send to SQS ; the third time the form is used, it's data of the second form they are send to SQS, etc...</p>

<h1>Cant' find the problem on the lambda code</h1>

<pre><code>const stripe = require('stripe')(""sk_test_xxxxxxxxxxxxxxxxxxxxxxxx"");
const ApiBuilder = require('claudia-api-builder');
const querystring = require('querystring');
var api = new ApiBuilder();
var aws  = require('aws-sdk');
var sqs = new aws.SQS();

api.post('/stripe',request =&gt; {
    let params = querystring.parse(request.body);

    return stripe.charges.create({
        amount: params.package,
        currency: 'eur',
        description: `foo 12 month charge`,
        source: params.stripeToken,
        receipt_email: params.email,
        metadata: {domain_name: params.domain_name, email: params.email},
    }).then(charge =&gt; {
        console.log(""Data to send in SQS :"" + {domain_name:     params.domain_name, email: params.email});
        var sqsMsg = {domain_name: params.domain_name, email: params.email};
        console.log(""Data in variable : sqsMsg"");
        console.log(JSON.stringify(sqsMsg));
        sendToSQS(JSON.stringify(sqsMsg));

        return charge;
    }).catch((err) =&gt; {
        return err;
    }, { success: { contentType: 'text/html'}});
});

function sendToSQS(data) {
    var params = {
        MessageBody: JSON.stringify(data),
        QueueUrl: 'https://sqs.eu-west-1.amazonaws.com/98XXXXXXXXXXX/foo-new-order',
        DelaySeconds: 0
    };
    sqs.sendMessage(params, function(err, data) {
        if(err) {
            console.log(err);
        }
        else {
            console.log('Sending Data to SQS Queue Successful ', data);
        }
    });
}

module.exports = api;
</code></pre>

<p>If someone, can explain me what's wrong. Thanks :)</p>
",5971441.0,,174777.0,,2018-10-03 02:05:53,2018-10-03 02:05:53,Stripe payment sent to SQS not send right now... with lambda,<amazon-web-services><aws-lambda><amazon-sqs>,0,3,1.0,,,CC BY-SA 4.0,What I imagine On my website  I have a form to buy a product  Information needed are email and domain name  Payment are managed by stripe and I use Lambda and API gateway to verify payment information  If payment is successful  Im sending email and domain name to SQS  What Ive got The data is not sent to SQS right after the validation  I dont know why but the first use of the form  nothing is set to SQS ;  the second time the form is used  data form the first form are send to SQS ; the third time the form is used  its data of the second form they are send to SQS  etc    Cant find the problem on the lambda code  If someone  can explain me whats wrong  Thanks :) 
52633661,1,52633828.0,,2018-10-03 18:36:24,,3,1079,"<p>I have 2 dozen lambdas. I noticed my bill was high this month and I see have 700k invocations. Is there an easy way to view all the stats for all my lambdas in one view, and sort by invocations within period? This kind of dashboard would be very valuable.</p>
",387203.0,,1747018.0,,2018-10-08 06:33:08,2018-10-24 08:51:08,How to figure out lambdas that are contributing to my invocation counts/costs,<amazon-web-services><aws-lambda><serverless>,2,0,,,,CC BY-SA 4.0,I have 2 dozen lambdas  I noticed my bill was high this month and I see have 700k invocations  Is there an easy way to view all the stats for all my lambdas in one view  and sort by invocations within period  This kind of dashboard would be very valuable  
52665792,1,,,2018-10-05 12:35:46,,1,510,"<p>How is Lambda pricing calculated?
It is f(Memory<em>Time, # of Calls, Free Quotas).
So I setup my Lambda with memory of 1GB and a max time of 60 seconds.
Assume I have 10,000 calls and the average call time is 30 seconds.
How would I calculate the Memory</em>Time, do I use the max time of 60 seconds or the average call time of 30 seconds?
if it is 30 seconds then the max time set up with the Lambda is to put a cap on max processing time and is used for a run-away process.
Thanks,
Marc</p>
",1154422.0,,174777.0,,2018-10-06 01:33:25,2018-10-06 01:33:25,AWS Lambda Pricing,<amazon-web-services><aws-lambda>,2,0,,,,CC BY-SA 4.0,How is Lambda pricing calculated  It is f(MemoryTime  # of Calls  Free Quotas)  So I setup my Lambda with memory of 1GB and a max time of 60 seconds  Assume I have 10 000 calls and the average call time is 30 seconds  How would I calculate the MemoryTime  do I use the max time of 60 seconds or the average call time of 30 seconds  if it is 30 seconds then the max time set up with the Lambda is to put a cap on max processing time and is used for a run-away process  Thanks  Marc 
52683265,1,52691130.0,,2018-10-06 20:55:04,,2,3705,"<p>I'm thinking to launch an open-source project to develop a serverless CMS working on top of AWS technologies. I want to have DynamoDB as the backend for storing data rather than having a simple markdown CMS. I want to know from the community here if this idea is reasonable and sounds good because I'm moderately experienced in AWS and so not fully firm about it and seek some help.</p>

<p>I will be developing the CMS in ASP.NET MVC Core served though Lambda. This CMS will have an external API which will be ASP.NET MVC Core Web API.</p>

<p>I also need some thoughts on the cost of running this CMS i.e. Do you think it will be cheaper to run as against the conventional CMS or if DynamoDB can squeeze a lot of juice!!!</p>
",2375361.0,,,,,2020-07-01 08:31:05,Serverless CMS On Lambda & DynamoDB,<asp.net-mvc-5><aws-lambda><amazon-dynamodb><aws-api-gateway>,1,0,,2020-07-01 15:03:20,,CC BY-SA 4.0,Im thinking to launch an open-source project to develop a serverless CMS working on top of AWS technologies  I want to have DynamoDB as the backend for storing data rather than having a simple markdown CMS  I want to know from the community here if this idea is reasonable and sounds good because Im moderately experienced in AWS and so not fully firm about it and seek some help  I will be developing the CMS in ASP NET MVC Core served though Lambda  This CMS will have an external API which will be ASP NET MVC Core Web API  I also need some thoughts on the cost of running this CMS i e  Do you think it will be cheaper to run as against the conventional CMS or if DynamoDB can squeeze a lot of juice    
52718442,1,52725563.0,,2018-10-09 10:08:57,,-1,133,"<p>We already have Azure function in Microsoft Azure, AWS Lambda in AWS, Google Cloud Function in Google.</p>

<p>Then What are reasons do we need to use Spotinst function?</p>

<p>Wil Spotinst function replicated and running on all Cloud Providers such as Azure, AWS, Google and all regions at the same time when we choose all Cloud Providers and regions.</p>

<p>Which Cloud Providers will have to pay for running a Spotinst function?</p>
",1381266.0,,1381266.0,,2018-10-09 12:23:38,2018-10-09 16:32:55,"Why we need Spotinst function when we already have Azure function, AWS Lambda, Google cloud function",<aws-lambda><google-cloud-functions><azure-functions>,1,0,,,,CC BY-SA 4.0,We already have Azure function in Microsoft Azure  AWS Lambda in AWS  Google Cloud Function in Google  Then What are reasons do we need to use Spotinst function  Wil Spotinst function replicated and running on all Cloud Providers such as Azure  AWS  Google and all regions at the same time when we choose all Cloud Providers and regions  Which Cloud Providers will have to pay for running a Spotinst function  
52779003,1,,,2018-10-12 11:54:10,,0,34,"<p>For an interface from our VPC to a FTP Server in the WWW we think about realizing it with talend. From our Security Officer and some other sources I heard that connecting from a VPC via talend to a source in the WWW is not the best idea. So we came up with two other versions:</p>

<ul>
<li>Put a microservice in between </li>
<li>Use a lambda function and let the
function put the data into a bucket</li>
</ul>

<p>From security &amp; cost &amp; efficiency perspective I think V3 would be the best idea but maybe someone else experienced something different or having even a better idea?   </p>

<p><a href=""https://i.stack.imgur.com/4zyRX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4zyRX.png"" alt=""enter image description here""></a></p>
",9266098.0,,174777.0,,2018-10-13 02:31:21,2018-10-13 02:31:21,Best way: talend interfaces from VPC to WWW,<amazon-web-services><interface><aws-lambda><talend>,0,3,,,,CC BY-SA 4.0,For an interface from our VPC to a FTP Server in the WWW we think about realizing it with talend  From our Security Officer and some other sources I heard that connecting from a VPC via talend to a source in the WWW is not the best idea  So we came up with two other versions:  Put a microservice in between  Use a lambda function and let the function put the data into a bucket  From security &amp; cost &amp; efficiency perspective I think V3 would be the best idea but maybe someone else experienced something different or having even a better idea      
52834146,1,52835943.0,,2018-10-16 11:12:19,,2,2666,"<p>I'm using IoT Core for recieve MQTT and HTTPs petitions</p>

<p>Now I want to use something similar to azure cloud services to listen all the time from a devices whose send me data through UDP/TCP</p>

<p>It would be possible to use a Lambda function? Maybe this will be too expensive because it could be called thousands of times per day.</p>

<p>If I use an EC2, how I could do it to receive and then send this data, for example, to S3/DynamoDB?</p>
",10346701.0,,10346701.0,,2019-01-22 08:14:05,2019-07-17 04:19:48,UDP/TCP server in AWS,<amazon-web-services><amazon-ec2><aws-lambda>,2,0,1.0,,,CC BY-SA 4.0,Im using IoT Core for recieve MQTT and HTTPs petitions Now I want to use something similar to azure cloud services to listen all the time from a devices whose send me data through UDP/TCP It would be possible to use a Lambda function  Maybe this will be too expensive because it could be called thousands of times per day  If I use an EC2  how I could do it to receive and then send this data  for example  to S3/DynamoDB  
52862167,1,52865222.0,,2018-10-17 19:20:30,,4,3186,"<p>I am new to Serverless architecture using AWS Lambda and still trying to figure out how some of the pieces fit together. I have converted my website from EC2 (React client, and node API) to a serverless architecture. The React Client is now using s3 static web hosting and the API has been converted over to use AWS Lambda and API Gateway. </p>

<p>In my previous implementation I was using redis as a cache for caching responses from other third party API's. </p>

<p>API Gateway has the option to enable a cache, but I have also looked into Elasticache as an option. They are both comparable in price with API Gateway cache being slightly costlier. </p>

<p>The one issue I have run into when trying to use Elasticache is that it needs to be running in a VPC and I can no longer call out to my third party API's. </p>

<p>I am wondering if there is any benefit to using one over the other? Right now the main purpose of my cache is to reduce requests to the API but that may change over time. Would it make sense to have a Lambda dedicated to checking Elasticache first to see if there is a value stored and if not triggering another Lambda to retrieve the information from the API or is this even possible. Or for my use case would API Gateway cache be the better option?</p>

<p>Or possibly a completely different solution all together. Its a bit of a shame that mainly everything else will qualify for the free tier but having some sort of cache will add around $15 a month. </p>

<p>I am still very new to this kind of setup so any kind of help or direction would be greatly appreciated. Thank you!     </p>
",5407641.0,,5407641.0,,2018-10-17 23:12:04,2018-10-18 00:16:58,AWS Elasticache Vs API Gateway Cache,<amazon-web-services><caching><aws-lambda><aws-api-gateway><amazon-elasticache>,1,0,2.0,,,CC BY-SA 4.0,I am new to Serverless architecture using AWS Lambda and still trying to figure out how some of the pieces fit together  I have converted my website from EC2 (React client  and node API) to a serverless architecture  The React Client is now using s3 static web hosting and the API has been converted over to use AWS Lambda and API Gateway   In my previous implementation I was using redis as a cache for caching responses from other third party APIs   API Gateway has the option to enable a cache  but I have also looked into Elasticache as an option  They are both comparable in price with API Gateway cache being slightly costlier   The one issue I have run into when trying to use Elasticache is that it needs to be running in a VPC and I can no longer call out to my third party APIs   I am wondering if there is any benefit to using one over the other  Right now the main purpose of my cache is to reduce requests to the API but that may change over time  Would it make sense to have a Lambda dedicated to checking Elasticache first to see if there is a value stored and if not triggering another Lambda to retrieve the information from the API or is this even possible  Or for my use case would API Gateway cache be the better option  Or possibly a completely different solution all together  Its a bit of a shame that mainly everything else will qualify for the free tier but having some sort of cache will add around $15 a month   I am still very new to this kind of setup so any kind of help or direction would be greatly appreciated  Thank you       
52872443,1,52876077.0,,2018-10-18 10:57:29,,0,6400,"<p>According to <a href=""https://stackoverflow.com/questions/52172497/how-to-stream-data-from-amazon-sqs-to-files-in-amazon-s3"">this</a> if I want to create a lambda function to send the data from SQS to S3 each SQS message will be stored in an individual S3 object (I assume this is due the lambda function will be trigger each time the SQS recieve a message)</p>

<p>Is there any way to send, for example, all the messages that SQS received in the last 24 hours to the same S3 object?</p>

<p><strong>EDIT</strong></p>

<p>This could be the code to received the message from the queue and send it to S3</p>

<pre><code>var receiveMessageRequest = new ReceiveMessageRequest { QueueUrl = myQueueUrl };
 var receiveMessageResponse = sqs.ReceiveMessageAsync(receiveMessageRequest).GetAwaiter().GetResult(); ;
 while (receiveMessageResponse.Messages.Count &gt; 0)
 {
     if (receiveMessageResponse.Messages != null)
     {
         Console.WriteLine(""Printing received message.\n"");
         foreach (var message in receiveMessageResponse.Messages)
         {
             if (!string.IsNullOrEmpty(message.Body))
             {
                 &lt;...&gt; SEND TO S3
             }
         }
         var messageRecieptHandle = receiveMessageResponse.Messages[0].ReceiptHandle;
         //Deleting a message
         Console.WriteLine(""Deleting the message.\n"");
         var deleteRequest = new DeleteMessageRequest { QueueUrl = myQueueUrl, ReceiptHandle = messageRecieptHandle };
         sqs.DeleteMessageAsync(deleteRequest).GetAwaiter().GetResult();
     }
     receiveMessageRequest = new ReceiveMessageRequest { QueueUrl = myQueueUrl };
     receiveMessageResponse = sqs.ReceiveMessageAsync(receiveMessageRequest).GetAwaiter().GetResult();
}
</code></pre>

<p>But, which would be the best option to send to S3? I mean, in S3 I would pay to put request and if I do it element by element this could be quite inefficient.</p>

<p>I also imagine that storing items in memory would not be a good idea either, so im not sure what should I use for the best result</p>

<p>Other question: when I developed locally i used ReceiveMessage but in Lambda function I have to use ReceiveMessageAsync, why this?</p>
",10346701.0,,10346701.0,,2019-06-18 13:36:44,2019-06-18 13:36:44,Amazon SQS to files in Amazon S3,<amazon-web-services><amazon-s3><aws-lambda><amazon-sqs>,2,1,,,,CC BY-SA 4.0,According to  if I want to create a lambda function to send the data from SQS to S3 each SQS message will be stored in an individual S3 object (I assume this is due the lambda function will be trigger each time the SQS recieve a message) Is there any way to send  for example  all the messages that SQS received in the last 24 hours to the same S3 object  EDIT This could be the code to received the message from the queue and send it to S3  But  which would be the best option to send to S3  I mean  in S3 I would pay to put request and if I do it element by element this could be quite inefficient  I also imagine that storing items in memory would not be a good idea either  so im not sure what should I use for the best result Other question: when I developed locally i used ReceiveMessage but in Lambda function I have to use ReceiveMessageAsync  why this  
52903482,1,,,2018-10-20 07:38:22,,7,612,"<p>I've enjoyed working with <a href=""https://aws-amplify.github.io"" rel=""noreferrer"">AWS Amplify</a> a lot lately, its code generation for GraphQL queries based on defined schema is outstanding.</p>

<p>I came across one complication for defining custom logic / validation server-side. Out of the bag <a href=""https://aws.amazon.com/appsync"" rel=""noreferrer"">AppSync</a> (part responsible for GraphQL api in Amplify) generates resolvers and DynamoDB tables for your schema. Resolvers are created using <a href=""http://velocity.apache.org/engine/1.7/user-guide.html"" rel=""noreferrer"">Apache Velocity</a> templating language and if you are new to it, its a bit of a learning curve in my opinion.</p>

<p>Furthermore, these resolvers are auto generated by Amplify cli. I'm not sure if editing them makes sense either in AppSync console or locally, as every time we push api changes they will be auto generated again?</p>

<p>To add to this, these resolvers that are auto generated actually achieve a lot in terms of linking type models together, enabling search and authentication checks, I really don't want to touch them since development velocity enabled by automatic generation is insane.</p>

<p>Hence only other solution to introduce my custom logic seems to be Lambda functions that listen for create / update events of associated DynamoDB tables.</p>

<p>I think I can set this up in a way thats demonstrated below, essentially allowing users to use GraphQL api normally and when action that requires server validation is made react to it in lambda?</p>

<p>For example player adds item to their inventory, we fire lambda function to check if player had this item before, if not it was purchased, we validate item data and subtract gold of its cost from player table. I think this works fine but my concerns are</p>

<ol>
<li>We allow to write unvalidated data to database first (although it is validated by graphql type system and auth check prior.)</li>
<li>Additional costs for involving Lambda (in my opinion worth it for time saving and ability to use NodeJS instead of Apache Velocity to define language)</li>
</ol>

<p>Am I missing something else?</p>

<p><a href=""https://i.stack.imgur.com/EXn0z.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/EXn0z.png"" alt=""enter image description here""></a></p>

<p>So lambda will do validation behind the scenes, we assume majority of users are good actors here and data they pass to GraphQL api is correct since they use our client.</p>

<p>In case data is unexpected (bad actor) lambda will react and ban the user.</p>

<p>Is this solution viable / common, is there other alternative?</p>
",911930.0,,911930.0,,2018-10-20 09:07:04,2018-10-20 09:07:04,Running Lambda functions for server-side validation with AppSync and DynamoDB,<amazon-web-services><aws-lambda><amazon-dynamodb><aws-appsync><aws-amplify>,0,2,1.0,,,CC BY-SA 4.0,Ive enjoyed working with  a lot lately  its code generation for GraphQL queries based on defined schema is outstanding  I came across one complication for defining custom logic / validation server-side  Out of the bag  (part responsible for GraphQL api in Amplify) generates resolvers and DynamoDB tables for your schema  Resolvers are created using  templating language and if you are new to it  its a bit of a learning curve in my opinion  Furthermore  these resolvers are auto generated by Amplify cli  Im not sure if editing them makes sense either in AppSync console or locally  as every time we push api changes they will be auto generated again  To add to this  these resolvers that are auto generated actually achieve a lot in terms of linking type models together  enabling search and authentication checks  I really dont want to touch them since development velocity enabled by automatic generation is insane  Hence only other solution to introduce my custom logic seems to be Lambda functions that listen for create / update events of associated DynamoDB tables  I think I can set this up in a way thats demonstrated below  essentially allowing users to use GraphQL api normally and when action that requires server validation is made react to it in lambda  For example player adds item to their inventory  we fire lambda function to check if player had this item before  if not it was purchased  we validate item data and subtract gold of its cost from player table  I think this works fine but my concerns are  We allow to write unvalidated data to database first (although it is validated by graphql type system and auth check prior ) Additional costs for involving Lambda (in my opinion worth it for time saving and ability to use NodeJS instead of Apache Velocity to define language)  Am I missing something else   So lambda will do validation behind the scenes  we assume majority of users are good actors here and data they pass to GraphQL api is correct since they use our client  In case data is unexpected (bad actor) lambda will react and ban the user  Is this solution viable / common  is there other alternative  
52972258,1,52977300.0,,2018-10-24 15:02:29,,1,175,"<p>I'm new to AWS Lambda (and AWS in general). I need to write some development code for AWS.</p>

<p>Since Lambda functions are billed by execution time and number of requests, I'd like to guarantee that no out-of-control function or route spam would skyrocket costs out of my budget and put me in debt. (It is development code, so I expect there to be mistakes and I don't want them to be expensive ones.)</p>

<p>I know AWS has budget alarms which send you emails, but this is not good enough for me, since it might take days/weeks until I notice a message somewhere.</p>

<p>Is there a way to tell AWS to shut down a service if it is exceeding a budget? I'm looking for something similar to what DigitalOcean does, where you can set a fixed budget.</p>
",1932379.0,,9238547.0,,2018-10-30 15:28:13,2018-10-30 15:28:13,Guarantee limiting AWS Lambda functions to specified budget,<javascript><amazon-web-services><aws-lambda>,2,1,,,,CC BY-SA 4.0,Im new to AWS Lambda (and AWS in general)  I need to write some development code for AWS  Since Lambda functions are billed by execution time and number of requests  Id like to guarantee that no out-of-control function or route spam would skyrocket costs out of my budget and put me in debt  (It is development code  so I expect there to be mistakes and I dont want them to be expensive ones ) I know AWS has budget alarms which send you emails  but this is not good enough for me  since it might take days/weeks until I notice a message somewhere  Is there a way to tell AWS to shut down a service if it is exceeding a budget  Im looking for something similar to what DigitalOcean does  where you can set a fixed budget  
53017599,1,53049011.0,,2018-10-26 23:56:12,,3,464,"<p>Here's my setup:</p>

<p>A <em>Python 3.6 lambda function</em>, which I want to keep pre-warmed at a certain concurrency level (say, 10). The lambda's initialization is painful enough that I don't want to inflict this cost on visitors at random. I call these lambdas ""workers""</p>

<p>A <em>Node lambda function</em> which runs every 5 minutes to try to pre-warm 10 instances. It uses the Event invocation type for 9 of them, and RequestResponse for 1. There's only either one or zero of this lambda running at any one time. I call this a ""warmer"".</p>

<p>I followed the guidelines at [<a href=""https://www.jeremydaly.com/lambda-warmer-optimize-aws-lambda-function-cold-starts/]"" rel=""nofollow noreferrer"">https://www.jeremydaly.com/lambda-warmer-optimize-aws-lambda-function-cold-starts/]</a>, namely:</p>

<ul>
<li>Dont ping more often than every 5 minutes</li>
<li>Invoke the function directly (i.e. dont use API Gateway to invoke it)</li>
<li>Pass in a test payload that can be identified as such</li>
<li>Create handler logic that replies accordingly without running the whole function</li>
</ul>

<p>Here's a problem: this works great for several minutes. Then, as I watch the logs, I start to get timeouts from my worker lambda invocations. The timeouts quickly take over all the invocations that the warmer is trying to launch. </p>

<p>Now, no worker lambdas are prewarmed any more. But the warmer keeps on trying, on a Cloudwatch event cron schedule, suffering 100% timeouts. Finally, Lambda stops trying to launch my worker lambdas at all. It feels like some aspect of Lambda's getting its state scrambled. The only way to recover is to re-deploy the lambda. That buys me another hour with pre-warmed lambdas working.</p>

<p>Questions:</p>

<ul>
<li>How do I get visibility into why my worker lambdas start timing out, and then become completely non-responsive?</li>
<li>What is the definition of a ""Concurrent Execution""? On the main Lambda dashboard it shows me this chart of them. Yet, it seems to have more than twice as many Concurrent Executions as I'm requesting.</li>
</ul>

<p><a href=""https://i.stack.imgur.com/nJSAz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nJSAz.png"" alt=""enter image description here""></a>
Here's the warmup lambda code (Node):</p>

<pre><code>// warmer
""use strict"";

/** Generated by Serverless WarmUP Plugin at ${new Date().toISOString()} */
const aws = require(""aws-sdk"");
aws.config.region = ""${this.options.region}"";
const lambda = new aws.Lambda({httpOptions: {timeout: 60000}});
const functionNames = ${JSON.stringify(functionNames)};
const delay = ms =&gt; new Promise(res =&gt; setTimeout(res, ms))
const concurrency = 10;
module.exports.warmUp = async (event, context, callback) =&gt; {
  console.log(""Warm Up Start"");
  const invokes = await Promise.all(functionNames.map(async (functionName) =&gt; {

    let invocations = [];

    try {
      for(let i=1;i &lt;= concurrency;i++){
          let params = {
            FunctionName: functionName,
            InvocationType: (i===concurrency)?'RequestResponse': 'Event',
            LogType: 'None',
            Qualifier: process.env.SERVERLESS_ALIAS || ""$LATEST"",
            Payload: JSON.stringify({
              source: 'serverless-plugin-warmup',
              '__WARMER_INVOCATION__': i,
              '__WARMER_CONCURRENCY__': concurrency,
              '__WARMER_REQUESTED__': new Date().toISOString(),
            })
          };

          invocations.push(lambda.invoke(params).promise())
      }
      return await delay(75).then(Promise.all(invocations.map(p =&gt; p.catch(e =&gt; e)))
        .then(results =&gt; console.log('results', results))
        .catch(e =&gt; {
          console.log(e);
          return e;
        }
        ))
    } catch (e) {
      console.log(\`Warm Up Invoke Error: \${functionName}\`, e);
      return false;
    }
  }));

  console.log(\`Warm Up Finished\`);

}
</code></pre>

<p>And here's the worker lambda (Python):</p>

<pre><code>    source = event.get('source')
    if source == 'serverless-plugin-warmup':
        time.sleep(0.05)
        print(event)
        return lambda_gateway_response(200, {""status"": ""lambda warmup""})
</code></pre>
",5054082.0,,472495.0,,2018-10-30 22:18:46,2018-10-30 22:18:46,Lambdas stop invoking after a period of time,<aws-lambda><serverless-framework>,1,5,2.0,,,CC BY-SA 4.0,Heres my setup: A Python 3 6 lambda function  which I want to keep pre-warmed at a certain concurrency level (say  10)  The lambdas initialization is painful enough that I dont want to inflict this cost on visitors at random  I call these lambdas workers A Node lambda function which runs every 5 minutes to try to pre-warm 10 instances  It uses the Event invocation type for 9 of them  and RequestResponse for 1  Theres only either one or zero of this lambda running at any one time  I call this a warmer  I followed the guidelines at [  namely:  Dont ping more often than every 5 minutes Invoke the function directly (i e  dont use API Gateway to invoke it) Pass in a test payload that can be identified as such Create handler logic that replies accordingly without running the whole function  Heres a problem: this works great for several minutes  Then  as I watch the logs  I start to get timeouts from my worker lambda invocations  The timeouts quickly take over all the invocations that the warmer is trying to launch   Now  no worker lambdas are prewarmed any more  But the warmer keeps on trying  on a Cloudwatch event cron schedule  suffering 100% timeouts  Finally  Lambda stops trying to launch my worker lambdas at all  It feels like some aspect of Lambdas getting its state scrambled  The only way to recover is to re-deploy the lambda  That buys me another hour with pre-warmed lambdas working  Questions:  How do I get visibility into why my worker lambdas start timing out  and then become completely non-responsive  What is the definition of a Concurrent Execution  On the main Lambda dashboard it shows me this chart of them  Yet  it seems to have more than twice as many Concurrent Executions as Im requesting    Heres the warmup lambda code (Node):  And heres the worker lambda (Python):  
53027549,1,,,2018-10-28 01:07:19,,0,46,"<p>I am trying to enable Paytm as my payment gateway in the ecommerce website I am creating using <code>kitsune</code>.</p>

<p>When I enable the Paytm component in the IDE, I can see the following in my kitsune-settings.json file (the preview section)</p>

<pre><code>""preview"": [
  {
    ""domain"": ""example.com"",
    ""gateway"": ""paytm"",
    ""api_secret"": ""API_SECRET"",
    ""api_key"": ""API_KEY"",
    ""redirect_path"": ""/transaction_status"",
    ""api_url"": ""https://pguat.paytm.com"",
    ""payment_request_endpoint"": ""/oltp-web/processTransaction"",
    ""transaction_status_endpoint"": ""/oltp/HANDLER_INTERNAL/getTxnStatus?JsonData=""
  }
]
</code></pre>

<p>Where do I find the right values for API_SECRET and API_KEY ?</p>
",10014224.0,,7079025.0,,2018-10-28 04:30:28,2018-10-28 04:30:28,Enabling paytm payment gateway on kitsune websites,<web><serverless><paytm>,1,2,,,,CC BY-SA 4.0,I am trying to enable Paytm as my payment gateway in the ecommerce website I am creating using   When I enable the Paytm component in the IDE  I can see the following in my kitsune-settings json file (the preview section)  Where do I find the right values for API_SECRET and API_KEY   
53057390,1,,,2018-10-30 04:22:57,,14,2593,"<p>I'm trying to decide whether to use binary, number, or string for my DynamoDB table's partition key.  My application is a React.js/Node.js social event-management application where as much as half of the data volume stored in DynamoDB will be used to store relationships between Items and Attributes to other Items and Attributes. For example: friends of a user, attendees at an event, etc.</p>

<p>Because the schema is so key-heavy, and because the maximum DynamoDB Item size is only 400KB, and for perf &amp; cost reasons, I'm concerned about keys taking up too much space. That said, I want to use UUIDs for partition keys. There are well-known reasons to prefer UUIDs (or something with similar levels of entropy and minimal chance of collisions) for distributed, serverless apps where multiple nodes are giving out new keys.</p>

<p>So, I think my choices are: </p>

<ol>
<li>Use a hex-encoded UUID (32 bytes stored after dashes are removed)</li>
<li>Encode the UUID using base64 (22 bytes)</li>
<li>Encode the UUID using <a href=""https://rfc.zeromq.org/spec:32/Z85/"" rel=""noreferrer"">z85</a> (20 bytes)</li>
<li>Use a binary-typed attribute for the key (16 bytes)</li>
<li>Use a number-typed attribute for the key (16-18 bytes?) - the Number type can only accommodate 127 bits, so I'd have to perform some tricks like stripping a version bit, but for my app that's probably OK. See
<a href=""https://stackoverflow.com/questions/53056504/how-many-bits-of-integer-data-can-be-stored-in-a-dynamodb-attribute-of-type-numb"">How many bits of integer data can be stored in a DynamoDB attribute of type Number?</a> for more info.</li>
</ol>

<p>Obviously there's a tradeoff in developer experience. Using a hex string is the clearest but also the largest. Encoded strings are smaller but harder to deal with in logs, while debugging, etc. Binary and Number are harder than strings, but are the smallest. </p>

<p>I'm sure I'm not the first person to think about these tradeoffs.  Is there a well-known best practice or heuristic to determine how UUID keys should be stored in DynamoDB?</p>

<p>If not, then I'm leaning towards using the Binary type, because it's the smallest storage and because its native representation (as a base64-encoded string) can be used everywhere humans need to view and reason about keys, including queries, logging, and client code. Other than having to transform it to/from a <code>Buffer</code> if I use <code>DocumentClient</code>, am I missing some problem with the Binary type or advantage of one of the other options in the list above?</p>

<p>If it matters, I'm planning for all access to DynamoDB to happen via a Lambda API, so even if there's conversion or marshalling required, that's OK because I can do it inside my API. </p>

<p>BTW, this question is a sequel to a 4-year-old question (<a href=""https://stackoverflow.com/questions/21888923/uuid-data-type-in-dynamodb"">UUID data type in DynamoDB</a>) but 4 years is a looooooong time in a fast-evolving space, so I figured it was worth asking again.</p>
",126352.0,,126352.0,,2020-02-21 22:49:01,2020-07-09 22:48:09,binary vs. string vs. number for storing UUID in DynamoDB partition key?,<amazon-web-services><amazon-dynamodb><uuid><serverless-framework><aws-sdk-js>,1,4,1.0,,,CC BY-SA 4.0,Im trying to decide whether to use binary  number  or string for my DynamoDB tables partition key   My application is a React js/Node js social event-management application where as much as half of the data volume stored in DynamoDB will be used to store relationships between Items and Attributes to other Items and Attributes  For example: friends of a user  attendees at an event  etc  Because the schema is so key-heavy  and because the maximum DynamoDB Item size is only 400KB  and for perf &amp; cost reasons  Im concerned about keys taking up too much space  That said  I want to use UUIDs for partition keys  There are well-known reasons to prefer UUIDs (or something with similar levels of entropy and minimal chance of collisions) for distributed  serverless apps where multiple nodes are giving out new keys  So  I think my choices are:   Use a hex-encoded UUID (32 bytes stored after dashes are removed) Encode the UUID using base64 (22 bytes) Encode the UUID using  (20 bytes) Use a binary-typed attribute for the key (16 bytes) Use a number-typed attribute for the key (16-18 bytes ) - the Number type can only accommodate 127 bits  so Id have to perform some tricks like stripping a version bit  but for my app thats probably OK  See  for more info   Obviously theres a tradeoff in developer experience  Using a hex string is the clearest but also the largest  Encoded strings are smaller but harder to deal with in logs  while debugging  etc  Binary and Number are harder than strings  but are the smallest   Im sure Im not the first person to think about these tradeoffs   Is there a well-known best practice or heuristic to determine how UUID keys should be stored in DynamoDB  If not  then Im leaning towards using the Binary type  because its the smallest storage and because its native representation (as a base64-encoded string) can be used everywhere humans need to view and reason about keys  including queries  logging  and client code  Other than having to transform it to/from a  if I use   am I missing some problem with the Binary type or advantage of one of the other options in the list above  If it matters  Im planning for all access to DynamoDB to happen via a Lambda API  so even if theres conversion or marshalling required  thats OK because I can do it inside my API   BTW  this question is a sequel to a 4-year-old question () but 4 years is a looooooong time in a fast-evolving space  so I figured it was worth asking again  
53203024,1,53203215.0,,2018-11-08 07:19:14,,1,3754,"<p>Using ExpressJs and ClaudiaJs I have published a web server to AWS Lambda. This server's job is to process Stripe payments. I am trying to get my React SPA to submit the Stripe checkout from the client, but am receiving a CORS error when I try to submit. </p>

<p>How can I avoid this CORS error? I would imagine I need to either publish both client and AWS server on the same TLD (not sure how to make that work), or I'd need to disable CORS on the server (but that seems insecure).</p>

<p>Stripe server call:</p>

<pre><code>    this.handler = StripeCheckout.configure({
        key: test_key,
        locale: ""auto"",
        mode: ""no-cors"",
        name: ""Company 1234"",
        description: ""donation"",
        token: token =&gt; {
            // Send the donation to your server
            console.log(""server pinged"");

            fetch(`${backendUrl}/charge`, {
                method: ""POST"",
                headers: {
                    ""Content-Type"": ""application/json""
                },
                body: JSON.stringify({
                    stripeToken: token,
                    chargeAmount: this.state.donationAmount
                })
            })
                .then(res =&gt; res.json())
                .then(json =&gt; {
                    console.log(""response is "" + json);
                    this.setState({ donationResponse: ""Thank you for donating!"" });
                })
                .catch(error =&gt; {
                    this.setState({
                        donationResponse:
                            ""There was an issue processing your request. Please try again later""
                    });
                });
        }
    });
</code></pre>

<p>Form submission</p>

<pre><code>formSubmit = async event =&gt; {
    console.log(""form submitted"");
    event.preventDefault();

    const amount = this.state.donationAmount * 100; // Needs to be an integer in cents
    this.handler.open({
        amount: Math.round(amount)
    });
};
</code></pre>
",4180797.0,,,,,2018-11-08 07:38:44,Access to fetch at $AWS_LAMBDA_SITE from origin 'http://localhost:3000' has been blocked by CORS policy,<amazon-web-services><cors><aws-lambda><single-page-application>,1,0,,,,CC BY-SA 4.0,Using ExpressJs and ClaudiaJs I have published a web server to AWS Lambda  This servers job is to process Stripe payments  I am trying to get my React SPA to submit the Stripe checkout from the client  but am receiving a CORS error when I try to submit   How can I avoid this CORS error  I would imagine I need to either publish both client and AWS server on the same TLD (not sure how to make that work)  or Id need to disable CORS on the server (but that seems insecure)  Stripe server call:  Form submission  
53219978,1,,,2018-11-09 04:42:01,,0,109,"<p>Let say someone is brute forcing my EC2 website and i block that ip address using my EC2 ubuntu firewall and now if that user from that ip would access my website it will show 400 BAD REQUEST.</p>

<p>So my question is that will amazon charge me for this 400 BAD Request as amazon charges you for each request sent out of your instance.</p>
",4919607.0,,174777.0,,2018-11-09 05:07:50,2018-11-09 05:07:50,Amazon EC2 pricing for blocked IP addresses,<amazon-web-services><amazon-ec2><aws-lambda>,1,1,,,,CC BY-SA 4.0,Let say someone is brute forcing my EC2 website and i block that ip address using my EC2 ubuntu firewall and now if that user from that ip would access my website it will show 400 BAD REQUEST  So my question is that will amazon charge me for this 400 BAD Request as amazon charges you for each request sent out of your instance  
53312611,1,53313071.0,,2018-11-15 04:51:19,,5,2083,"<p>Is there any reference to connect lex bot with my rest api project?, i want to get pricing from my products , this enpoint is included in my project in a rest method (json request and response), however, I have tried to find any reference to call my rest api but I cant find any example.</p>

<p>The bot is using lambda function (nodejs), I am just beginning with amazon lex so I do not have any reference, I'm trying to modify the existing examples they provide (pizza ordering) but no idea by the moment!</p>

<p>Thanks in advance!</p>
",2777991.0,,,,,2020-07-29 13:12:10,Lex Bot Integration with Rest Api,<java><node.js><json><aws-lambda><aws-lex>,1,0,1.0,,,CC BY-SA 4.0,Is there any reference to connect lex bot with my rest api project   i want to get pricing from my products   this enpoint is included in my project in a rest method (json request and response)  however  I have tried to find any reference to call my rest api but I cant find any example  The bot is using lambda function (nodejs)  I am just beginning with amazon lex so I do not have any reference  Im trying to modify the existing examples they provide (pizza ordering) but no idea by the moment  Thanks in advance  
53372107,1,53379640.0,,2018-11-19 09:55:21,,16,4678,"<p>Currently, I have an AWS SQS as a trigger to my AWS Lambda function.</p>

<p>I would like to implement long polling to reduce costs since I've used up 70% of my monthly free tier, mostly from empty receives.</p>

<p>I tried setting up long polling by changing the queue attribute <code>ReceiveMessageWaitTimeSeconds</code> to <code>20 seconds</code>:</p>

<p><a href=""https://i.stack.imgur.com/sddPb.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/sddPb.png"" alt=""SQS Details showing ReceiveMessageWaitTimeSeconds set to 20s""></a></p>

<p>However, this didn't seem to reduce the number of empty receives, where the settings were changed on 11/19, between 2:00 - 3:00. 
<a href=""https://i.stack.imgur.com/dOjjI.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/dOjjI.png"" alt=""Empty Receives graph shows the same trend even after setting long polling""></a></p>

<p>According to the <a href=""https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-long-polling.html"" rel=""noreferrer"">AWS Documentation</a>, <code>WaitTimeSeconds</code> has priority over the queue attribute <code>ReceiveMessageWaitTimeSeconds</code></p>

<blockquote>
  <p>Short polling occurs when the WaitTimeSeconds parameter of a
  ReceiveMessage request is set to 0 in one of two ways:</p>
  
  <ul>
  <li>The ReceiveMessage call sets WaitTimeSeconds to 0.</li>
  <li>The ReceiveMessage call doesnt set WaitTimeSeconds, but the queue    attribute ReceiveMessageWaitTimeSeconds is set to 0.</li>
  </ul>
  
  <p><strong>Note</strong></p>
  
  <p>For the WaitTimeSeconds parameter of the ReceiveMessage action, a
  value set between 1 and 20 has priority over any value set for the
  queue attribute ReceiveMessageWaitTimeSeconds.</p>
</blockquote>

<p>Since AWS Lambda is receiving the SQS requests, I don't think <code>WaitTimeSeconds</code> can be configured. </p>

<p>Why doesn't my long polling configuration work in this situation? Am I misunderstanding something, or did I configure it wrong?</p>

<p>Thank you!</p>
",7110144.0,,,,,2018-11-19 17:15:11,AWS SQS Long Polling doesn't reduce empty receives,<amazon-web-services><aws-lambda><amazon-sqs><long-polling>,1,2,1.0,,,CC BY-SA 4.0,Currently  I have an AWS SQS as a trigger to my AWS Lambda function  I would like to implement long polling to reduce costs since Ive used up 70% of my monthly free tier  mostly from empty receives  I tried setting up long polling by changing the queue attribute  to :  However  this didnt seem to reduce the number of empty receives  where the settings were changed on 11/19  between 2:00 - 3:00    According to the    has priority over the queue attribute   Short polling occurs when the WaitTimeSeconds parameter of a   ReceiveMessage request is set to 0 in one of two ways:  The ReceiveMessage call sets WaitTimeSeconds to 0  The ReceiveMessage call doesnt set WaitTimeSeconds  but the queue    attribute ReceiveMessageWaitTimeSeconds is set to 0   Note For the WaitTimeSeconds parameter of the ReceiveMessage action  a   value set between 1 and 20 has priority over any value set for the   queue attribute ReceiveMessageWaitTimeSeconds   Since AWS Lambda is receiving the SQS requests  I dont think  can be configured   Why doesnt my long polling configuration work in this situation  Am I misunderstanding something  or did I configure it wrong  Thank you  
53467019,1,,,2018-11-25 11:34:20,,3,953,"<p>Before everything:
I checked familar questions like:</p>

<ul>
<li><p><a href=""https://stackoverflow.com/questions/52386062/access-aws-api-gateway-using-access-token-from-identityserver"">access AWS API gateway using access token from identityserver</a></p></li>
<li><p><a href=""https://stackoverflow.com/questions/50800104/aws-lambda-building-serverless-api-using-net-core"">AWS Lambda - Building serverless API using .NET Core</a></p></li>
</ul>

<p>And this one:</p>

<ul>
<li><a href=""https://aws.amazon.com/blogs/developer/running-serverless-asp-net-core-web-apis-with-amazon-lambda/"" rel=""nofollow noreferrer"">https://aws.amazon.com/blogs/developer/running-serverless-asp-net-core-web-apis-with-amazon-lambda/</a></li>
</ul>

<p>But I still have question:</p>

<p>So. Lets say I have Simple microservices project.</p>

<p>I have:</p>

<ul>
<li><strong>Ocelot</strong> - As Gateway</li>
<li><strong>Identity server 4</strong> - for All authorization and Authentication logic</li>
<li><strong>Asp.Core MVC</strong> - As my web project app.</li>
<li><strong>Microsetrvice A</strong> </li>
<li><strong>Microservcie B</strong></li>
<li><strong>RabbitMQ</strong> - Event bus </li>
<li>Some <strong>databases</strong> installed on premise....</li>
</ul>

<p>Now I want to migrate this to serverless architecture..</p>

<p>From what I understand, I see here 2 options:</p>

<ol>
<li><p>First Option </p>

<ul>
<li>Is Launch....<strong>EC2</strong> (And this mean I need to pay for EC2 instance + Scale this EC2 instances) instance for deploy my Identity server. </li>
<li>Build <strong>Authorization Lambda</strong>. </li>
<li>Then Build AWS API Gateway. And All endpoint(which actually will AWS LAMBDA)  which required  protect by <strong>Authorization Lambda</strong>??
This options mean...all time when I call Autorize protected endpoint - I will call 2 lambda funciton ( Authorize lambda + ) - If this correct?</li>
</ul></li>
<li><p>Second options, (IF I understand right) - Is Deploy <strong>Identity Server</strong> as <strong>AWS Lambda</strong> as well?</p></li>
</ol>

<p>Databases and <strong>RabbitMQ</strong> I switch to <strong>Kinesis/SQS</strong>, and <strong>databases</strong> to <strong>RDS/DynamoDB</strong></p>
",8669133.0,,,,,2018-11-25 11:34:20,.NET Core | IdentityServer | AWS | Serverless | Lambda,<amazon-web-services><amazon-ec2><aws-lambda><aws-serverless>,0,1,,,,CC BY-SA 4.0,Before everything: I checked familar questions like:     And this one:    But I still have question: So  Lets say I have Simple microservices project  I have:  Ocelot - As Gateway Identity server 4 - for All authorization and Authentication logic Asp Core MVC - As my web project app  Microsetrvice A  Microservcie B RabbitMQ - Event bus  Some databases installed on premise      Now I want to migrate this to serverless architecture   From what I understand  I see here 2 options:  First Option   Is Launch    EC2 (And this mean I need to pay for EC2 instance + Scale this EC2 instances) instance for deploy my Identity server   Build Authorization Lambda   Then Build AWS API Gateway  And All endpoint(which actually will AWS LAMBDA)  which required  protect by Authorization Lambda   This options mean   all time when I call Autorize protected endpoint - I will call 2 lambda funciton ( Authorize lambda + ) - If this correct   Second options  (IF I understand right) - Is Deploy Identity Server as AWS Lambda as well   Databases and RabbitMQ I switch to Kinesis/SQS  and databases to RDS/DynamoDB 
53498090,1,,,2018-11-27 10:55:47,,2,592,"<p>I've setup my CloudFront distribution to use a wildcard CNAME (*.domain.com)
I'd like to know the subdomain in my Lambda Proxy Integration. For this i need the Host header which is available in CloudFront Access Logs under x-host-header.</p>

<p>API Gateway assigns Host header to example.execute-api.<a href=""https://forums.aws.amazon.com/.amazonaws.com"" rel=""nofollow noreferrer"">https://forums.aws.amazon.com/.amazonaws.com</a> endpoint name.</p>

<p>Is it still not possible to realise this with API Gateway? Adding an additional lambda origin request adds too much overhead and cost.</p>

<p>Additional resource related to this:
Link: <a href=""https://stackoverflow.com/questions/44741358/get-cloudfront-custom-domain-in-the-headers-of-a-request"">Get CloudFront custom domain in the headers of a request</a>
Link: <a href=""https://forums.aws.amazon.com/thread.jspa?threadID=84588&amp;start=75&amp;tstart=0"" rel=""nofollow noreferrer"">https://forums.aws.amazon.com/thread.jspa?threadID=84588&amp;start=75&amp;tstart=0</a></p>
",2259635.0,,,,,2018-11-27 10:55:47,Forward Host header from CloudFront to API Gateway proxy integration,<amazon-web-services><aws-lambda><aws-api-gateway><amazon-cloudfront>,0,1,,,,CC BY-SA 4.0,Ive setup my CloudFront distribution to use a wildcard CNAME (* domain com) Id like to know the subdomain in my Lambda Proxy Integration  For this i need the Host header which is available in CloudFront Access Logs under x-host-header  API Gateway assigns Host header to example execute-api  endpoint name  Is it still not possible to realise this with API Gateway  Adding an additional lambda origin request adds too much overhead and cost  Additional resource related to this: Link:  Link:  
53523580,1,53525766.0,,2018-11-28 16:04:25,,0,432,"<p>We have an AWS Kinesis stream that ingests around 15 small binary messages per second. As a last resort data recovery strategy, we'd like to dump all messages received in an S3 bucket with 1-2 weeks TTL.</p>

<p>We could use a Lambda function to dump every Kinesis message to a new file in S3. But many small PUTs is expensive, especially because this data will not be accessed often (manually if so).</p>

<p>Alternatively, AWS Firehose would aggregate messages for us and push them to S3 as a single S3 object. But as I understand - please do correct me - Firehose simply concatenates records, so this doesn't work where messages binary and logically separate (unlike lines in a log file).</p>

<p>My current thoughts are to use a Lambda function attached to Firehose, so Firehose aggregates records over X minutes which we then zip/tar up, creating a file for each record, and send to S3 as a single archive.</p>

<p>Is this appropriate? If so, how do we aggregate records using Lambda? We process many-to-one, so I'm unsure what result/status codes to pass back to Firehose. <em>(The AWS ecosystem is very new to me, so I think I might've missed the obvious solution.)</em></p>
",2246637.0,,,,,2018-11-28 18:21:38,Push Firehose messages to an S3 bucket with minimal PUTs,<amazon-web-services><amazon-s3><aws-lambda><amazon-kinesis-firehose>,1,0,,,,CC BY-SA 4.0,We have an AWS Kinesis stream that ingests around 15 small binary messages per second  As a last resort data recovery strategy  wed like to dump all messages received in an S3 bucket with 1-2 weeks TTL  We could use a Lambda function to dump every Kinesis message to a new file in S3  But many small PUTs is expensive  especially because this data will not be accessed often (manually if so)  Alternatively  AWS Firehose would aggregate messages for us and push them to S3 as a single S3 object  But as I understand - please do correct me - Firehose simply concatenates records  so this doesnt work where messages binary and logically separate (unlike lines in a log file)  My current thoughts are to use a Lambda function attached to Firehose  so Firehose aggregates records over X minutes which we then zip/tar up  creating a file for each record  and send to S3 as a single archive  Is this appropriate  If so  how do we aggregate records using Lambda  We process many-to-one  so Im unsure what result/status codes to pass back to Firehose  (The AWS ecosystem is very new to me  so I think I mightve missed the obvious solution ) 
53609409,1,58391776.0,,2018-12-04 09:18:11,,16,10131,"<p>I have a Sagemaker Jupyter notebook instance that I keep leaving online overnight by mistake, unnecessarily costing money... </p>

<p>Is there any way to automatically stop the Sagemaker notebook instance when there is no activity for say, 1 hour? Or would I have to make a custom script?</p>
",653331.0,,,,,2021-10-06 18:35:08,"Automatically ""stop"" Sagemaker notebook instance after inactivity?",<amazon-web-services><aws-lambda><amazon-cloudwatch><amazon-sagemaker>,4,0,1.0,,,CC BY-SA 4.0,I have a Sagemaker Jupyter notebook instance that I keep leaving online overnight by mistake  unnecessarily costing money     Is there any way to automatically stop the Sagemaker notebook instance when there is no activity for say  1 hour  Or would I have to make a custom script  
53629318,1,,,2018-12-05 09:44:54,,4,791,"<h2>Question</h2>

<p>I've read <a href=""https://stackoverflow.com/questions/51313937/can-i-customize-partitioning-in-kinesis-firehose-before-delivering-to-s3/51343593#51343593"">this</a> and <a href=""https://stackoverflow.com/questions/45432265/partitioning-aws-kinesis-firehose-data-to-s3-by-payload/45432318#45432318"">this</a> and <a href=""https://forums.aws.amazon.com/thread.jspa?threadID=244663"" rel=""nofollow noreferrer"">this</a> articles. But they provide contradictory answers to the question: <em>how to customize partitioning on ingesting data to S3 from Kinesis Stream?</em> </p>

<h2>More details</h2>

<p>Currently, I'm using Firehose to deliver data from Kinesis Streams to Athena. Afterward, data will be processed with EMR Spark.</p>

<p>From time to time I have to handle historical bulk ingest into Kinesis Streams. The issue is that my Spark logic hardly depends on data partitioning and order of event handling. But Firehouse supports partitioning only by <code>ingestion_time</code> (into Kinesis Stream), not by any other custom field (I need by <code>event_time</code>). </p>

<p>For example, under Firehouse's partition <code>2018/12/05/12/some-file.gz</code> I can get data for the last few years. </p>

<h2>Workarounds</h2>

<p><em>Could you please help me to choose between the following options?</em></p>

<ol>
<li>Copy/partition data from Kinesis Steam with help of custom lambda. But this looks more complex and error-prone for me. Maybe because I'm not very familiar with AWS lambdas. Moreover, I'm not sure how well it will perform on bulk load. At <a href=""https://forums.aws.amazon.com/thread.jspa?threadID=244663"" rel=""nofollow noreferrer"">this article</a> it was said that Lambda option is much more expensive than Firehouse delivery. </li>
<li>Load data with Firehouse, then launch Spark EMR job to copy the data to another bucket with right partitioning. At least it sounds simpler for me (biased, I just starting with AWS Lambas). But it has the drawback of double-copy and additional spark Job.</li>
</ol>

<p>At one hour I could have up to 1M rows that take up to 40 MB of memory (at compressed state). From <a href=""https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html"" rel=""nofollow noreferrer"">Using AWS Lambda with Amazon Kinesis</a> I know that Kinesis to Lambda event sourcing has a limitation of 10,000 records per batch. Would it be effective to process such volume of data with Lambda?</p>
",2761509.0,,7452905.0,,2018-12-05 12:22:02,2021-06-14 17:46:36,Kinesis to S3 custom partitioning,<amazon-web-services><aws-lambda><amazon-emr><amazon-kinesis><amazon-kinesis-firehose>,2,0,,,,CC BY-SA 4.0,Question Ive read  and  and  articles  But they provide contradictory answers to the question: how to customize partitioning on ingesting data to S3 from Kinesis Stream   More details Currently  Im using Firehose to deliver data from Kinesis Streams to Athena  Afterward  data will be processed with EMR Spark  From time to time I have to handle historical bulk ingest into Kinesis Streams  The issue is that my Spark logic hardly depends on data partitioning and order of event handling  But Firehouse supports partitioning only by  (into Kinesis Stream)  not by any other custom field (I need by )   For example  under Firehouses partition  I can get data for the last few years   Workarounds Could you please help me to choose between the following options   Copy/partition data from Kinesis Steam with help of custom lambda  But this looks more complex and error-prone for me  Maybe because Im not very familiar with AWS lambdas  Moreover  Im not sure how well it will perform on bulk load  At  it was said that Lambda option is much more expensive than Firehouse delivery   Load data with Firehouse  then launch Spark EMR job to copy the data to another bucket with right partitioning  At least it sounds simpler for me (biased  I just starting with AWS Lambas)  But it has the drawback of double-copy and additional spark Job   At one hour I could have up to 1M rows that take up to 40 MB of memory (at compressed state)  From  I know that Kinesis to Lambda event sourcing has a limitation of 10 000 records per batch  Would it be effective to process such volume of data with Lambda  
53719673,1,,,2018-12-11 07:58:23,,0,288,"<p>I've got an iOS Shopping App and want to send crash dumps to an AWS Lambda Function.</p>

<p>To save costs for an API Gateway I want to send them directly to Lambda.</p>

<p>How can I authenticate the App and configure it so no other App can send crash dumps to my Lambda Function?</p>
",10695606.0,,174777.0,,2018-12-12 02:43:18,2019-10-01 09:11:06,How can I authenticate AWS Lambda without an API Gateway?,<amazon-web-services><aws-lambda><cloud>,2,6,,,,CC BY-SA 4.0,Ive got an iOS Shopping App and want to send crash dumps to an AWS Lambda Function  To save costs for an API Gateway I want to send them directly to Lambda  How can I authenticate the App and configure it so no other App can send crash dumps to my Lambda Function  
53716544,1,53849793.0,,2018-12-11 02:33:40,,1,886,"<p>We're hosting our webapp on CloudFront and S3. This infrastructure is configured in a Terraform module. We're using the same module (managed by Terragrunt) to deploy our webapp to our staging and production environments.</p>

<p>Obviously, we don't want public access to our staging environment. As such, we've created a Lambda function to enabled Basic HTTP Auth and are using the <code>lambda_function_association</code> within the <code>aws_cloudfront_distribution</code> resource to enable it.</p>

<p>The issue is we don't want the Lambda to run on our prod environment as well. I haven't been able to conditionally set the association on the resource.</p>

<p>I've also tried creating two resources with the same name and setting the <code>count</code> property so that only of the resources exists.</p>

<p>e.g.</p>

<pre><code># Basic Auth Guard
resource ""aws_cloudfront_distribution"" ""default"" {
  count = ""${var.behind_auth_guard}""
  ...
}

# No Basic Auth Guard
resource ""aws_cloudfront_distribution"" ""default"" {
  count = ""${var.behind_auth_guard ? 0 : 1}""
}
</code></pre>

<p>However when I try to deploy the code, I get <code>aws_cloudfront_distribution.default: resource repeated multiple times</code>.</p>

<p>Is there any way to achieve what I want?</p>

<p>Another option that I've considered is setting the Lambda on both versions, but having it not do anything in prod. However, this seems inefficient and costly as the Lamdba will be called on every request, and would like to avoid it if possible.</p>
",394491.0,,,,,2021-05-13 13:51:11,Optional CloudFront Lambda function association in Terraform,<aws-lambda><amazon-cloudfront><terraform>,2,0,0.0,,,CC BY-SA 4.0,Were hosting our webapp on CloudFront and S3  This infrastructure is configured in a Terraform module  Were using the same module (managed by Terragrunt) to deploy our webapp to our staging and production environments  Obviously  we dont want public access to our staging environment  As such  weve created a Lambda function to enabled Basic HTTP Auth and are using the  within the  resource to enable it  The issue is we dont want the Lambda to run on our prod environment as well  I havent been able to conditionally set the association on the resource  Ive also tried creating two resources with the same name and setting the  property so that only of the resources exists  e g   However when I try to deploy the code  I get   Is there any way to achieve what I want  Another option that Ive considered is setting the Lambda on both versions  but having it not do anything in prod  However  this seems inefficient and costly as the Lamdba will be called on every request  and would like to avoid it if possible  
53723822,1,53726426.0,,2018-12-11 12:04:00,,5,1689,"<p>I've seen a lot of people using SNS to trigger their lambda function rather than using the API gateway to do it. Any specific reasons to do this?</p>

<p>Personally i think allowing the API gateway to do this is a lot more flexible than using SNS. Any good elaboration as to why do this ? Would i get any performance or cost improvements if i use SNS to trigger the function?</p>
",626485.0,,,,,2020-01-23 16:24:45,"Why use SNS to trigger a lambda function, and not API gateway?",<amazon-web-services><aws-lambda><aws-api-gateway><amazon-sns>,1,1,2.0,,,CC BY-SA 4.0,Ive seen a lot of people using SNS to trigger their lambda function rather than using the API gateway to do it  Any specific reasons to do this  Personally i think allowing the API gateway to do this is a lot more flexible than using SNS  Any good elaboration as to why do this   Would i get any performance or cost improvements if i use SNS to trigger the function  
53726405,1,,,2018-12-11 14:36:32,,0,42,"<p>I have a DynamoDB table which contains columns names A and B (both numeric). I would like to find all rows which satisfy A-B &lt; 0. What would be the fastest way to do this assuming that we may have as many as 100000 rows that need to be scanned ?</p>

<p>I would be using a Lambda function so the idea is to have least possible memory and CPU cost.</p>
",2530978.0,,13070.0,,2018-12-11 14:59:20,2018-12-11 14:59:20,Fastest way to search DynamoDB by calculated value,<amazon-web-services><aws-lambda><amazon-dynamodb>,0,4,,,,CC BY-SA 4.0,I have a DynamoDB table which contains columns names A and B (both numeric)  I would like to find all rows which satisfy A-B &lt; 0  What would be the fastest way to do this assuming that we may have as many as 100000 rows that need to be scanned   I would be using a Lambda function so the idea is to have least possible memory and CPU cost  
53781371,1,,,2018-12-14 14:13:04,,1,228,"<p>I need to create something on Azure that can process incoming streams of messages for a set of entities. We will have anywhere between 20 and 2,000 entities at any point in time; these get created and discarded dynamically. Messages will be generated using our on-premises system, and sent to Azure using some queueing mechanism. Each message will be associated with a specific entity through an <code>EntityId</code> property. Messages belonging to the same entity must be processed in-order with respect to each other. </p>

<p>At the same time, the solution must be scalable with respect to entities. If I have steady streams of messages for 1,000 entities, I'd want to have 1,000 concurrent executions of my logic. If an entity takes a long time to process one of its messages, this must not block any of the other entities from processing <em>their</em> messages. Each message may take anywhere from 100ms to 10s to process (vast majority below 1s), and each entity would receive an average of one message per second.</p>

<p>Disappointingly, the Azure serverless stack does not seem to have any means of achieving this. These are the options I've considered and their problems:</p>

<ul>
<li><p>Azure Functions triggered by Service Bus queue with sessions. Azure Functions can be run as serverless on a consumption plan, making them perfect for elastic scaling. Service Bus sessions provide for in-order delivery, and are the closest implementation of my requirements. However, they are not supported in Azure Functions: <a href=""https://github.com/Azure/azure-functions-host/issues/563"" rel=""nofollow noreferrer"">Support Service Bus queues and topics which use sessions</a>.</p></li>
<li><p>Logic Apps triggered by Service Bus queue with sessions: This is supported out-of-the-box through the ""<a href=""https://blogs.msdn.microsoft.com/logicapps/2017/05/02/in-order-delivery-of-correlated-messages-in-logic-apps-by-using-service-bus-sessions/"" rel=""nofollow noreferrer"">Correlated in-order delivery using service bus sessions</a>"" template. The Logic App can then hook to an HTTP-triggered Azure Function for processing messages. The Logic App's only purpose is to prevent multiple messages belonging to the same entity/session from being processed concurrently. However, from the comments to my <a href=""https://stackoverflow.com/a/53589812/9685117"">former question</a>, I found this would not be scalable either. A Logic App can only execute 300,000 actions per 5 minutes, has a trigger concurrency limit of 50, and is said to be expensive. See <a href=""https://docs.microsoft.com/en-us/azure/logic-apps/logic-apps-limits-and-config"" rel=""nofollow noreferrer"">Limits and configuration information for Azure Logic Apps</a>.</p></li>
<li><p>Azure Event Hubs with partitions, as discussed in <a href=""https://medium.com/@jeffhollan/in-order-event-processing-with-azure-functions-bb661eb55428"" rel=""nofollow noreferrer"">In order event processing with Azure Functions</a>. This tends to be the most popular option, and the one recommended by Microsoft. However, Event Hubs only permit up to 32 partitions, with the number of partitions needing to be specified at creation: <a href=""https://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-features#partitions"" rel=""nofollow noreferrer"">Features and terminology in Azure Event Hubs</a>. This limitation of a fixed static set of partitions goes against the spirit of ""serverless""; if we're limiting our degree of parallelism to 32, then we're not getting any better scalability than a parallel application running on a 32-core machine. The partition limit can be increased beyond 32 via a support ticket to Microsoft, but I wouldn't want to ask for scalability that's two orders of magnitude beyond what's available for general use. Event Hubs also lack some other basic properties, such as at-most-once delivery.</p></li>
<li><p>We can dynamically create a Service Bus queue per entity, and have a singleton Azure Function spawned for it, bound exclusively to that specific queue. However, this would entail invoking the Azure resource management APIs as part of our operational code, and my impression is that Azure Functions weren't designed to be spawned dynamically this way.</p></li>
<li><p>Optimistic concurrency control against a persistent backing store, such as Redis, using the <code>SequenceNumber</code> property of the Service Bus queue messages for ordering. However, the programming model for this is quite complex and easy to get wrong  operations need to be performed in retry loops with consideration explicitly paid to atomicity and idempotency each time. Also, it requires all messages to contain the full entity state; otherwise, information would be lost when we discard stale messages in case of races. It would be expensive for us (in terms of computation and bandwidth) to send the full entity snapshot with each incremental change, so we'd rather find a means that would allow us to process incremental messages in-order instead.</p></li>
</ul>

<p>Is there any clean way of achieving in-order processing for a scalable number of entities on the Azure serverless stack?</p>
",9685117.0,,275715.0,,2019-03-24 21:00:38,2020-05-08 09:22:09,Scalable in-order message processing on Azure serverless,<azure><azure-functions><azure-logic-apps><serverless>,1,2,,,,CC BY-SA 4.0,I need to create something on Azure that can process incoming streams of messages for a set of entities  We will have anywhere between 20 and 2 000 entities at any point in time; these get created and discarded dynamically  Messages will be generated using our on-premises system  and sent to Azure using some queueing mechanism  Each message will be associated with a specific entity through an  property  Messages belonging to the same entity must be processed in-order with respect to each other   At the same time  the solution must be scalable with respect to entities  If I have steady streams of messages for 1 000 entities  Id want to have 1 000 concurrent executions of my logic  If an entity takes a long time to process one of its messages  this must not block any of the other entities from processing their messages  Each message may take anywhere from 100ms to 10s to process (vast majority below 1s)  and each entity would receive an average of one message per second  Disappointingly  the Azure serverless stack does not seem to have any means of achieving this  These are the options Ive considered and their problems:  Azure Functions triggered by Service Bus queue with sessions  Azure Functions can be run as serverless on a consumption plan  making them perfect for elastic scaling  Service Bus sessions provide for in-order delivery  and are the closest implementation of my requirements  However  they are not supported in Azure Functions:   Logic Apps triggered by Service Bus queue with sessions: This is supported out-of-the-box through the  template  The Logic App can then hook to an HTTP-triggered Azure Function for processing messages  The Logic Apps only purpose is to prevent multiple messages belonging to the same entity/session from being processed concurrently  However  from the comments to my   I found this would not be scalable either  A Logic App can only execute 300 000 actions per 5 minutes  has a trigger concurrency limit of 50  and is said to be expensive  See   Azure Event Hubs with partitions  as discussed in   This tends to be the most popular option  and the one recommended by Microsoft  However  Event Hubs only permit up to 32 partitions  with the number of partitions needing to be specified at creation:   This limitation of a fixed static set of partitions goes against the spirit of serverless; if were limiting our degree of parallelism to 32  then were not getting any better scalability than a parallel application running on a 32-core machine  The partition limit can be increased beyond 32 via a support ticket to Microsoft  but I wouldnt want to ask for scalability thats two orders of magnitude beyond whats available for general use  Event Hubs also lack some other basic properties  such as at-most-once delivery  We can dynamically create a Service Bus queue per entity  and have a singleton Azure Function spawned for it  bound exclusively to that specific queue  However  this would entail invoking the Azure resource management APIs as part of our operational code  and my impression is that Azure Functions werent designed to be spawned dynamically this way  Optimistic concurrency control against a persistent backing store  such as Redis  using the  property of the Service Bus queue messages for ordering  However  the programming model for this is quite complex and easy to get wrong  operations need to be performed in retry loops with consideration explicitly paid to atomicity and idempotency each time  Also  it requires all messages to contain the full entity state; otherwise  information would be lost when we discard stale messages in case of races  It would be expensive for us (in terms of computation and bandwidth) to send the full entity snapshot with each incremental change  so wed rather find a means that would allow us to process incremental messages in-order instead   Is there any clean way of achieving in-order processing for a scalable number of entities on the Azure serverless stack  
53811923,1,,,2018-12-17 09:07:10,,0,449,"<p>I am accessing <code>auroradb</code> service from my <code>java</code> <code>lambda</code> code. Here I set my lambda concurrency as 1. 
Since creating/closing database connection is an expensive process, I have created the <code>mysql</code> connection and made it static. So it will reuse the same connection every time. I haven't added the code to close the connection. </p>

<p>Will it cause any problems? </p>

<p>Will it automatically close after some days?</p>
",3343195.0,,,user10796762,2018-12-18 04:12:30,2018-12-18 04:12:30,AWS auroradb automatically close connection,<amazon-web-services><jdbc><serverless><aws-serverless><amazon-aurora>,1,1,,,,CC BY-SA 4.0,I am accessing  service from my   code  Here I set my lambda concurrency as 1   Since creating/closing database connection is an expensive process  I have created the  connection and made it static  So it will reuse the same connection every time  I havent added the code to close the connection   Will it cause any problems   Will it automatically close after some days  
53823944,1,,,2018-12-17 22:32:19,,1,319,"<p>I have a serverless application deployed using AWS Lambda. I'm using java8 as my language as I am also using Dagger for DI. One of the dependencies I'm calling has multiple endpoints I can use (It has the concept of multiple endpoints to ensure availability). </p>

<p>Now, this application needs to call the downstream service with the endpoint which is geographically close. I have a method which returns me the closest endpoint based on geographic distance. I'm trying to understand how I can get the Dagger module to ""refresh"" periodically so that its able to redirect it to the correct endpoint?  I have some relevant code below to illustrate the use case. </p>

<pre><code>@Provider
@Singleton
public MyDownstreamClient downstreamClient() {
    String endpointUrl = getClosestEndpoint(); 
    return MyDownstreamClient.builder()
        .endpoint(endpointURL) &lt;-- This will change often based on the closest endpoint
        .arguments(someArguments)
        .build();
}
</code></pre>

<p>Now, I'm trying to understand as to how this endpointURL can dynamically be generated. My confusion here is with the fact that since the Dagger provider isn't invoked on every API call/request, it would choose the same endpoint. I looked into the idea of instantiating the client inside my actual business logic and that seemed too expensive. </p>

<p>I'm trying to understand if there's a way by which we can force the Dagger module to refreshed periodically of some sort? Is that a possibility? </p>
",5001360.0,,,,,2018-12-17 22:32:19,Using Dagger in AWS lambda with refreshing module periodically,<java><aws-lambda><dagger>,0,0,,,,CC BY-SA 4.0,I have a serverless application deployed using AWS Lambda  Im using java8 as my language as I am also using Dagger for DI  One of the dependencies Im calling has multiple endpoints I can use (It has the concept of multiple endpoints to ensure availability)   Now  this application needs to call the downstream service with the endpoint which is geographically close  I have a method which returns me the closest endpoint based on geographic distance  Im trying to understand how I can get the Dagger module to refresh periodically so that its able to redirect it to the correct endpoint   I have some relevant code below to illustrate the use case    Now  Im trying to understand as to how this endpointURL can dynamically be generated  My confusion here is with the fact that since the Dagger provider isnt invoked on every API call/request  it would choose the same endpoint  I looked into the idea of instantiating the client inside my actual business logic and that seemed too expensive   Im trying to understand if theres a way by which we can force the Dagger module to refreshed periodically of some sort  Is that a possibility   
53931410,1,,,2018-12-26 11:23:49,,0,579,"<p>We are using AWS serverless architecture for our Contact center. We are storing audio recordings on S3 bucket and using lambda functions to process them.
Our requirement is to remove sensitive details from audio recording such as Payment information.
 So we need to fetch audio recording from S3 bucket and slice that using start time and duration for sensitive payment details and then join remaining recording clips into one.</p>

<p>How can we achieve this by using AWS lambda(NodeJS/Python), S3?</p>

<p>Thanks,
 Ganesh</p>
",10689370.0,,,,,2018-12-26 12:32:10,Slice Audio files using AWS lambda and S3 on serverless architecture,<node.js><amazon-web-services><amazon-s3><aws-lambda><aws-serverless>,1,0,1.0,,,CC BY-SA 4.0,We are using AWS serverless architecture for our Contact center  We are storing audio recordings on S3 bucket and using lambda functions to process them  Our requirement is to remove sensitive details from audio recording such as Payment information   So we need to fetch audio recording from S3 bucket and slice that using start time and duration for sensitive payment details and then join remaining recording clips into one  How can we achieve this by using AWS lambda(NodeJS/Python)  S3  Thanks   Ganesh 
53950399,1,53957807.0,,2018-12-27 20:16:07,,1,385,"<p>I have a multi-endpoint webservice written in Flask and running on API Gateway and Lambda thanks to <a href=""https://github.com/Miserlou/Zappa"" rel=""nofollow noreferrer"">Zappa</a>.</p>

<p>I have a second, very tiny, lambda, written in Node, that periodically hits one of the webservice endpoints. I do this by configuring the little lambda to have Internet access then use Node's <code>https.request</code> with these options:</p>

<pre><code>const options = {
  hostname: 'XXXXXXXXXX.execute-api.us-east-1.amazonaws.com',
  port: 443,
  path: '/path/to/my/endpoint',
  method: 'POST',
  headers: {
    'Authorization': `Bearer ${s3cretN0tSt0r3d1nTheC0de}`,
  }
};
</code></pre>

<p>and this works beautifully. But now I am wondering whether I <em>should</em> instead make the little lambda invoke the API endpoint directly using the AWS SDK. I have seen other S.O. questions on invoking lambdas from lambdas but I did not see any examples where the target lambda was a multi-endpoint webservice. All the examples I found used <code>new AWS.Lambda({...})</code> and then called <code>invokeFunction</code> with params.</p>

<p>Is there a way to pass, say, an event to the target lambda which contained the path of the specific endpoint I want to call? (and the auth headers, etc.)  * * * * OR * * * * is this just a really dumb idea, given that I have working code already? My thinking is that a direct SDK lambda invocation might (is this true?) bypass API Gateway and be cheaper, BUT, hitting the endpoint directly via API Gateway is better for logging. And since the periodic lambda runs once a day, it's probably free anyway.</p>

<p>If what I have now is best, that's a fine answer. A lambda invocation answer would be cool too, since I've not been able to find a good example in which the target lambda had multiple https endpoints.</p>
",831878.0,,,,,2018-12-28 11:25:39,Programmatically invoke a specific endpoint of a webservice hosted on AWS Lambda,<amazon-web-services><aws-lambda>,1,2,1.0,,,CC BY-SA 4.0,I have a multi-endpoint webservice written in Flask and running on API Gateway and Lambda thanks to   I have a second  very tiny  lambda  written in Node  that periodically hits one of the webservice endpoints  I do this by configuring the little lambda to have Internet access then use Nodes  with these options:  and this works beautifully  But now I am wondering whether I should instead make the little lambda invoke the API endpoint directly using the AWS SDK  I have seen other S O  questions on invoking lambdas from lambdas but I did not see any examples where the target lambda was a multi-endpoint webservice  All the examples I found used  and then called  with params  Is there a way to pass  say  an event to the target lambda which contained the path of the specific endpoint I want to call  (and the auth headers  etc )  * * * * OR * * * * is this just a really dumb idea  given that I have working code already  My thinking is that a direct SDK lambda invocation might (is this true ) bypass API Gateway and be cheaper  BUT  hitting the endpoint directly via API Gateway is better for logging  And since the periodic lambda runs once a day  its probably free anyway  If what I have now is best  thats a fine answer  A lambda invocation answer would be cool too  since Ive not been able to find a good example in which the target lambda had multiple https endpoints  
53974548,1,53975270.0,,2018-12-30 01:04:01,,3,1355,"<p>I have always learnt in theory that creating new connection to databases is costly operation. So we should keep open connection pool and use it for db operations.</p>

<p>When considering AWS lambda. Suppose lambda function wants to operate on db, then we need to create a connection to db. After operations, db needs to be closed. If simultaneously 100s of lambda function are executing then 100s of db connection open/close are being done. Which is theoretically bad pattern.</p>

<p>If this is so then is it inappropriate to use AWS lambda when db operations are involved?</p>
",1101527.0,,,,,2019-12-17 06:04:43,AWS lambda and Database,<aws-lambda><connection-pooling>,2,0,,2018-12-31 04:08:45,,CC BY-SA 4.0,I have always learnt in theory that creating new connection to databases is costly operation  So we should keep open connection pool and use it for db operations  When considering AWS lambda  Suppose lambda function wants to operate on db  then we need to create a connection to db  After operations  db needs to be closed  If simultaneously 100s of lambda function are executing then 100s of db connection open/close are being done  Which is theoretically bad pattern  If this is so then is it inappropriate to use AWS lambda when db operations are involved  
53990306,1,53990737.0,,2018-12-31 18:16:00,,1,379,"<p>I have to upload video files into an S3 bucket from my React web application. I am currently developing a simple react application and from this application, I am trying to upload video files into an S3 bucket so I have decided two approaches for implementing the uploading part.</p>

<p><strong>1) Amazon EC2 instance:</strong> From the front-end, I am hitting the API and the server is running in the Amazon EC2 instance. So I can upload the files into S3 bucket from the ec2 instance.</p>

<p><strong>2) Amazon API Gateway + Lambda:</strong> I am directly sending the local files into an S3 bucket through API + Lambda function by calling the https URL with data.</p>

<p>But I am not happy with these two methods because both are more costly. I have to upload files into an S3 bucket, and the files are more than 200MB. I don't know I can optimize this uploading process. Video uploading part is necessary for my application and I should be very careful to do this part and also I have to increase the performance and cost-effective.</p>

<p>If someone knows any solution please share with me, I will be very helpful for me to continue my process.</p>

<p>Thanks in advance.</p>
",10118996.0,,,,,2018-12-31 19:32:04,Best choice of uploading files into S3 bucket,<amazon-web-services><amazon-s3><amazon-ec2><aws-lambda><aws-api-gateway>,2,0,,,,CC BY-SA 4.0,I have to upload video files into an S3 bucket from my React web application  I am currently developing a simple react application and from this application  I am trying to upload video files into an S3 bucket so I have decided two approaches for implementing the uploading part  1) Amazon EC2 instance: From the front-end  I am hitting the API and the server is running in the Amazon EC2 instance  So I can upload the files into S3 bucket from the ec2 instance  2) Amazon API Gateway + Lambda: I am directly sending the local files into an S3 bucket through API + Lambda function by calling the https URL with data  But I am not happy with these two methods because both are more costly  I have to upload files into an S3 bucket  and the files are more than 200MB  I dont know I can optimize this uploading process  Video uploading part is necessary for my application and I should be very careful to do this part and also I have to increase the performance and cost-effective  If someone knows any solution please share with me  I will be very helpful for me to continue my process  Thanks in advance  
54077437,1,,,2019-01-07 15:42:25,,3,1361,"<p>I am experiencing unexpected spikes in timeouts for an AWS Lambda function. The function is called ~7 million times per day - with a constant numbers of calls occurring every 5 minutes. The spikes I see are usually ~1,000 function timeouts within a half hour period. The rest of the day there are generally no timeouts.</p>

<p>Here is an example day with timeouts counted on the y-axis.</p>

<p><a href=""https://i.stack.imgur.com/ezpkj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ezpkj.png"" alt=""Timeouts over the course of a day""></a></p>

<p>The timeout setting on the function is 30 seconds. The function has an average runtime of ~50ms and an expected maximum runtime of ~5 seconds for large input. The function uses the Python3.6 runtime and does not utilize the ENI/VPC Lambda feature, thus cold starts generally only take a couple of seconds.</p>

<p>In order to investigate the timeouts, I dug through the CloudWatch logs while the timeouts were occurring. There were no log messages or exceptions from the timed-out invocations, just the message: <code>Task timed out after 30.03 seconds</code>. </p>

<p>Originally the function ran in a single thread, so I assumed the code might be hanging somewhere and failing to log. I attempted to add more information to the logs by modifying the function to operate like this:</p>

<pre><code>def business_logic():
    ...
    queue.put(result)

def lambda_handler(event, context):
    thread = threading.Thread(target=business_logic)
    thead.start()

    while queue.empty():
        if seconds_running &gt; 10:
            comprehensive_logging()
</code></pre>

<p>I tested this functioned correctly by adding sleeps in <code>business_logic</code> and confirming that the function did <code>comprehensive_logging</code>. However, during the actual timeouts <code>comprehensive_logging</code> was <strong>never</strong> reached. </p>

<p>From this experiment I concluded that the timed-out Lambda invocation was never reaching my code. Due to the spiking nature of the timeouts, <em>I have a suspicion that one of the Lambda microVMs running my function is getting into a bad state, and is unable to process requests for a period before it is recycled</em>. </p>

<p><strong>Question</strong>:</p>

<ul>
<li>Does this theory hold water?</li>
<li>Are there other possible causes I may have overlooked?</li>
<li>Has anyone experienced anything similar?</li>
</ul>

<p>Other details:</p>

<ul>
<li>The functions is invoked directly with a SigV4 signed request</li>
<li>I use the <a href=""https://chalice.readthedocs.io/en/latest/index.html"" rel=""nofollow noreferrer"">Chalice</a> library to process requests.</li>
<li>There are a few large binary files that Python interacts with on every execution through SWIG</li>
<li>The function <strong>does not</strong> interact with any external resources</li>
</ul>

<p>Note: I am reluctant to add comprehensive logging for all calls due to the cost of CloudWatch. </p>
",4027688.0,,4027688.0,,2019-01-07 21:49:36,2019-01-07 21:49:36,AWS Lambda Timeouts,<python><python-3.x><amazon-web-services><aws-lambda><timeout>,1,3,,,,CC BY-SA 4.0,I am experiencing unexpected spikes in timeouts for an AWS Lambda function  The function is called ~7 million times per day - with a constant numbers of calls occurring every 5 minutes  The spikes I see are usually ~1 000 function timeouts within a half hour period  The rest of the day there are generally no timeouts  Here is an example day with timeouts counted on the y-axis   The timeout setting on the function is 30 seconds  The function has an average runtime of ~50ms and an expected maximum runtime of ~5 seconds for large input  The function uses the Python3 6 runtime and does not utilize the ENI/VPC Lambda feature  thus cold starts generally only take a couple of seconds  In order to investigate the timeouts  I dug through the CloudWatch logs while the timeouts were occurring  There were no log messages or exceptions from the timed-out invocations  just the message:    Originally the function ran in a single thread  so I assumed the code might be hanging somewhere and failing to log  I attempted to add more information to the logs by modifying the function to operate like this:  I tested this functioned correctly by adding sleeps in  and confirming that the function did   However  during the actual timeouts  was never reached   From this experiment I concluded that the timed-out Lambda invocation was never reaching my code  Due to the spiking nature of the timeouts  I have a suspicion that one of the Lambda microVMs running my function is getting into a bad state  and is unable to process requests for a period before it is recycled   Question:  Does this theory hold water  Are there other possible causes I may have overlooked  Has anyone experienced anything similar   Other details:  The functions is invoked directly with a SigV4 signed request I use the  library to process requests  There are a few large binary files that Python interacts with on every execution through SWIG The function does not interact with any external resources  Note: I am reluctant to add comprehensive logging for all calls due to the cost of CloudWatch   
54923071,1,,,2019-02-28 10:07:23,,0,830,"<p>One Lambda is calling another in a synchronous manner, gets some data and continues in processing. </p>

<p>This means, the costs are taken into account for both Lambdas in the call time period.</p>

<p>Is it possible to use AWS Step Functions in this case without breaking the processing into a state machine blocks, which would make the code too complex?</p>

<p>Node.js 8.10 code:</p>

<pre><code>const AWS = require('aws-sdk');
const lambda = new AWS.Lambda({ apiVersion: '2015-03-31' });

exports.handle = async event =&gt; {
    try {
        const packageId = event.pathParameters['id'];

        const entry = await packageEntry(packageId);            
        const tags = await tagsForEntry(entry.id);

        return httpResponse(200, { ...entry, tags });
    }
    catch (err) {
        return httpResponse(500, err.message);
    }
}

async function tagsForEntry(entryId) {
    const response = await lambda.invoke({
        FunctionName: process.env.TAGGING_LAMBDA,
        InvocationType: 'RequestResponse',
        Payload: JSON.stringify({
            method: 'GET_TAGS',
            payload: { entryId }
        })
    }).promise();
    const payload = response.Payload ? JSON.parse(response.Payload.toString()) : {};
    return payload.tags || [];
}

async function packageEntry(packageId) {
    // return a package entry from a database
}

function httpResponse(statusCode, data) {
    // return a HTTP response object
}
</code></pre>
",2190498.0,,2190498.0,,2019-02-28 10:28:00,2019-02-28 10:28:00,Invoke AWS Lambda via AWS Step Functions?,<amazon-web-services><aws-lambda><aws-step-functions>,1,0,,,,CC BY-SA 4.0,One Lambda is calling another in a synchronous manner  gets some data and continues in processing   This means  the costs are taken into account for both Lambdas in the call time period  Is it possible to use AWS Step Functions in this case without breaking the processing into a state machine blocks  which would make the code too complex  Node js 8 10 code:  
65240422,1,,,2020-12-10 18:39:59,,0,120,"<p>Lambda Specs: Python Version - 3.7 || Memory - 10240MB (10GB)</p>
<p>The Synchronous API timeout limit is 30 seconds. My code executes for 4 seconds. The below code is a sample one that sleeps for 4 seconds.
I'm using EFS mounted in Ubuntu for storing the packages as the limit of lambda deployment is 250MB.</p>
<pre><code>import time
import json
import os
import sys 
sys.path.append(&quot;/mnt/access&quot;)
import math
import pandas as pd
import statsmodels.api as sm
import numpy as np
from datetime import datetime, timedelta
import sqlalchemy
from statsmodels.stats.outliers_influence import variance_inflation_factor

def lambda_handler(event, context):
    time.sleep(4)
</code></pre>
<ol>
<li>This is a simple code integrated with API Gateway. It works fine when executed one or two times but when executed more than 10 times it runs into a timeout error.</li>
<li>I tried to use Provisioned Concurrency but it is so expensive. What to do to make it work all the time? Any Alternatives?</li>
</ol>
",12984637.0,,12984637.0,,2020-12-11 10:07:17,2020-12-11 10:07:17,"Executing API of a AWS Lambda function multiple times throws ""message"": ""Endpoint request timed out"". How to resolve this problem?",<amazon-web-services><aws-lambda><aws-api-gateway><api-gateway>,1,5,,,,CC BY-SA 4.0,Lambda Specs: Python Version - 3 7 || Memory - 10240MB (10GB) The Synchronous API timeout limit is 30 seconds  My code executes for 4 seconds  The below code is a sample one that sleeps for 4 seconds  Im using EFS mounted in Ubuntu for storing the packages as the limit of lambda deployment is 250MB    This is a simple code integrated with API Gateway  It works fine when executed one or two times but when executed more than 10 times it runs into a timeout error  I tried to use Provisioned Concurrency but it is so expensive  What to do to make it work all the time  Any Alternatives   
69707921,1,69711624.0,,2021-10-25 12:17:11,,0,104,"<p>I have registered a free tier AWS Lambda account and created a simple, public service for me and others to play around with. However, since I do not know yet how usage is going to be, I want to be careful for now. Otherwise someone could simply flood my service with one million requests and I get billed for it. I'd rather not have the service available.</p>
<p>Therefore, I want to create a budget action that shuts down all services as soon as $0.01 is exceeded. The way I've done this is that I've granted the Lambda service role (which was auto-created when I setup the lambda service) the budget permission (budgets.amazonaws.com) and then have an IAM action setup that adds the <code>AWSDenyAll</code> managed policy to the role itself once the budget is exceeded.</p>
<p>This does not seem to work. If I manually attach the <code>AWSDenyAll</code> policy, the Lambda service still is available. My understanding of the roles/policies system may be also fundamentally wrong.</p>
<p>How can I achieve a &quot;total shutdown&quot; action that can be triggered from a budget alert?</p>
",4146541.0,,4800344.0,,2021-10-25 13:05:34,2021-10-25 16:26:46,Creating budget actions to shutdown Lambda,<amazon-web-services><aws-lambda>,1,2,,,,CC BY-SA 4.0,I have registered a free tier AWS Lambda account and created a simple  public service for me and others to play around with  However  since I do not know yet how usage is going to be  I want to be careful for now  Otherwise someone could simply flood my service with one million requests and I get billed for it  Id rather not have the service available  Therefore  I want to create a budget action that shuts down all services as soon as $0 01 is exceeded  The way Ive done this is that Ive granted the Lambda service role (which was auto-created when I setup the lambda service) the budget permission (budgets amazonaws com) and then have an IAM action setup that adds the  managed policy to the role itself once the budget is exceeded  This does not seem to work  If I manually attach the  policy  the Lambda service still is available  My understanding of the roles/policies system may be also fundamentally wrong  How can I achieve a total shutdown action that can be triggered from a budget alert  
69724457,1,,,2021-10-26 13:56:20,,1,69,"<p>I am using AWS Lambda to host a nodeJs service that fetch my open invoices on Stripe and execute a payment and update my database.
The problem is that most of the time, but not all the time (sometimes everything goes how it should), it hang on the call of invoice list and do nothing.</p>
<p>Here's the part of the code where log stops :</p>
<pre class=""lang-javascript prettyprint-override""><code>const stripe = require('stripe')(process.env.STRIPE_SECRET_KEY, {
    maxNetworkRetries: 1,
    timeout: 2000
});
[other imports]

const microservice = async (event, context, callback) =&gt; {
    [some code including database connection]

    console.log('retrieving all open invoices...')
    let invoices;
    try {
        invoices = await stripe.invoices.list({
            status: 'open',
            limit: 100,
        });
        console.log(invoices.data.length + ' data retrieved.');
    } catch (error) {
        console.log('Unable fetch stripe invoices : ', error);
        console.log('Exiting due to stripe connection error.');
        reports.push(new Report('Unable fetch stripe invoices', 'ERROR'));
        return {
        statusCode: 500,
        };
    }
    
    [code that process invoices]

    return {};
};
module.exports.microservice = microservice;
</code></pre>
<p>And here the log output :</p>
<pre><code>START RequestId: d628aa1e-dee6-4cc6-9ce0-f7c11cf73249 Version: $LATEST
2021-10-26T00:04:05.741Z d628aa1e-dee6-4cc6-9ce0-f7c11cf73249 INFO Connecting to database...
2021-10-26T00:04:05.929Z d628aa1e-dee6-4cc6-9ce0-f7c11cf73249 INFO Executing (default): SELECT 1+1 AS result
2021-10-26T00:04:05.931Z d628aa1e-dee6-4cc6-9ce0-f7c11cf73249 INFO Connection has been established successfully.
2021-10-26T00:04:05.931Z d628aa1e-dee6-4cc6-9ce0-f7c11cf73249 INFO retrieving all open invoices...
END RequestId: d628aa1e-dee6-4cc6-9ce0-f7c11cf73249
REPORT RequestId: d628aa1e-dee6-4cc6-9ce0-f7c11cf73249 Duration: 15015.49 ms Billed Duration: 15000 ms Memory Size: 400 MB Max Memory Used: 40 MB
2021-10-26T00:04:20.754Z d628aa1e-dee6-4cc6-9ce0-f7c11cf73249 Task timed out after 15.02 seconds
</code></pre>
<p>And when it gooes all right it's like that :</p>
<pre><code>START RequestId: e5fb6b08-adf9-433f-b1da-fd9ec29dde31 Version: $LATEST
2021-10-25T14:35:03.369Z e5fb6b08-adf9-433f-b1da-fd9ec29dde31 INFO Connecting to database...
2021-10-25T14:35:03.590Z e5fb6b08-adf9-433f-b1da-fd9ec29dde31 INFO Executing (default): SELECT 1+1 AS result
2021-10-25T14:35:03.600Z e5fb6b08-adf9-433f-b1da-fd9ec29dde31 INFO Connection has been established successfully.
2021-10-25T14:35:03.600Z e5fb6b08-adf9-433f-b1da-fd9ec29dde31 INFO retrieving all open invoices...
2021-10-25T14:35:04.011Z e5fb6b08-adf9-433f-b1da-fd9ec29dde31 INFO 0 data retrieved.
2021-10-25T14:35:04.011Z e5fb6b08-adf9-433f-b1da-fd9ec29dde31 INFO Everything went smoothly !
END RequestId: e5fb6b08-adf9-433f-b1da-fd9ec29dde31
REPORT RequestId: e5fb6b08-adf9-433f-b1da-fd9ec29dde31 Duration: 646.58 ms Billed Duration: 647 ms Memory Size: 400 MB
</code></pre>
<p>I don't get why it hangs with no error or log...</p>
",5612271.0,,,,,2021-10-27 05:37:19,AWS Lambda hang until it timout on stipe invoices list,<amazon-web-services><aws-lambda><stripe-payments><serverless-framework><serverless>,2,0,,,,CC BY-SA 4.0,I am using AWS Lambda to host a nodeJs service that fetch my open invoices on Stripe and execute a payment and update my database  The problem is that most of the time  but not all the time (sometimes everything goes how it should)  it hang on the call of invoice list and do nothing  Heres the part of the code where log stops :  And here the log output :  And when it gooes all right its like that :  I dont get why it hangs with no error or log    
69726172,1,,,2021-10-26 15:45:22,,0,101,"<p>I was looking at our bill and apparently we are charged more than $600 for Amazon Simple Storage Service USE2-Requests-Tier2, meaning that we have more than 1 billion GET requests a month, so about 3 million every day? We made sure that none of our S3 buckets are public so attacks should not be possible. I have no idea how we are getting so many requests as we only have about 20 active users of our app everyday. Assuming that each of them were to make about 10 GET requests to our API, which uses lambda and boto3 to download 10 files from S3 bucket to the lambda's tmp folders, then returns a value, it still wouldn't make sense for us to have about 3 millions GET requests a day.</p>
<p>We also have another EventBridge triggered lambda, which uses Athena to query our database (S3), and will run every 2 hours. I don't know if this is a potential cause? Can anyone shed some light on this? And how we can take a better look into where and why are we getting so many GET requests? Thank you.</p>
",13730659.0,,,,,2021-10-28 08:48:25,Millions of GET requests (Amazon S3 USE2-Requests-Tier2) every day?,<amazon-s3><aws-lambda><amazon-athena><aws-event-bridge><amazon-api-gateway>,1,1,,,,CC BY-SA 4.0,I was looking at our bill and apparently we are charged more than $600 for Amazon Simple Storage Service USE2-Requests-Tier2  meaning that we have more than 1 billion GET requests a month  so about 3 million every day  We made sure that none of our S3 buckets are public so attacks should not be possible  I have no idea how we are getting so many requests as we only have about 20 active users of our app everyday  Assuming that each of them were to make about 10 GET requests to our API  which uses lambda and boto3 to download 10 files from S3 bucket to the lambdas tmp folders  then returns a value  it still wouldnt make sense for us to have about 3 millions GET requests a day  We also have another EventBridge triggered lambda  which uses Athena to query our database (S3)  and will run every 2 hours  I dont know if this is a potential cause  Can anyone shed some light on this  And how we can take a better look into where and why are we getting so many GET requests  Thank you  
69779139,1,,,2021-10-30 11:19:28,,0,114,"<p><strong>Quick Summary:
I can not make the Payment Sheet compatible with a Market Place Concept.</strong></p>
<p>I am implementing the flutter_payment Package to create an App where Vendors can offer Products which can be purchased by clients. I use Stripe Standard Accounts and a Direct Charge.</p>
<p>The Working Process is:</p>
<p>1.) User clics on Button and request Data from Firebase Cloud Functons.</p>
<p>2.) Firebase Cloud Functions:
2.1.) Create <strong>CustomerId</strong> (automatically on Platform Account)/ Get Customer Id from Firebase
2.2.) Get ephemeralKeys from <strong>CustomerId</strong></p>
<pre><code>   export const createEphemeralKey =
functions.https.onCall(async (data, context) =&gt; {
  // Checking that the user is authenticated.
  if (!context.auth) {
    // Throwing an HttpsError so that the client gets the error details.
    throw new functions.https.HttpsError(
        &quot;failed-precondition&quot;,
        &quot;The function must be called while authenticated!&quot;
    );
  }
  const uid = context.auth.uid;
  try {
    if (!uid) throw new Error(&quot;Not authenticated!&quot;);
    // Get stripe customer id
    const FireCustomer = await admin.firestore()
        .collection(&quot;stripe_customers&quot;).doc(uid).get();

    const customer = FireCustomer.data()!.customer_id;

    const key = await stripe.ephemeralKeys.create(
        {customer},
        {apiVersion: &quot;2020-08-27&quot;}
    );
    console.log(&quot;The Key is&quot;);
    console.log(key);
    return key;
  } catch (error) {
    console.log(error);
  }
}
);
</code></pre>
<p>2.3.) Create Payment Intent</p>
<pre><code>export const createPaymentIntent = functions.https.onCall(
async (data, context) =&gt; {
  const amount:number = data.amount;
  console.log(&quot;The amount is&quot;);
  console.log(amount);
  const customerId = data.stripeCustomerId;
  const vendorId = data.vendorId;
  const fee:number = (amount* 8.1/100)| 0;
  console.log(&quot;here comes the fee&quot;);
  console.log(fee);
  console.log(vendorId);
  try {
    const paymentIntent = await stripe.paymentIntents.create({
      amount: amount,
      currency: &quot;eur&quot;,
      customer: customerId,
     // application_fee_amount: fee,
     // stripeAccount: vendorId,
    },
    );
    console.log(&quot;HEre comes the paymentIntent&quot;);
    console.log(paymentIntent);
    return paymentIntent;
  } catch (e) {
    return console.log(e);
  }
});
</code></pre>
<p>3.) Then I pass the Data back to the App and pass them into the Payment Sheet</p>
<pre><code> await Stripe.instance.initPaymentSheet(
    paymentSheetParameters: SetupPaymentSheetParameters(
        // Main params
        applePay: true,
        googlePay: true,
        style: ThemeMode.dark,
        testEnv: true,
        merchantCountryCode: 'DE',
        merchantDisplayName: 'Flutter Stripe Store Demo',
        customerId: stripeCustomerId,
        paymentIntentClientSecret: piSecret,
        customerEphemeralKeySecret: ephemeralKey),
        
  );
  await Stripe.instance.presentPaymentSheet();
</code></pre>
<p>Problem:
The Client is created on the Platform Account and the payment data is stored there. If I try to create the Payment Intent with the &quot;stripeAccount: vendorId&quot; property ,the Customer Id is not being recognised as it's not being stored on the Connect account.</p>
<p>Now what the Stripe Documentation says is to clone the Payment Method:</p>
<p><a href=""https://stripe.com/docs/payments/payment-methods/connect#cloning-payment-methods"" rel=""nofollow noreferrer"">https://stripe.com/docs/payments/payment-methods/connect#cloning-payment-methods</a></p>
<p>I can not clone the Payment Method because it is being created by flutter_stripe</p>
<p>So I have no idea how to proceed as the flutter_stripe package is not well documented and the package is not compatible to the Stripe Documentation.
Is a Payment Sheet a possible solution here at all?</p>
<p>Thank you very much,</p>
<p>Tim</p>
",16114837.0,,,,,2021-10-30 11:19:28,Flutter_Stripe Package - Create Market Place with Payment Sheet (+Firebase Cloud Functions),<google-cloud-functions><stripe-payments><serverless><marketplace>,0,0,,,,CC BY-SA 4.0,Quick Summary: I can not make the Payment Sheet compatible with a Market Place Concept  I am implementing the flutter_payment Package to create an App where Vendors can offer Products which can be purchased by clients  I use Stripe Standard Accounts and a Direct Charge  The Working Process is: 1 ) User clics on Button and request Data from Firebase Cloud Functons  2 ) Firebase Cloud Functions: 2 1 ) Create CustomerId (automatically on Platform Account)/ Get Customer Id from Firebase 2 2 ) Get ephemeralKeys from CustomerId  2 3 ) Create Payment Intent  3 ) Then I pass the Data back to the App and pass them into the Payment Sheet  Problem: The Client is created on the Platform Account and the payment data is stored there  If I try to create the Payment Intent with the stripeAccount: vendorId property  the Customer Id is not being recognised as its not being stored on the Connect account  Now what the Stripe Documentation says is to clone the Payment Method:  I can not clone the Payment Method because it is being created by flutter_stripe So I have no idea how to proceed as the flutter_stripe package is not well documented and the package is not compatible to the Stripe Documentation  Is a Payment Sheet a possible solution here at all  Thank you very much  Tim 
69843744,1,,,2021-11-04 17:55:22,,-2,49,"<p>I have checked most of the GCP services and Cloud Functions &amp; Run are awesome, but they have limited runtime, also the App engine is designed for website hosting, not for temporary backend tasks.</p>
<p>so is there any service that supports my case, with serverless behavior where just pay for usage?</p>
",5921034.0,,,,,2021-11-11 16:32:56,Which is the best service to run scheduled docker container which take several days?,<google-cloud-platform><cloud><serverless>,1,3,,,,CC BY-SA 4.0,I have checked most of the GCP services and Cloud Functions &amp; Run are awesome  but they have limited runtime  also the App engine is designed for website hosting  not for temporary backend tasks  so is there any service that supports my case  with serverless behavior where just pay for usage  
69848043,1,,,2021-11-05 02:25:54,,0,138,"<p>We have s3 event notifications defined on a bucket without any prefix filtering and all events are sent to an SNS topic.</p>
<p>We would like to have an SNS filter policy to invoke a lambda for a specific s3 prefix, say <code>error</code>. We could not modify the existing s3 event notification to create different event notifications based on prefix. Among other reasons there are limitations on the number of event notifications that can be configured on a bucket, capped at 100, and we have 100's of dirs in the bucket.</p>
<p>Example: SNS message delivery of an S3 event notification.</p>
<pre><code>{
  &quot;Records&quot;: [
  {
    &quot;EventSource&quot;: &quot;aws:sns&quot;,
    &quot;EventVersion&quot;: &quot;1.0&quot;,
    &quot;EventSubscriptionArn&quot;: &quot;arn:aws:sns:us-east-1:12345:test_sns_topic:7654321&quot;
    &quot;Sns&quot;: {
      &quot;Type&quot; : &quot;Notification&quot;,
      &quot;MessageId&quot; : &quot;ff6531c7-6df8-502b-94e3-6839e30215a2&quot;,
      &quot;TopicArn&quot; : &quot;arn:aws:sns:us-east-1:12345:test_sns_topic&quot;,
      &quot;Subject&quot; : &quot;Amazon S3 Notification&quot;,
      &quot;Message&quot; : &quot;{\&quot;Records\&quot;:[{\&quot;eventVersion\&quot;:\&quot;2.1\&quot;,\&quot;eventSource\&quot;:\&quot;aws:s3\&quot;,\&quot;awsRegion\&quot;:\&quot;us-east-1\&quot;,\&quot;eventTime\&quot;:\&quot;2021-11-05T01:45:29.400Z\&quot;,\&quot;eventName\&quot;:\&quot;ObjectCreated:Copy\&quot;,\&quot;userIdentity\&quot;:{\&quot;principalId\&quot;:\&quot;AWS:AROA5PFLZNQS45URZIYA5:karthik.raj.chime.com\&quot;},\&quot;requestParameters\&quot;:{\&quot;sourceIPAddress\&quot;:\&quot;208.127.85.61\&quot;},\&quot;responseElements\&quot;:{\&quot;x-amz-request-id\&quot;:\&quot;V5HK4WQ4J919DGJX\&quot;,\&quot;x-amz-id-2\&quot;:\&quot;mFHZyrASNQsU+/Uk7SOYX0tBz6qxRIbvXCVhq05WiEdGbElReSmsFa8+yXmqUBA7eI5J5axwynbmRCZXU46RHZ+qigalGx74Vk608PSUjZQ=\&quot;},\&quot;s3\&quot;:{\&quot;s3SchemaVersion\&quot;:\&quot;1.0\&quot;,\&quot;configurationId\&quot;:\&quot;s3_test_event_notification\&quot;,\&quot;bucket\&quot;:{\&quot;name\&quot;:\&quot;test-bukit\&quot;,\&quot;ownerIdentity\&quot;:{\&quot;principalId\&quot;:\&quot;A1QESTMWKH1F5F\&quot;},\&quot;arn\&quot;:\&quot;arn:aws:s3:::test-bukit\&quot;},\&quot;object\&quot;:{\&quot;key\&quot;:\&quot;error/test_data.json\&quot;,\&quot;size\&quot;:246,\&quot;eTag\&quot;:\&quot;3c78caecf5775a553557384a0c6c7678\&quot;,\&quot;sequencer\&quot;:\&quot;0061848CB952FB369A\&quot;}}}]}&quot;,
      &quot;Timestamp&quot; : &quot;2021-11-05T01:45:30.805Z&quot;,
      &quot;SignatureVersion&quot; : &quot;1&quot;,
      &quot;Signature&quot; : &quot;hnOhLkYa+xge/hRIi8ODX/dG8tVDKdoKNUVsTDH46hKTVGERO87KFWDRvBjuuinlVCFaVpONbIdhznLHhcYiOVYH9a3K7xsXBuHrxF/et5S0IGePavm21YVVVXbIJMucN1GkRwDwUXIQWmH0tJmjBB+06fep9Db/4O1WerLBlDnxnAgJ0FZIhVfl/hNx9DDc9vXVWHwPxVpPhdiNW8EZfiVXwJMNjXpJZJbPM9sZQrS8es9qCn4Db+PI9qqDqAkH2cqVmXAOI1rptlHar7WoIhNuvHWG75MJRYBNoalrixjF1FpQF2lANQTEdHUxUNxDgjBwZFwRChCqnMOY+c8g7A==&quot;,
      &quot;SigningCertURL&quot; : &quot;https://sns.us-east-1.amazonaws.com/SimpleNotificationService-7ff5318490ec183fbaddaa2a969abfda.pem&quot;,
      &quot;UnsubscribeURL&quot; : &quot;https://sns.us-east-1.amazonaws.com/?Action=Unsubscribe&amp;SubscriptionArn=arn:aws:sns:us-east-1:925925731365:snowpipe_sns_test:aff81da7-aa31-46d5-ab60-02c119e2ba44&quot;
      }
    }
  ]
}
</code></pre>
<p>SNS filter matching doesn't seem to have support to filter strings by regex patterns or <code>contains</code>
Options available are</p>
<ul>
<li>Exact matching</li>
<li>Prefix matching</li>
<li>Anything-but matching</li>
<li>IP address matching</li>
</ul>
<p>Ref: <a href=""https://docs.aws.amazon.com/sns/latest/dg/string-value-matching.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sns/latest/dg/string-value-matching.html</a></p>
<p>Is there a way to achieve this filtering in SNS. The other option is to have a lambda be invoked for all events and filter events in the lambda, but the volume of events in the specific s3 dir is significantly lower than the total events and it will not be cost efficient to have the lambda be invoked for all events.</p>
",2312951.0,,,,,2021-11-05 02:25:54,AWS SNS wildcard filtering,<amazon-web-services><amazon-s3><aws-lambda><amazon-sns>,0,0,,,,CC BY-SA 4.0,We have s3 event notifications defined on a bucket without any prefix filtering and all events are sent to an SNS topic  We would like to have an SNS filter policy to invoke a lambda for a specific s3 prefix  say   We could not modify the existing s3 event notification to create different event notifications based on prefix  Among other reasons there are limitations on the number of event notifications that can be configured on a bucket  capped at 100  and we have 100s of dirs in the bucket  Example: SNS message delivery of an S3 event notification   SNS filter matching doesnt seem to have support to filter strings by regex patterns or  Options available are  Exact matching Prefix matching Anything-but matching IP address matching  Ref:  Is there a way to achieve this filtering in SNS  The other option is to have a lambda be invoked for all events and filter events in the lambda  but the volume of events in the specific s3 dir is significantly lower than the total events and it will not be cost efficient to have the lambda be invoked for all events  
69886381,1,69887023.0,,2021-11-08 15:56:37,,0,35,"<p>I want to know if there are charges for uploading/dropping files in an S3 bucket created in EU region from a lambda function running in the US region.</p>
<p>Also how much of a performance hit it is, to have lambda function and the S3 bucket to be present in these two different regions.</p>
<p>Please help!!</p>
",13180908.0,,13180908.0,,2021-11-08 16:10:15,2021-11-08 16:46:33,S3 Bucket pricing and performance,<amazon-web-services><amazon-s3><aws-lambda>,1,0,,,,CC BY-SA 4.0,I want to know if there are charges for uploading/dropping files in an S3 bucket created in EU region from a lambda function running in the US region  Also how much of a performance hit it is  to have lambda function and the S3 bucket to be present in these two different regions  Please help   
69902809,1,,,2021-11-09 17:49:29,,0,34,"<p>My requirement is to log the full request/response body.
I have a microservice architecture set up with API gateway and lambda functions as a backend.
Now I want a logging solution independent from my backend Lambda functions.
The best thing to consider was API Gateway execution logs, but it is truncating the request/response bodies over 1024 bytes (AWS limitation).
What can be used as a solution for this with a minimum increase in cost and latency ?</p>
",8152666.0,,,,,2021-11-09 17:49:29,How to setup Full Request/Response logging solution for AWS API Gateway,<amazon-web-services><aws-lambda><microservices><aws-api-gateway><amazon-cloudwatch>,0,0,,,,CC BY-SA 4.0,My requirement is to log the full request/response body  I have a microservice architecture set up with API gateway and lambda functions as a backend  Now I want a logging solution independent from my backend Lambda functions  The best thing to consider was API Gateway execution logs  but it is truncating the request/response bodies over 1024 bytes (AWS limitation)  What can be used as a solution for this with a minimum increase in cost and latency   
69923628,1,,,2021-11-11 05:35:02,,2,194,"<p>I have about 60 lambda functions in my AWS account.</p>
<p>Is it possible to list AWS lambda functions by their costs (invocations * duration during a period of time)?</p>
<p>Because I'm going to change their architecture from <code>x86_64</code> to <code>arm64</code> to save money. And I want to start doing this from the most expensive lambda funciton.</p>
<p><a href=""https://awscli.amazonaws.com/v2/documentation/api/latest/reference/lambda/list-functions.html"" rel=""nofollow noreferrer""><code>aws lambda list-functions</code></a> doesn't seem to provide this feature.</p>
",1408347.0,,,,,2021-11-15 06:35:19,How to find the lambda function that costs the most money?,<amazon-web-services><aws-lambda>,2,0,,,,CC BY-SA 4.0,I have about 60 lambda functions in my AWS account  Is it possible to list AWS lambda functions by their costs (invocations * duration during a period of time)  Because Im going to change their architecture from  to  to save money  And I want to start doing this from the most expensive lambda funciton   doesnt seem to provide this feature  
51475033,1,,,2018-07-23 08:59:08,,2,593,"<p>So I have been working on a couple Lambda functions for a Alexa skills including DynamoDB (actually 5 tables each with 5 RCU/WCU. I am on the free tier and I haven't used the work-in-progress functions for a couple of days, but today I realized in the cost overview that I have used about 9000 hours and will possibly use about 12000 at the end of the month. See here:</p>

<p><a href=""https://i.stack.imgur.com/Dzp8e.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Dzp8e.png"" alt=""enter image description here""></a></p>

<p>So was wondering why?
I know that you reserve the hourly capacity. And since none of the skills is live, I sticked with 5 RCU/WCU for each table (it says 25 RCU/WCU is okay for the free tier). But these numbers doesn't make sense? </p>

<p>Or do they? 
And the question is what happens when I scale this down to 1 RCU/WCU when all the skills go live? </p>

<p>I know that this would mean that only one request per second can be processed, but I am sure that this would be enough for the skills (at least for the first couple of month) - I guess that I won't have more than 200-300 monthly users. Will this significantly reduce the above numbers? Or will I be charged huge bill for some reason I don't understand after going though the documentation?</p>
",6852849.0,,,,,2018-07-23 13:15:18,DynamoDB: How is read/write capacity calculated and what does it mean (for the free tier) if it is scaled quickly?,<amazon-web-services><aws-lambda><amazon-dynamodb><aws-billing>,1,0,1.0,,,CC BY-SA 4.0,So I have been working on a couple Lambda functions for a Alexa skills including DynamoDB (actually 5 tables each with 5 RCU/WCU  I am on the free tier and I havent used the work-in-progress functions for a couple of days  but today I realized in the cost overview that I have used about 9000 hours and will possibly use about 12000 at the end of the month  See here:  So was wondering why  I know that you reserve the hourly capacity  And since none of the skills is live  I sticked with 5 RCU/WCU for each table (it says 25 RCU/WCU is okay for the free tier)  But these numbers doesnt make sense   Or do they   And the question is what happens when I scale this down to 1 RCU/WCU when all the skills go live   I know that this would mean that only one request per second can be processed  but I am sure that this would be enough for the skills (at least for the first couple of month) - I guess that I wont have more than 200-300 monthly users  Will this significantly reduce the above numbers  Or will I be charged huge bill for some reason I dont understand after going though the documentation  
69943722,1,,,2021-11-12 13:34:12,,0,31,"<p>I'm using AWS API Gateway and passing one large CSV file to backend Lambda. Due to limitation of AWS API gateway response time that is 30 seconds, it's not processing.</p>
<p>I explored web socket option but it's an expensive option because connections need to open all the time.</p>
<p>Can someone please advice me best solution to solve this issue?
Thanks</p>
",10583553.0,,,,,2021-11-12 13:34:12,API Gateway Response time issue,<websocket><aws-lambda><aws-api-gateway>,0,3,,,,CC BY-SA 4.0,Im using AWS API Gateway and passing one large CSV file to backend Lambda  Due to limitation of AWS API gateway response time that is 30 seconds  its not processing  I explored web socket option but its an expensive option because connections need to open all the time  Can someone please advice me best solution to solve this issue  Thanks 
36157907,1,,,2016-03-22 15:00:38,,31,31164,"<p>At the moment we are running our application on an AWS Beanstalk but are trying to determine the suitablilty of Azure.</p>

<p>Our biggest issue is the amount of wasted CPU time we are paying for but not using. We are running on t2.small instances as these have the min amount of RAM we need but we never use even the base amount of CPU time allotted. (20% for a t2.small ) We need lots of CPU power during short bursts of the day and bringing more instances on line in advance of this is the only way we can handle it.
AWS Lambda looks a good solution for us but we have dependencies on Windows components like SAPI so we have to run inside of Windows VMs.</p>

<p>Looking at Azure cloud services we thought using a Web role would be best fit for our app but it seems a Web role is nothing more than a Win 2012 VM with IIS enabled. So as the app scales it just brings on more of these VMs which is exactly what we have at the moment. Does Azure have a service similar to Lambda where you just pay for the CPU processing time you use? 
The reason for our inefficient use of CPU resources is that our speech generation app uses lost of 3rd party voices but can only run single threaded when calling into SAPI because the voice engine is prone to crashing when multithreading.  We have no control over this voice engine. It must have access to a system registry and Windows SAPI so the ideal solution is to somehow wrap all dependencies is a package and deploy this onto Azure and then kick off multiple instances of this.  What ""this"" is I have no Idea</p>
",965895.0,,4825211.0,,2016-04-01 02:43:49,2016-04-01 02:43:49,What is the Azure equivalent of AWS Lambda?,<azure><aws-lambda><azure-functions>,4,0,2.0,,,CC BY-SA 3.0,At the moment we are running our application on an AWS Beanstalk but are trying to determine the suitablilty of Azure  Our biggest issue is the amount of wasted CPU time we are paying for but not using  We are running on t2 small instances as these have the min amount of RAM we need but we never use even the base amount of CPU time allotted  (20% for a t2 small ) We need lots of CPU power during short bursts of the day and bringing more instances on line in advance of this is the only way we can handle it  AWS Lambda looks a good solution for us but we have dependencies on Windows components like SAPI so we have to run inside of Windows VMs  Looking at Azure cloud services we thought using a Web role would be best fit for our app but it seems a Web role is nothing more than a Win 2012 VM with IIS enabled  So as the app scales it just brings on more of these VMs which is exactly what we have at the moment  Does Azure have a service similar to Lambda where you just pay for the CPU processing time you use   The reason for our inefficient use of CPU resources is that our speech generation app uses lost of 3rd party voices but can only run single threaded when calling into SAPI because the voice engine is prone to crashing when multithreading   We have no control over this voice engine  It must have access to a system registry and Windows SAPI so the ideal solution is to somehow wrap all dependencies is a package and deploy this onto Azure and then kick off multiple instances of this   What this is I have no Idea 
54579390,1,54595559.0,,2019-02-07 17:52:41,,0,85,"<p>The challenge is to run a set of data processing and data science scripts that consume more memory than expected.</p>

<p><strong>Here are my requirements:</strong></p>

<ul>
<li>Running 10-15 Python 3.5 scripts via Cron Scheduler</li>
<li>These different 10-15 scripts each take somewhere between 10 seconds to 20 minutes to complete</li>
<li>They run on different hours of the day, some of them run every 10 minute while some run once a day</li>
<li>Each script logs what it has done so that I can take a look at it later if something goes wrong</li>
<li>Some of the scripts sends e-mails to me and to my team mates</li>
<li>None of the scripts have an HTTP/web server component; they all run on Cron schedules and not user-facing</li>
<li>All the scripts' code is fed from my Github repository; when scripts wake up, they first do a git pull origin master and then start executing. That means, pushing to master causes all scripts to be on the latest version.</li>
</ul>

<p><strong>Here is what I currently have:</strong></p>

<ul>
<li>Currently I am using 3 Digital Ocean servers (droplets) for these scripts</li>
<li>Some of the scripts require a huge amount of memory (I get segmentation fault in droplets with less than 4GB of memory)</li>
<li>I am willing to introduce a new script that might require even larger memory (the new script currently faults in a 4GB droplet)</li>
<li>The setup of the droplets are relatively easy (thanks to Python venv) but not to the point of executing a single command to spin off a new droplet and set it up</li>
</ul>

<p>Having a full dedicated 8GB / 16B droplet for my new script sounds a bit inefficient and expensive. </p>

<p>What would be a more efficient way to handle this?</p>
",870011.0,,890242.0,,2019-02-08 16:31:47,2019-02-08 18:26:45,Can a serverless architecture support high memory needs?,<python><cron><data-science><digital-ocean><serverless>,1,3,1.0,,,CC BY-SA 4.0,The challenge is to run a set of data processing and data science scripts that consume more memory than expected  Here are my requirements:  Running 10-15 Python 3 5 scripts via Cron Scheduler These different 10-15 scripts each take somewhere between 10 seconds to 20 minutes to complete They run on different hours of the day  some of them run every 10 minute while some run once a day Each script logs what it has done so that I can take a look at it later if something goes wrong Some of the scripts sends e-mails to me and to my team mates None of the scripts have an HTTP/web server component; they all run on Cron schedules and not user-facing All the scripts code is fed from my Github repository; when scripts wake up  they first do a git pull origin master and then start executing  That means  pushing to master causes all scripts to be on the latest version   Here is what I currently have:  Currently I am using 3 Digital Ocean servers (droplets) for these scripts Some of the scripts require a huge amount of memory (I get segmentation fault in droplets with less than 4GB of memory) I am willing to introduce a new script that might require even larger memory (the new script currently faults in a 4GB droplet) The setup of the droplets are relatively easy (thanks to Python venv) but not to the point of executing a single command to spin off a new droplet and set it up  Having a full dedicated 8GB / 16B droplet for my new script sounds a bit inefficient and expensive   What would be a more efficient way to handle this  
58876798,1,,,2019-11-15 12:10:31,,1,523,"<p>I'm doing a POC to find out how fast DynamoDB is. The example problem I'm trying to solve is the following: </p>

<p>I have a 100 keys on the input. For each key I have to look up associated info. The associated info for each key has ~8kB. </p>

<p>I've stored the keys with their associated info to DynamoDB, table <code>test_table</code>, set the capacity to on-demand mode. I'm getting the info from an AWS Lambda function. The table and the Lambda are in the same region (EU Frankfurt). Here is the code sample for getting info for the given key list:</p>

<pre><code>key_list = [
    'key0',
    # ...
    'key99'
]
dynamo_client = boto3.client('dynamodb')
response = dynamo_client.batch_get_item(
    RequestItems={
        'test_table': {
            'Keys': [
                {
                    'key': {'S': k}
                } for k in key_list
            ],
            'ConsistentRead': False
        }
    }
)
</code></pre>

<p>I always run the Lambda a few times to make sure it's warmed up. When I did this test for the first time, it took ~2000ms. My expectations are to get this in ~500ms. Things I did to speed it up:</p>

<ul>
<li>Increased Lambda memory from 1GB to max (~3GB). I got to ~1300ms. On Lambda, with more memory, you get more bandwidth, so I guess this is why it got faster.</li>
<li>I've provisioned big read capacity in DynamoDB (1000), to see if auto-scaling isn't slowing things down. I saw no measurable effect.</li>
<li>I checked CloudWatch metrics for the DynamoDB table, the latency there is ~180ms. This is pretty good, so it looks like it's not a DynamoDB issue.</li>
<li>I installed aws x-ray sdk to check if there's something slowing things down on the lambda side. The <code>batch_get_item</code> call is taking ~1250ms out of ~1300ms, the rest is lambda initialization. So My guess was that the slowdown must be happening somewhere between the Lambda and DynamoDB</li>
<li>I've put the Lambda into a VPC and used a Service Endpoint for DynamoDB - it had no measurable effect.</li>
<li>I've put the code to a EC2 instance with as much network bandwidth as I could get (100 Gigabit) and it took ~900ms. Still not good.</li>
</ul>

<p>So my guess is that the bottleneck is somewhere between the Lambda and the DynamoDB table.</p>

<p>I haven't tried DAX, because it involves some extra costs for the cluster, plus I'm not sure it would help, as DAX promises to speed things up on the DynamoDB side, which doesn't seem to be the bottleneck.</p>

<p>My question is: Is it somewhat normal that getting ~8MB from a DynamoDB to a Lambda takes >1000ms? Or am I doing something wrong?</p>
",319397.0,,319397.0,,2019-11-16 07:20:04,2019-11-16 07:20:04,Calling DynamoDB BatchGetItem from Lambda has >1000ms latency. Is it normal?,<python><aws-lambda><amazon-dynamodb><latency>,0,2,,,,CC BY-SA 4.0,Im doing a POC to find out how fast DynamoDB is  The example problem Im trying to solve is the following:  I have a 100 keys on the input  For each key I have to look up associated info  The associated info for each key has ~8kB   Ive stored the keys with their associated info to DynamoDB  table   set the capacity to on-demand mode  Im getting the info from an AWS Lambda function  The table and the Lambda are in the same region (EU Frankfurt)  Here is the code sample for getting info for the given key list:  I always run the Lambda a few times to make sure its warmed up  When I did this test for the first time  it took ~2000ms  My expectations are to get this in ~500ms  Things I did to speed it up:  Increased Lambda memory from 1GB to max (~3GB)  I got to ~1300ms  On Lambda  with more memory  you get more bandwidth  so I guess this is why it got faster  Ive provisioned big read capacity in DynamoDB (1000)  to see if auto-scaling isnt slowing things down  I saw no measurable effect  I checked CloudWatch metrics for the DynamoDB table  the latency there is ~180ms  This is pretty good  so it looks like its not a DynamoDB issue  I installed aws x-ray sdk to check if theres something slowing things down on the lambda side  The  call is taking ~1250ms out of ~1300ms  the rest is lambda initialization  So My guess was that the slowdown must be happening somewhere between the Lambda and DynamoDB Ive put the Lambda into a VPC and used a Service Endpoint for DynamoDB - it had no measurable effect  Ive put the code to a EC2 instance with as much network bandwidth as I could get (100 Gigabit) and it took ~900ms  Still not good   So my guess is that the bottleneck is somewhere between the Lambda and the DynamoDB table  I havent tried DAX  because it involves some extra costs for the cluster  plus Im not sure it would help  as DAX promises to speed things up on the DynamoDB side  which doesnt seem to be the bottleneck  My question is: Is it somewhat normal that getting ~8MB from a DynamoDB to a Lambda takes &gt;1000ms  Or am I doing something wrong  
59698857,1,59700633.0,,2020-01-11 21:37:54,,1,230,"<p>We have been running multi-tier application on aws and using various aws services like ECS, Lambda and RDS. Looking for a solution to map billing items to actual system components, finding the most money spending component etc.</p>

<p>AWS improved its Detailed Cost Usage Reports and have Cost Explorer API however it only break down the billing to services or instances. However per instance breakdown does not bring so much value if you looking for what is the cost of each component. Any solutions/recommendations for this?    </p>
",12696478.0,,,,,2020-01-13 17:20:32,aws billing breakdown to system components and artifacts,<amazon-web-services><aws-lambda><devops><amazon-ecs><aws-billing>,1,0,,,,CC BY-SA 4.0,We have been running multi-tier application on aws and using various aws services like ECS  Lambda and RDS  Looking for a solution to map billing items to actual system components  finding the most money spending component etc  AWS improved its Detailed Cost Usage Reports and have Cost Explorer API however it only break down the billing to services or instances  However per instance breakdown does not bring so much value if you looking for what is the cost of each component  Any solutions/recommendations for this      
54708106,1,,,2019-02-15 11:14:19,,0,90,"<p>I'm currently working on an indexing component where it creates the indexes in different database tables (DynamoDB, ElasticSearch etc) based on client configurations.</p>

<p>There could be potentially 100+ clients who need different indexing records and system's TPS is 700. Currently I have 2 approaches,</p>

<ol>
<li>Have a single lambda listening to SQS (where events are subscribed) and pulls the data from the source and build index records for each clients and put the indexes in DB in parallel. </li>
<li>Or, In above approach, execution time will be longer - as parallel computation is depending on total number of cores available. So, have a small lambda that duplicates the incoming events with small number of clients (let's say for 1 incoming message, this lambda will create 10 new messages with 10 client's name in it) and push it to another SQS. And another lambda subscribed to it will listen to these messages and build index records for each clients and put the indexes in DB in parallel.</li>
</ol>

<p>So, both approaches have few shortcomings,</p>

<ol>
<li>Execution time is higher but less lambda invocations. Higher memory &amp; higher CPU.</li>
<li>Less execution time but more lambda invocations. Less memory and moderately high CPU.</li>
</ol>

<p>So, I want to have a cost effective solutions where I can achieve indexing component's maximum fan-out for building indexes for multiple clients. Also, what's the maximum parallelization I can achieve per AWS lambda instance?</p>
",1546508.0,,174777.0,,2019-02-16 05:57:17,2019-02-16 05:57:17,Single AWS lambda with bigger responsibility vs multiple lambda's with less responsibility,<amazon-web-services><parallel-processing><aws-lambda>,0,2,,,,CC BY-SA 4.0,Im currently working on an indexing component where it creates the indexes in different database tables (DynamoDB  ElasticSearch etc) based on client configurations  There could be potentially 100+ clients who need different indexing records and systems TPS is 700  Currently I have 2 approaches   Have a single lambda listening to SQS (where events are subscribed) and pulls the data from the source and build index records for each clients and put the indexes in DB in parallel   Or  In above approach  execution time will be longer - as parallel computation is depending on total number of cores available  So  have a small lambda that duplicates the incoming events with small number of clients (lets say for 1 incoming message  this lambda will create 10 new messages with 10 clients name in it) and push it to another SQS  And another lambda subscribed to it will listen to these messages and build index records for each clients and put the indexes in DB in parallel   So  both approaches have few shortcomings   Execution time is higher but less lambda invocations  Higher memory &amp; higher CPU  Less execution time but more lambda invocations  Less memory and moderately high CPU   So  I want to have a cost effective solutions where I can achieve indexing components maximum fan-out for building indexes for multiple clients  Also  whats the maximum parallelization I can achieve per AWS lambda instance  
54709373,1,54744520.0,,2019-02-15 12:25:50,,0,279,"<p>I came across the article : <a href=""https://cloudplatform.googleblog.com/2018/07/bringing-the-best-of-serverless-to-you.html"" rel=""nofollow noreferrer"">Bringing the best of serverless to you</a>
where I came to know about upcoming product called <strong>Serverless containers on Cloud Functions</strong> which is currently in Alpha.</p>

<p>As described in the article:</p>

<blockquote>
  <p>Today, were also introducing serverless containers, which allow you
  to run container-based workloads in a fully managed environment and
  still only pay for what you use.</p>
</blockquote>

<p>and in <a href=""https://cloud.google.com/serverless/"" rel=""nofollow noreferrer"">GCP solutions page</a></p>

<blockquote>
  <p>Serverless containers on Cloud Functions enables you to run your own containerized workloads on
  GCP with all the benefits of serverless. And you will still pay only
  for what you use. If you are interested in learning more about
  serverless containers, please sign up for the alpha.</p>
</blockquote>

<p>So my question is <strong>how this serverless containers different from app engine flexible with custom runtime, which also use a docker file?</strong></p>

<p>And it's my suspicion, since mentioned named is <strong>Serverless containers on Cloud Functions</strong>, the differentiation may include role of cloud functions. If so what is the role played by cloud functions in the serverless containers?</p>

<p>Please clarify.</p>
",7356355.0,,7356355.0,,2019-02-18 12:04:57,2019-03-08 17:49:32,What is the difference between Serverless containers and App Engine flexible with custom runtimes?,<containers><serverless><app-engine-flexible>,2,0,,,,CC BY-SA 4.0,I came across the article :  where I came to know about upcoming product called Serverless containers on Cloud Functions which is currently in Alpha  As described in the article:  Today  were also introducing serverless containers  which allow you   to run container-based workloads in a fully managed environment and   still only pay for what you use   and in   Serverless containers on Cloud Functions enables you to run your own containerized workloads on   GCP with all the benefits of serverless  And you will still pay only   for what you use  If you are interested in learning more about   serverless containers  please sign up for the alpha   So my question is how this serverless containers different from app engine flexible with custom runtime  which also use a docker file  And its my suspicion  since mentioned named is Serverless containers on Cloud Functions  the differentiation may include role of cloud functions  If so what is the role played by cloud functions in the serverless containers  Please clarify  
54805578,1,54806240.0,,2019-02-21 11:04:17,,1,291,"<p>I have an application which needs to read data from AWS dynamodb table every 5 seconds.
Currently I fetch the data using lambda, and then getting the data from dynamodb back to the user. </p>

<p>The problem with querying the table every 5 seconds is that it can have performance affect and moreover there is a pricing issue. (Most of the time the data might not even be changed at all but when it is changed I want to be notified it immediately).</p>

<p>An important clarification is that my app sits outsite of AWS, and only access the AWS dynamodb to get data (using simple http request built with c#).</p>

<p>Is there any way I can get a notification to my app when a new data is inserted into dynamodb? </p>
",1779053.0,,,,,2019-02-21 11:53:57,AWS Real time data fetching,<amazon-web-services><aws-lambda><amazon-dynamodb>,3,0,,,,CC BY-SA 4.0,I have an application which needs to read data from AWS dynamodb table every 5 seconds  Currently I fetch the data using lambda  and then getting the data from dynamodb back to the user   The problem with querying the table every 5 seconds is that it can have performance affect and moreover there is a pricing issue  (Most of the time the data might not even be changed at all but when it is changed I want to be notified it immediately)  An important clarification is that my app sits outsite of AWS  and only access the AWS dynamodb to get data (using simple http request built with c#)  Is there any way I can get a notification to my app when a new data is inserted into dynamodb   
36627575,1,36627799.0,,2016-04-14 15:27:34,,0,942,"<p>AWS Lambda functions are supposed to respond quickly to events. I would like to create a function that fires off a quick request to a slow API, and then terminates without waiting for a response. Later, when a response comes back, I would like a different Lambda function to handle the response. I know this sounds kind of crazy, when you think about what AWS would have to do to hang on to an open connection from one Lambda function and then send the response to another, but this seems to be very much in the spirit of how Lambda was designed to be used.</p>

<p>Ideas:</p>

<ul>
<li>Send messages to an SQS queue that represent a request to be made. Have some kind of message/HTTP proxy type service on an EC2 / EB cluster listen to the queue and actually make the HTTP requests. It would put response objects on another queue, tagged to identify the associated request, if necessary. This feels like a lot of complexity for something that would be trivial for a traditional service.</li>
<li>Just live with it. Lambda functions are allowed to run for 60 seconds, and these API calls that I make don't generally take longer than 10 seconds. Not sure how costly it would to have LFs spend 95% of their running time waiting on a response, but ""waiting"" isn't what LFs are for.</li>
<li>Don't use Lambda for anything that interacts with 3rd party APIs that aren't lightning fast :( That is what most of my projects do these days, though.</li>
</ul>
",410023.0,,410023.0,,2016-04-14 15:36:37,2016-04-14 15:37:15,"Is it possible to make an HTTP request from one Lambda function, and handle the response in another?",<amazon-web-services><aws-lambda>,1,8,,,,CC BY-SA 3.0,AWS Lambda functions are supposed to respond quickly to events  I would like to create a function that fires off a quick request to a slow API  and then terminates without waiting for a response  Later  when a response comes back  I would like a different Lambda function to handle the response  I know this sounds kind of crazy  when you think about what AWS would have to do to hang on to an open connection from one Lambda function and then send the response to another  but this seems to be very much in the spirit of how Lambda was designed to be used  Ideas:  Send messages to an SQS queue that represent a request to be made  Have some kind of message/HTTP proxy type service on an EC2 / EB cluster listen to the queue and actually make the HTTP requests  It would put response objects on another queue  tagged to identify the associated request  if necessary  This feels like a lot of complexity for something that would be trivial for a traditional service  Just live with it  Lambda functions are allowed to run for 60 seconds  and these API calls that I make dont generally take longer than 10 seconds  Not sure how costly it would to have LFs spend 95% of their running time waiting on a response  but waiting isnt what LFs are for  Dont use Lambda for anything that interacts with 3rd party APIs that arent lightning fast :( That is what most of my projects do these days  though   
59135114,1,59145546.0,,2019-12-02 08:33:54,,2,629,"<p>I have a WebSocket API hosted on AWS using the serverless framework. </p>

<p>The problem is that I am seeing 150k logs every minute, and it is costing a bunch.</p>

<p>The following log groups are the problem:</p>

<ul>
<li>/aws/apigateway/mzl9lpgzn0/production</li>
<li>API-Gateway-Execution-Logs_xkkvpjzgqj/production</li>
</ul>

<p>I have followed their docs to disable logs like so, but it is showing an error.</p>

<pre><code>service: BotBindSocketAPI

custom:
  customDomain:
    - domainName: ${ssm:/botapi/${opt:stage}/wsName}
      stage: ${opt:stage}
      websocket: true

provider:
  name: aws
  runtime: nodejs10.x
  stage: ${opt:stage}
  region: us-east-1
  logRetentionInDays: 7
  logs:
    restApi:
      accessLogging: false
      executionLogging: false
      level: ERROR
      fullExecutionData: true
    websocket: false
    frameworkLambda: false
  iamRoleStatements:
    - Effect: Allow
      Action:
        - ""execute-api:ManageConnections""
      Resource:
        - ""arn:aws:execute-api:*:*:**/@connections/*""
    - Effect: Allow
      Action:
        - ""s3:GetObject""
        - ""s3:PutObject""
      Resource:
        - ""arn:aws:s3:::botbind-addons/*""
  vpc:
    truncated

functions:
  websocketHandler:
    handler: handler.websocketHandler
    events:
      - websocket:
          route: $connect
          authorizer: authHandler
      - websocket:
          route: $disconnect
      - websocket:
          route: $default
      - websocket:
          route: ping
      - websocket:
          route: getBot
  authHandler:
    handler: handler.authHandler

plugins:
  - serverless-offline
  - serverless-domain-manager
</code></pre>

<p>Error: </p>

<pre><code>Serverless Error ---------------------------------------

An error occurred: WebsocketsDeploymentStage - The following context variables are not supported: [$context.status] (Service: AmazonApiGatewayV2; Status Code: 400; Error Code: BadRequestException; Request ID: 4a522b6c-6ade-4f86-afcf-35017a22c30c).
</code></pre>

<p>Is this the right way to disable API logs, so I can reduce my CloudWatch costs?</p>
",5008677.0,,174777.0,,2019-12-03 18:02:43,2019-12-03 18:02:43,Disable Cloudwatch logs for serverless websocket app,<amazon-web-services><websocket><aws-lambda><serverless>,1,0,0.0,,,CC BY-SA 4.0,I have a WebSocket API hosted on AWS using the serverless framework   The problem is that I am seeing 150k logs every minute  and it is costing a bunch  The following log groups are the problem:  /aws/apigateway/mzl9lpgzn0/production API-Gateway-Execution-Logs_xkkvpjzgqj/production  I have followed their docs to disable logs like so  but it is showing an error   Error:   Is this the right way to disable API logs  so I can reduce my CloudWatch costs  
55340604,1,,,2019-03-25 14:54:21,,3,1422,"<p>would love to hear some opinions regarding hosting of an Angular Universal app.</p>

<p>Question - EC2 vs AWS Lambda</p>

<p>After finishing my application I initially created t2.micro linux instance to host my app, was happy to see that the site scores 97 in Googles page speed insight test.</p>

<p>Afterwards I came across AWS lambda, a serverless way to run my server rendering app!, as its cost depends on the amount of requests (which Is extremely low on my website) thought that could be a nice way to avoid paying $10 a month.</p>

<p>The only issue is - Google speed test (using AWS Lambda) scored a sad 80...  with a huge red flag on server response time.
After doing a few more tests seems like the function became warmer and got up to 92. that's not 98 but I can live with that.</p>

<p>The thing is, as Im planning to get about 20-50 requests spread through out the entire day it will stay cold, so SEO wise I'll stay on 80 score website instead of 98.</p>

<p>Is there something I'm missing? As convenient as it is should I just stick with EC2 for my needs?</p>

<p>Thanks for reading &lt;3</p>
",7210967.0,,,,,2019-03-25 15:30:46,Angular Universal - AWS Lambda or EC2,<angular><amazon-ec2><aws-lambda><angular-universal><aws-serverless>,2,0,,,,CC BY-SA 4.0,would love to hear some opinions regarding hosting of an Angular Universal app  Question - EC2 vs AWS Lambda After finishing my application I initially created t2 micro linux instance to host my app  was happy to see that the site scores 97 in Googles page speed insight test  Afterwards I came across AWS lambda  a serverless way to run my server rendering app   as its cost depends on the amount of requests (which Is extremely low on my website) thought that could be a nice way to avoid paying $10 a month  The only issue is - Google speed test (using AWS Lambda) scored a sad 80     with a huge red flag on server response time  After doing a few more tests seems like the function became warmer and got up to 92  thats not 98 but I can live with that  The thing is  as Im planning to get about 20-50 requests spread through out the entire day it will stay cold  so SEO wise Ill stay on 80 score website instead of 98  Is there something Im missing  As convenient as it is should I just stick with EC2 for my needs  Thanks for reading &lt;3 
55352734,1,55352925.0,,2019-03-26 08:29:10,,1,470,"<p>I have built a deployment package with pandas, numpy, etc for my sample code to run. The size is some 46 MB. Doubt is, do I have to zip my code update every time and again update the entire deployment package to AWS S3 for a simple code update too?</p>

<p>Is there any other way, by which, I can avoid the 45 MB upload cost of S3 everytime and just upload the few KBs of code?</p>
",5536733.0,,8708364.0,,2019-03-26 08:30:19,2019-04-22 00:23:44,AWS Lambda Deployment Package minute code change need every time update,<python><pandas><amazon-web-services><aws-lambda><python-3.6>,2,0,,,,CC BY-SA 4.0,I have built a deployment package with pandas  numpy  etc for my sample code to run  The size is some 46 MB  Doubt is  do I have to zip my code update every time and again update the entire deployment package to AWS S3 for a simple code update too  Is there any other way  by which  I can avoid the 45 MB upload cost of S3 everytime and just upload the few KBs of code  
55421789,1,,,2019-03-29 16:31:31,,0,102,"<p>The goal is to scan and return all of the items in a DynamoDB table, <strong>but</strong> before the response is returned, modify a specific attribute of each specific item.</p>

<p>I have this completed already, but I'm curious to know if there is a more cost-effective way without looping through all the items.</p>

<p>Currently I'm returning a complete scan of the table and looping through each list item (found out it is not an object but a list):</p>

<pre><code>    dynamodb = boto3.resource('dynamodb')
    table = dynamodb.Table('&lt;table name&gt;')

    response = table.scan()

    items = response['Items']

    for item in items:
        item['Thumbnail'] = 'https://s3.amazonaws.com/&lt;s3bucket&gt;/' + item['Thumbnail']

    return items
</code></pre>

<p>I doubt the solution can be resolved without looping but if there is a solution that avoids looping I'm eager to hear it!</p>
",6122410.0,,6122410.0,,2019-03-29 17:16:34,2019-03-29 18:07:17,Modify item attribute after scan using boto3 in AWS Lambda,<python><amazon-web-services><aws-lambda><amazon-dynamodb><boto3>,1,0,,,,CC BY-SA 4.0,The goal is to scan and return all of the items in a DynamoDB table  but before the response is returned  modify a specific attribute of each specific item  I have this completed already  but Im curious to know if there is a more cost-effective way without looping through all the items  Currently Im returning a complete scan of the table and looping through each list item (found out it is not an object but a list):  I doubt the solution can be resolved without looping but if there is a solution that avoids looping Im eager to hear it  
37439656,1,37473351.0,,2016-05-25 14:08:07,,4,2351,"<p>What I'm trying to do here is sending a notification via SNS and APNS when a specific user is part of a newly added DynamoDB Item. I want to send it to the users Cognito Identity ID, not to device token.</p>

<p>So Lambda should be triggered when the item is added and then go through a list of Cognito Identity IDs, which is also part of the item. </p>

<p>Then Lambda is supposed to publish the push notifications to each Cognito Identity ID. </p>

<p>All the devices are registered as endpoints within sns. I also keep the Cognito Identity ID in the ""user data"" row for the endpoint.</p>

<p>But i didn't find a way to send notifications directly to a Cognito Identity ID. Do i have to add a topic for each user and send the notification to that topic? Or do i have to store another DynamoDB table to map Cognito Identity IDs to device tokens? It would be great if someone knew an easier and not too expensive way!</p>

<p>Thank you!</p>
",5506731.0,,5506731.0,,2016-05-25 14:14:45,2016-05-27 01:21:41,AWS SNS Publish to specific User via Cognito Identity ID,<amazon-web-services><amazon-dynamodb><amazon-sns><aws-lambda>,1,0,2.0,,,CC BY-SA 3.0,What Im trying to do here is sending a notification via SNS and APNS when a specific user is part of a newly added DynamoDB Item  I want to send it to the users Cognito Identity ID  not to device token  So Lambda should be triggered when the item is added and then go through a list of Cognito Identity IDs  which is also part of the item   Then Lambda is supposed to publish the push notifications to each Cognito Identity ID   All the devices are registered as endpoints within sns  I also keep the Cognito Identity ID in the user data row for the endpoint  But i didnt find a way to send notifications directly to a Cognito Identity ID  Do i have to add a topic for each user and send the notification to that topic  Or do i have to store another DynamoDB table to map Cognito Identity IDs to device tokens  It would be great if someone knew an easier and not too expensive way  Thank you  
55740175,1,55740844.0,,2019-04-18 06:22:56,,0,736,"<p>What is the simplest and most cost effective way to trigger/run an AWS Lambda job whenever a file is put in the AWS S3 Bucket (in a certain path; Even though I know its all object in S3 and not like a File System, but still folder system exists for users' ease)?</p>
",5536733.0,,174777.0,,2019-04-18 07:10:40,2019-04-18 07:10:40,How to Trigger AWS Lambda job when a data is loaded into a certain S3 bucket?,<amazon-web-services><amazon-s3><aws-lambda><amazon-sns><amazon-cloudwatch>,1,0,,,,CC BY-SA 4.0,What is the simplest and most cost effective way to trigger/run an AWS Lambda job whenever a file is put in the AWS S3 Bucket (in a certain path; Even though I know its all object in S3 and not like a File System  but still folder system exists for users ease)  
56991822,1,,,2019-07-11 14:41:22,,7,360,"<p>I'm trying to understand the various steps and requirements I need to go through in order to make our website available from China, both on the regulation side (Great Firewall) but also on the technical side (technical limitations and changes to perform) for <a href=""https://unly.org/"" rel=""noreferrer"">https://unly.org/</a></p>
<p>Right now, it doesn't seem to be allowed: <a href=""http://www.chinafirewalltest.com/?siteurl=https%3A%2F%2Funly.org%2F"" rel=""noreferrer"">http://www.chinafirewalltest.com/?siteurl=https%3A%2F%2Funly.org%2F</a></p>
<p><a href=""https://i.stack.imgur.com/9JKIs.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/9JKIs.png"" alt=""enter image description here"" /></a></p>
<p>I don't need nor want to own a .ch website, I just want to make my website available for Chinese at <a href=""https://unly.org/"" rel=""noreferrer"">https://unly.org/</a>. Also, the website is currently hosted on AWS Lambda (using the Serverless framework), and only deployed in the eu-west-1 region (Ireland) only.</p>
<p>The website doesn't sell anything online: It's an information website, not e-commerce.</p>
<hr />
<p>I've looked into this issue for a few hours, but I'm a bit lost regarding the exact steps needed to make it happen.</p>
<p>Here are a few questions I haven't found answers for:</p>
<ul>
<li>Does deploying the lambda to <code>cn-north-1</code> (China Beijing) is a requirement or can Chinese users access my <code>eu-west-1</code> lambda if I get an ICP license?</li>
</ul>
<p>Regardless of the deploying region, I seem to need an ICP License, as the AWS FAQ says at
<a href=""https://www.amazonaws.cn/en/about-aws/china/faqs/#new%20step"" rel=""noreferrer"">https://www.amazonaws.cn/en/about-aws/china/faqs/#new%20step</a>:</p>
<blockquote>
<p><strong>Q: Do I need to file for ICP Recordal or ICP License if I want to host public content on AWS China (Beijing) Region or AWS China (Ningxia) Region?</strong></p>
<p>Yes. In accordance with Chinese laws and regulations, if you use either AWS China Region to host a website providing non-commercial internet information services, you must undertake filing procedures for a non-commercial website (ICP Recordal) through the relevant government authority. If you use either AWS China Region to host a website providing commercial internet information services, you must obtain a value-added telecommunications license for a commercial website (ICP License) from the relevant government authority. You may be required to produce your ICP Recordal or ICP License, as applicable, before you host public content using one of the AWS China Regions.</p>
<p>AWS China (Beijing) Region is operated by Sinnet, who is responsible for content hosted in the Beijing Region, while AWS China (Ningxia) Region is operated by NWCD, who is responsible for content hosted in the Ningxia Region. Both Sinnet and NWCD provide support at no additional charge for customers seeking ICP related services, though customers are responsible for any fees imposed by the applicable government authorities. To learn more about the filing procedures, please visit Sinnet at <a href=""http://www.sinnet.com.cn/service.aspx?PartNodeId=35"" rel=""noreferrer"">http://www.sinnet.com.cn/service.aspx?PartNodeId=35</a> and NWCD at <a href=""http://nwcdcloud.cn/ICP.aspx"" rel=""noreferrer"">http://nwcdcloud.cn/ICP.aspx</a>.</p>
</blockquote>
<p>As for actually getting the license, it's a bit out of topic here, but I couldn't understand the first provider workflow:</p>
<ul>
<li><a href=""http://www.sinnet.com.cn/en/"" rel=""noreferrer"">http://www.sinnet.com.cn/en/</a> website is a mix of english and chinese and I got lost in translation (even when using their english website version)</li>
<li><a href=""http://nwcdcloud.cn/ContactUs.aspx"" rel=""noreferrer"">http://nwcdcloud.cn/ContactUs.aspx</a> seems to require to send an email to support@amazonaws.com.cn, no idea what happens next</li>
</ul>
<p>Anyway, the process seems to take around 4-6 weeks. So, it likely takes even more time than that.</p>
<hr />
<p>Regarding the technical details now, it seems like the China region (<code>cn-north-1</code> Beijing and <code>cn-northwest-1</code> Ningxia) behave in a very particular way on AWS Lambda.</p>
<ol>
<li>They only support <code>REGIONAL</code> endpoints</li>
<li>They do not support native Serverless <code>environment</code> variables</li>
</ol>
<p>See</p>
<ul>
<li><a href=""https://github.com/serverless/serverless/pull/4665#issuecomment-365843810"" rel=""noreferrer"">https://github.com/serverless/serverless/pull/4665#issuecomment-365843810</a></li>
<li><a href=""https://stackoverflow.com/questions/56080161/lambda-environmentvariablesfeature-is-not-supported-in-cn-north-1-region/56080504#56080504"">Lambda - EnvironmentVariablesFeature is not supported in cn-north-1 region</a></li>
</ul>
<p>Also, there are technical impacts on the website itself:</p>
<ul>
<li>Google services are banned, or limited (Google Analytics (limited), Google Tag Manager, Google Fonts (banned)) and must be changed, converted to owned CDN, etc.</li>
</ul>
<p>And I've probably missed other technical limitations, since that's just those I learned about within 2h of digging around.</p>
<p>Are there other steps I overlooked? (regulation or technical)
Do you have any advices or feedback about how to make a website hosted on AWS Lambda available in China?</p>
",2391795.0,,-1.0,,2020-06-20 09:12:55,2020-04-16 23:02:44,AWS Lambda + Serverless framework - Make website accessible from China,<amazon-web-services><aws-lambda><serverless-framework><aws-serverless><great-firewall-of-china>,1,0,3.0,,,CC BY-SA 4.0,Im trying to understand the various steps and requirements I need to go through in order to make our website available from China  both on the regulation side (Great Firewall) but also on the technical side (technical limitations and changes to perform) for  Right now  it doesnt seem to be allowed:   I dont need nor want to own a  ch website  I just want to make my website available for Chinese at   Also  the website is currently hosted on AWS Lambda (using the Serverless framework)  and only deployed in the eu-west-1 region (Ireland) only  The website doesnt sell anything online: Its an information website  not e-commerce   Ive looked into this issue for a few hours  but Im a bit lost regarding the exact steps needed to make it happen  Here are a few questions I havent found answers for:  Does deploying the lambda to  (China Beijing) is a requirement or can Chinese users access my  lambda if I get an ICP license   Regardless of the deploying region  I seem to need an ICP License  as the AWS FAQ says at :  Q: Do I need to file for ICP Recordal or ICP License if I want to host public content on AWS China (Beijing) Region or AWS China (Ningxia) Region  Yes  In accordance with Chinese laws and regulations  if you use either AWS China Region to host a website providing non-commercial internet information services  you must undertake filing procedures for a non-commercial website (ICP Recordal) through the relevant government authority  If you use either AWS China Region to host a website providing commercial internet information services  you must obtain a value-added telecommunications license for a commercial website (ICP License) from the relevant government authority  You may be required to produce your ICP Recordal or ICP License  as applicable  before you host public content using one of the AWS China Regions  AWS China (Beijing) Region is operated by Sinnet  who is responsible for content hosted in the Beijing Region  while AWS China (Ningxia) Region is operated by NWCD  who is responsible for content hosted in the Ningxia Region  Both Sinnet and NWCD provide support at no additional charge for customers seeking ICP related services  though customers are responsible for any fees imposed by the applicable government authorities  To learn more about the filing procedures  please visit Sinnet at  and NWCD at    As for actually getting the license  its a bit out of topic here  but I couldnt understand the first provider workflow:   website is a mix of english and chinese and I got lost in translation (even when using their english website version)  seems to require to send an email to support@amazonaws com cn  no idea what happens next  Anyway  the process seems to take around 4-6 weeks  So  it likely takes even more time than that   Regarding the technical details now  it seems like the China region ( Beijing and  Ningxia) behave in a very particular way on AWS Lambda   They only support  endpoints They do not support native Serverless  variables  See     Also  there are technical impacts on the website itself:  Google services are banned  or limited (Google Analytics (limited)  Google Tag Manager  Google Fonts (banned)) and must be changed  converted to owned CDN  etc   And Ive probably missed other technical limitations  since thats just those I learned about within 2h of digging around  Are there other steps I overlooked  (regulation or technical) Do you have any advices or feedback about how to make a website hosted on AWS Lambda available in China  
56997106,1,,,2019-07-11 20:48:45,,0,318,"<p>I am working on creating a lambda manager that interacts with other lambda instances.</p>

<p>The manager, which is also a lambda function will manage another lambda function that is executing a workload.</p>

<p><strong>edit:</strong>
<em>I am trying to execute workloads on a lambdas as if they were in a VM to take advantage of the serverless pricing model, performance, and convenience. This is a research project inspired by the findings of a few recent research papers.</em></p>

<p>This gives a quick overview of one of the papers: <a href=""https://www.serverlesscomputing.org/wosc4/presentations/p7-wosc2018_lloyd_pdf_version.pdf"" rel=""nofollow noreferrer"">https://www.serverlesscomputing.org/wosc4/presentations/p7-wosc2018_lloyd_pdf_version.pdf</a></p>

<p>The manager will have the ability to:</p>

<ul>
<li>act as a keep alive function to allow the workload to persist</li>
<li>destroy the lambda executing a workload (in python: call sys.exit()) </li>
<li>have some scheduling properties</li>
</ul>

<p>and so my thinking is that I need a way to send a message or 
interact with that lambda instance.</p>

<p><strong>Methods for communicating between two lambda instances?</strong></p>

<p>I have tried using the Gateway API with websockets but that only lets me redirect to specific lambda functions, which is useful but not for the current problem.</p>

<p>I have looked at SNS, I think it can be done with SNS by creating a new topic and passing the arn to the invoked worker lambda but that seems ineffecient. The idea  came from here: <a href=""https://stackoverflow.com/questions/44655685/aws-sqs-asynchronous-queuing-pattern-request-response"">AWS SQS Asynchronous Queuing Pattern (Request/Response)</a>.</p>

<p>Thank you so much!</p>
",6069825.0,,6069825.0,,2019-07-24 23:36:10,2019-07-24 23:36:10,Communication options for lambda instances?,<amazon-web-services><aws-lambda><boto3>,0,3,1.0,,,CC BY-SA 4.0,I am working on creating a lambda manager that interacts with other lambda instances  The manager  which is also a lambda function will manage another lambda function that is executing a workload  edit: I am trying to execute workloads on a lambdas as if they were in a VM to take advantage of the serverless pricing model  performance  and convenience  This is a research project inspired by the findings of a few recent research papers  This gives a quick overview of one of the papers:  The manager will have the ability to:  act as a keep alive function to allow the workload to persist destroy the lambda executing a workload (in python: call sys exit())  have some scheduling properties  and so my thinking is that I need a way to send a message or  interact with that lambda instance  Methods for communicating between two lambda instances  I have tried using the Gateway API with websockets but that only lets me redirect to specific lambda functions  which is useful but not for the current problem  I have looked at SNS  I think it can be done with SNS by creating a new topic and passing the arn to the invoked worker lambda but that seems ineffecient  The idea  came from here:   Thank you so much  
39153410,1,39153501.0,,2016-08-25 19:34:22,,1,679,"<p>I am seeing a steep increase on my AWS account costs. The largest cost items are: <strong>EC2: 67% RDS: 12%</strong></p>

<p>I have more than 50 stopped EC2s. One of them has been sitting there in a stopped state from September of the year 2015.</p>

<p>I found the way to get the stopped time of EC2s using variable called:</p>

<blockquote>
  <p>state_transition_reason</p>
</blockquote>

<p>Here how the code looks:</p>

<pre><code>import boto3

session = boto3.Session(region_name=""us-east-1"")
ec2 = session.resource('ec2')

instances = ec2.instances.filter(
            Filters=[{'Name': 'instance-state-name', 'Values': ['stopped']}])

count = 0

for i in instances:
   print ""{0}, {1}, {2}"".format( i.id, i.state_transition_reason, i.state['Name']) 
   count +=1

print count
</code></pre>

<p>It prints out the following information:</p>

<pre><code>i-pll78233b, User initiated (2016-07-06 21:14:03 GMT), stopped
i-tr62l5647, User initiated (2015-12-18 21:35:20 GMT), stopped
i-9oc4391ca, User initiated (2016-03-17 04:37:46 GMT), stopped
55
</code></pre>

<p><strong>My question is</strong>: How can I sort instances (EC2s) by their time being stopped. In my example I would love to see the output in the following order starting from year 2015 accordingly:</p>

<pre><code>i-tr62l5647, User initiated (2015-12-18 21:35:20 GMT), stopped
i-9oc4391ca, User initiated (2016-03-17 04:37:46 GMT), stopped
i-pll78233b, User initiated (2016-07-06 21:14:03 GMT), stopped
55
</code></pre>

<p>Thanks.</p>
",6728217.0,,,,,2016-08-25 20:04:25,"How to sort stopped EC2s by time using ""state_transition_reason"" variable? Python Boto3",<python><amazon-web-services><amazon-ec2><aws-lambda><boto3>,1,0,1.0,,,CC BY-SA 3.0,I am seeing a steep increase on my AWS account costs  The largest cost items are: EC2: 67% RDS: 12% I have more than 50 stopped EC2s  One of them has been sitting there in a stopped state from September of the year 2015  I found the way to get the stopped time of EC2s using variable called:  state_transition_reason  Here how the code looks:  It prints out the following information:  My question is: How can I sort instances (EC2s) by their time being stopped  In my example I would love to see the output in the following order starting from year 2015 accordingly:  Thanks  
56049038,1,,,2019-05-08 21:04:53,,-3,310,"<p>What are other cloud-based, serverless, managed databases that have a pricing system similar to DynamoDB? I'm specifically talking about the <strong>On-Demand</strong> mode of DynamoDB</p>

<p>By similar I mean, being able to pay per requests and per GB/Month or similar, instead of paying a fixed monthly fee, I understand this will mostly gear towards NoSQL databases but for me <strong>it doesn't matter the type of database</strong>.</p>

<p>Apart from DynamoDB, I have also found FaunaDB, but it lacks documentation and tools/integrations</p>

<p>It's important to note that I'm not considering pay per hour here, as that is irrelevant once you go to production.</p>
",3529833.0,,3416135.0,,2019-06-18 18:27:06,2019-06-18 18:27:06,DynamoDB alternatives in terms of pricing? (Only pay what you use),<nosql><amazon-dynamodb><cloud><serverless><amazon-timestream>,1,2,,,,CC BY-SA 4.0,What are other cloud-based  serverless  managed databases that have a pricing system similar to DynamoDB  Im specifically talking about the On-Demand mode of DynamoDB By similar I mean  being able to pay per requests and per GB/Month or similar  instead of paying a fixed monthly fee  I understand this will mostly gear towards NoSQL databases but for me it doesnt matter the type of database  Apart from DynamoDB  I have also found FaunaDB  but it lacks documentation and tools/integrations Its important to note that Im not considering pay per hour here  as that is irrelevant once you go to production  
55031936,1,,,2019-03-06 20:52:10,,0,733,"<p>The context here is simple, there's a lambda (lambda1) that creates a file asynchronously and then uploads it to S3.</p>

<p>Then, another lambda (lambda2) receives the soon-to-exist file name and needs to keep checking S3 until the file exists.</p>

<p>I don't think S3 triggers will work because lambda2 is invoked by a client request</p>

<p>1) Do I get charged for this kind of request between lambda and S3? I will be polling it until the object exists</p>

<p>2) What other way could I achieve this that doesn't incur charges?</p>

<p>3) What method do I use to check if a file exists in S3? (just try to get it and check status code?)</p>
",3529833.0,,3529833.0,,2019-03-06 21:02:15,2019-03-06 21:15:14,Long polling AWS S3 to check if item exists?,<python-3.x><amazon-web-services><amazon-s3><aws-lambda><boto3>,2,4,,,,CC BY-SA 4.0,The context here is simple  theres a lambda (lambda1) that creates a file asynchronously and then uploads it to S3  Then  another lambda (lambda2) receives the soon-to-exist file name and needs to keep checking S3 until the file exists  I dont think S3 triggers will work because lambda2 is invoked by a client request 1) Do I get charged for this kind of request between lambda and S3  I will be polling it until the object exists 2) What other way could I achieve this that doesnt incur charges  3) What method do I use to check if a file exists in S3  (just try to get it and check status code ) 
59736237,1,,,2020-01-14 14:52:46,,0,530,"<p>I have a aws lambda function which is invoked by API Gateway. The Lambda function calls external API endpoints and it sometime receives network time out while calling external API.
What is the best way to implement retry mechanism in aws lambda to handle network time out or other server side errors? Also is it good to use retry inside lambda function, which cost as per execution time?
Any recommendation is highly appreciated.
Regards</p>
",2503050.0,,3689450.0,,2020-01-14 14:59:01,2020-01-14 16:35:54,Retries in aws lambda for External API timeout,<amazon-web-services><aws-lambda><timeout>,2,0,,,,CC BY-SA 4.0,I have a aws lambda function which is invoked by API Gateway  The Lambda function calls external API endpoints and it sometime receives network time out while calling external API  What is the best way to implement retry mechanism in aws lambda to handle network time out or other server side errors  Also is it good to use retry inside lambda function  which cost as per execution time  Any recommendation is highly appreciated  Regards 
55226441,1,,,2019-03-18 16:58:32,,1,208,"<p>I am developing a React Native application for IOS and Android. I am using Django for my backend and aws rds for my database. It is deployed on AWS Lambda and both my lambdas and my rds are in a VPC. Everything worked well except for push notifications as they require my lambda functions to communicate to the public internet.</p>

<p>One way would be to create a NAT Gateway to allow that communication, but a NAT Gateway is quite costly.</p>

<p>I am thinking of another way which involves AWS Simple Notification Services (SNS). If I integrate that to my Django app, would the lambda functions be able to communicate with AWS SNS without requiring a NAT Gateway?</p>
",8083096.0,,8083096.0,,2019-03-18 17:15:08,2019-03-18 17:15:08,AWS Lambda and AWS SNS: Does it need a NAT Gateway?,<aws-lambda><apple-push-notifications><firebase-cloud-messaging><amazon-sns><aws-vpc>,1,1,,,,CC BY-SA 4.0,I am developing a React Native application for IOS and Android  I am using Django for my backend and aws rds for my database  It is deployed on AWS Lambda and both my lambdas and my rds are in a VPC  Everything worked well except for push notifications as they require my lambda functions to communicate to the public internet  One way would be to create a NAT Gateway to allow that communication  but a NAT Gateway is quite costly  I am thinking of another way which involves AWS Simple Notification Services (SNS)  If I integrate that to my Django app  would the lambda functions be able to communicate with AWS SNS without requiring a NAT Gateway  
55227785,1,,,2019-03-18 18:22:25,,2,364,"<p>I'm processing relatively large images using AWS Lambda (<a href=""https://registry.opendata.aws/sentinel-2/"" rel=""nofollow noreferrer"">https://registry.opendata.aws/sentinel-2/</a>). </p>

<p>In order to process these images, I split them into smaller images (~1500 ""chips"") which can be processed independently (the number of chips varies unpredictably depending on the content of the source image). Chips are processed in parallel using multiple invocations of a Lambda that takes in a ""page"" of a couple of hundred chips. </p>

<p>Here's where I'm stuck: when all pages have been processed, I need to combine results into a single output image, but how to know when all pages - the ""variable batch of invocations"" - are complete?</p>

<p>I've considered e.g. writing progress information to s3 or dynamo and invoking the combining function after every page so that only the last invocation of that function goes ahead (when a progress check returns as complete). I've seen options like futures/promises, but the processing time of a page of chips is of the order of 10-15 minutes so I don't want to keep a ""controller"" function waiting for the futures/promises to complete, because at that point it's cheaper to go with multiple invocations.</p>

<p>Is there a better solution that writing out progress information and checking it multiple times?</p>

<p>(NB I've seen this question: <a href=""https://stackoverflow.com/questions/41631548/fork-and-join-with-amazon-lambda"">Fork and Join with Amazon Lambda</a>)</p>
",723506.0,,,,,2019-03-18 18:42:33,"""Fork and Join"" with serverless functions (e.g. AWS Lambda) / Python",<python><amazon-web-services><aws-lambda>,1,0,,,,CC BY-SA 4.0,Im processing relatively large images using AWS Lambda ()   In order to process these images  I split them into smaller images (~1500 chips) which can be processed independently (the number of chips varies unpredictably depending on the content of the source image)  Chips are processed in parallel using multiple invocations of a Lambda that takes in a page of a couple of hundred chips   Heres where Im stuck: when all pages have been processed  I need to combine results into a single output image  but how to know when all pages - the variable batch of invocations - are complete  Ive considered e g  writing progress information to s3 or dynamo and invoking the combining function after every page so that only the last invocation of that function goes ahead (when a progress check returns as complete)  Ive seen options like futures/promises  but the processing time of a page of chips is of the order of 10-15 minutes so I dont want to keep a controller function waiting for the futures/promises to complete  because at that point its cheaper to go with multiple invocations  Is there a better solution that writing out progress information and checking it multiple times  (NB Ive seen this question: ) 
41707732,1,41713578.0,,2017-01-17 21:56:49,,2,2863,"<p>Having trouble configuring AWS Lambda to be triggered by a Rule->Trigger as a Scheduled Event Source using CloudFormation (in reality, using Python's Troposphere.) This has cost me a couple of days already, and any help would be appreciated.</p>

<p>Here's the relevant CF JSON snippet -</p>

<pre><code>        ""DataloaderRetrier"": {
        ""Properties"": {
            ""Code"": {
                ""S3Bucket"": ""mycompanylabs-config"",
                ""S3Key"": ""v3/mycompany-component-loader-lambda-0.5.jar""
            },
            ""FunctionName"": ""DataloaderRetriervitest27"",
            ""Handler"": ""mycompany.ScheduledEventHandler::handleRequest"",
            ""MemorySize"": 320,
            ""Role"": ""arn:aws:iam::166662328783:role/kinesis-lambda-role"",
            ""Runtime"": ""java8"",
            ""VpcConfig"": {
                ""SecurityGroupIds"": [
                    ""sg-2f1f6047""
                ],
                ""SubnetIds"": [
                    ""subnet-ec3c1435""
                ]
            }
        },
        ""Type"": ""AWS::Lambda::Function""
    },
    ""DataloaderRetrierEventTriggerPermission"": {
        ""Properties"": {
            ""Action"": ""lambda:InvokeFunction"",
            ""FunctionName"": {
                ""Fn::GetAtt"": [
                    ""DataloaderRetrier"",
                    ""Arn""
                ]
            },
            ""Principal"": ""events.amazonaws.com"",
            ""SourceAccount"": {
                ""Ref"": ""AWS::AccountId""
            },
            ""SourceArn"": {
                ""Fn::GetAtt"": [
                    ""DataloaderRetrierEventTriggerRule"",
                    ""Arn""
                ]
            }
        },
        ""Type"": ""AWS::Lambda::Permission""
    },
    ""DataloaderRetrierEventTriggerRule"": {
        ""DependsOn"": ""DataloaderRetrier"",
        ""Properties"": {
            ""Description"": ""Reminding the lambda to read from the retry SQS"",
            ""Name"": ""DataloaderRetrierEventTriggerRulevitest27"",
            ""ScheduleExpression"": ""rate(1 minute)"",
            ""State"": ""ENABLED"",
            ""Targets"": [
                {
                    ""Arn"": {
                        ""Fn::GetAtt"": [
                            ""DataloaderRetrier"",
                            ""Arn""
                        ]
                    },
                    ""Id"": ""DataloaderRetrierEventTriggerTargetvitest27"",
                    ""Input"": ""{\""Hey\"":\""WAKE UP!\""}""
                }
            ]
        },
        ""Type"": ""AWS::Events::Rule""
    }
</code></pre>

<p>The AWS Lambda function shows zero invocations, and the Events->Rules metric shows the correct number of invocations, however they all fail. The Lambda shows the trigger in the Triggers section, and the Rule shows the lambda in its trigger sections. They link up fine.</p>

<p>However, if I go in and manually create <em>the same trigger</em> under the rule in the web console, it will happily start sending events to the Lambda. </p>

<p>PS - here's the troposphere code:</p>

<pre><code># DATALOADER RETRIER LAMBDA
dataloader_retrier = t.add_resource(awslambda.Function(
    ""DataloaderRetrier"",
    Code=awslambda.Code(
        ""DataloaderRetrierCode"",
        S3Bucket='mycompanylabs-config',
        S3Key='v3/mycompany-snowplow-loader-lambda-0.5.jar'
    ),
    FunctionName=suffix(""DataloaderRetrier""),
    Handler=""mycompany.ScheduledEventHandler::handleRequest"",
    MemorySize=""320"",
    Role=""arn:aws:iam::166662328783:role/kinesis-lambda-role"",
    Runtime=""java8"",
    VpcConfig=lambda_vpc_config
))

dataloader_retrier_scheduled_rule = t.add_resource(events.Rule(
    ""DataloaderRetrierEventTriggerRule"",
    Name=suffix(""DataloaderRetrierEventTriggerRule""),
    Description=""Reminding the lambda to read from the retry SQS"",
    Targets=[events.Target(
        Id=suffix(""DataloaderRetrierEventTriggerTarget""),
        Arn=tr.GetAtt(""DataloaderRetrier"", ""Arn""),
        Input='{""Hey"":""WAKE UP!""}'
    )],
    State='ENABLED',
    ScheduleExpression=""rate(1 minute)"",
    DependsOn=""DataloaderRetrier""
)),

t.add_resource(awslambda.Permission(
    ""DataloaderRetrierEventTriggerPermission"",
    Action=""lambda:InvokeFunction"",
    FunctionName=tr.GetAtt(""DataloaderRetrier"", ""Arn""),
    Principal=""events.amazonaws.com"",
    SourceAccount=tr.Ref(""AWS::AccountId""),
    SourceArn=tr.GetAtt(""DataloaderRetrierEventTriggerRule"", ""Arn"")
))
</code></pre>
",395990.0,,,,,2017-01-18 07:32:44,AWS Scheduled Event Rule for Lambda doesn't work in CloudFormation,<amazon-web-services><aws-lambda><amazon-cloudformation><troposphere>,1,0,,,,CC BY-SA 3.0,Having trouble configuring AWS Lambda to be triggered by a Rule-&gt;Trigger as a Scheduled Event Source using CloudFormation (in reality  using Pythons Troposphere ) This has cost me a couple of days already  and any help would be appreciated  Heres the relevant CF JSON snippet -  The AWS Lambda function shows zero invocations  and the Events-&gt;Rules metric shows the correct number of invocations  however they all fail  The Lambda shows the trigger in the Triggers section  and the Rule shows the lambda in its trigger sections  They link up fine  However  if I go in and manually create the same trigger under the rule in the web console  it will happily start sending events to the Lambda   PS - heres the troposphere code:  
55237861,1,55239218.0,,2019-03-19 09:44:04,,1,269,"<p>Ive looked at the tutorials but I dont understand. I have a function that processes stripe payments. I think it has something to do with packages.json but I cant find information about where to put it or how to run it. </p>

<p>This is the code:</p>

<pre><code>https://github.com/alexmacarthur/netlify-lambda-function-example/blob/master/lambda-src/purchase.js
</code></pre>

<p>With the important line simply being:</p>

<pre><code>require('dotenv').config();
</code></pre>

<p>The error I get is:</p>

<pre><code>{""errorMessage"":""Cannot find module 'dotenv'""
</code></pre>

<p>How do I set about including this? Do I have to put a packages.json file in the same folder as my function? I tried that and it didnt make any difference. </p>
",1738522.0,,,,,2019-03-19 10:51:03,Including modules in Netlify AWS lambda function,<node.js><aws-lambda><netlify>,1,1,,,,CC BY-SA 4.0,Ive looked at the tutorials but I dont understand  I have a function that processes stripe payments  I think it has something to do with packages json but I cant find information about where to put it or how to run it   This is the code:  With the important line simply being:  The error I get is:  How do I set about including this  Do I have to put a packages json file in the same folder as my function  I tried that and it didnt make any difference   
59420115,1,59486281.0,,2019-12-20 05:09:45,,4,685,"<p>I am fetching prices (usually thousands) from an API using Axios and then want to store it in DynamoDB. If I invoke the lambda function locally everything works as expected, but if I deploy the function and call it using AWS CLI it does not store any values in DynamoDB anymore. The data I receive in the request and also the response from the Axios call is the same.</p>

<p>I somehow think it is a scope issue of the async function calling DynamoDB, but I am not able to solve it. Looking forward to your suggestions. Let me know if you need more code.</p>

<p>updatePrice.js</p>

<pre><code>import { updatePrice } from ""./libs/pricing-lib"";
import {
    success,
    failure
} from ""./libs/response-lib"";

export async function main(event, context) {
    try {
        let result = await updatePrice(event.pathParameters.id, event.pathParameters.date);
        return success(result);
    } catch(e) {
        return failure(e);
    }
}
</code></pre>

<p>dynamodb-lib-js</p>

<pre><code>import AWS from ""aws-sdk"";
export function call(action, params) {
    const dynamoDb = new AWS.DynamoDB.DocumentClient();
    return dynamoDb[action](params).promise();
}
</code></pre>

<p>pricing-lib.js</p>

<pre><code>export async function updatePrice(stockid, from) {
  try {
    let url = getUrl(stockid, from);
    const resultEodApi = (await axios.get(url)).data;

    resultEodApi.map((price) =&gt; {
      try {
        let priceParams = {
          TableName: process.env.pricesTableName,
          Item: {
            stockid: stockid,
            date: price.date,
            close: price.close
          }
        };
        dynamoDbLib.call(""put"", priceParams);
      } catch (e) {
        return e;
      }
    });

    return true;
  } catch (e) {
    return e;
  }
}
</code></pre>
",1568969.0,,174777.0,,2019-12-20 05:49:19,2021-11-30 11:49:11,DynamoDB record not added through map loop,<node.js><aws-lambda><amazon-dynamodb><serverless>,3,2,,,,CC BY-SA 4.0,I am fetching prices (usually thousands) from an API using Axios and then want to store it in DynamoDB  If I invoke the lambda function locally everything works as expected  but if I deploy the function and call it using AWS CLI it does not store any values in DynamoDB anymore  The data I receive in the request and also the response from the Axios call is the same  I somehow think it is a scope issue of the async function calling DynamoDB  but I am not able to solve it  Looking forward to your suggestions  Let me know if you need more code  updatePrice js  dynamodb-lib-js  pricing-lib js  
59771715,1,,,2020-01-16 14:23:46,,1,124,"<p>We have a production scenario with users invoking expensive NLP functions running for short periods of time (say 30s). Because of the high load and intermittent usage, we're looking into Lambda function deployment. However - our packages are big. </p>

<p>I'm trying to fit AllenNLP in a lambda function, which in turn depends on pytorch, scipy, spacy and numpy and a few other libs. </p>

<h2>What I've tried</h2>

<p>Following recommendations made <a href=""https://medium.com/@angelatao0123/serving-pytorch-nlp-models-on-aws-lambda-f735190ec16c"" rel=""nofollow noreferrer"">here</a> and the example <a href=""https://github.com/ryfeus/lambda-packs"" rel=""nofollow noreferrer"">here</a>, tests and additional files are removed. I also use a non-cuda version of Pytorch which gets its' size down. I can package an AllenNLP deployment down to about 512mb. Currently, this is still too big for AWS Lambda. </p>

<h2>Possible fixes?</h2>

<p>I'm wondering if anyone of has experience with one of the following potential pathways:</p>

<ol>
<li><p>Cutting PyTorch out of AllenNLP. Without Pytorch, we're in reach of getting it to 250mb. We only need to load archived models in production, but that does seem to use some of the PyTorch infrastructure. Maybe there are alternatives?</p></li>
<li><p>Invoking PyTorch in (a fork of) AllenNLP as a second lambda function. </p></li>
<li><p>Using S3 to deliver some of the dependencies: SIMlinking some of the larger <code>.so</code> files and serving them from an S3 bucket might help. This does create an additional problem: the Semnatic Role Labelling we're using from AllenNLP also requires some language models of around 500mb, for which the ephemeral storage could be used - but maybe these can be streamed directly into RAM from S3?</p></li>
</ol>

<p>Maybe i'm missing an easy solution. Any direction or experiences would be much appreciated!</p>
",7967438.0,,,,,2020-01-16 15:36:45,How to circumvent AWS package and ephemeral limits for large packages + large models,<python><aws-lambda><pytorch><allennlp>,1,0,,,,CC BY-SA 4.0,We have a production scenario with users invoking expensive NLP functions running for short periods of time (say 30s)  Because of the high load and intermittent usage  were looking into Lambda function deployment  However - our packages are big   Im trying to fit AllenNLP in a lambda function  which in turn depends on pytorch  scipy  spacy and numpy and a few other libs   What Ive tried Following recommendations made  and the example   tests and additional files are removed  I also use a non-cuda version of Pytorch which gets its size down  I can package an AllenNLP deployment down to about 512mb  Currently  this is still too big for AWS Lambda   Possible fixes  Im wondering if anyone of has experience with one of the following potential pathways:  Cutting PyTorch out of AllenNLP  Without Pytorch  were in reach of getting it to 250mb  We only need to load archived models in production  but that does seem to use some of the PyTorch infrastructure  Maybe there are alternatives  Invoking PyTorch in (a fork of) AllenNLP as a second lambda function   Using S3 to deliver some of the dependencies: SIMlinking some of the larger  files and serving them from an S3 bucket might help  This does create an additional problem: the Semnatic Role Labelling were using from AllenNLP also requires some language models of around 500mb  for which the ephemeral storage could be used - but maybe these can be streamed directly into RAM from S3   Maybe im missing an easy solution  Any direction or experiences would be much appreciated  
55454539,1,55530230.0,,2019-04-01 11:55:50,,11,1525,"<p>While learning the Serverless Framework I came across several tutorials showing how to run an Express instance in a Lambda. This seems to me like an overkill and against the purpose of Lambda functions.</p>

<p>The approach usually involves running an Express instance in the Lambda and proxying API Gateway requests to the Express router for internal handling.</p>

<p>To me the trivial approach is to just create an API in API Gateway and route individual requests to a Lambda for handling. Am I missing something?</p>

<p>Taking into account that Lambdas' execution time is 15 minutes, isn't just spinning up the Express instance quite expensive in terms of memory? Also, limited to 100 concurrent Lambda executions would create a bottleneck, no? Wouldn't an EC2 instance be a better fit in such case? Using a Lambda like this seems like an overkill.</p>

<p><strong>The only two benefits I see in running an Express instance in a Lambda are:</strong></p>

<ol>
<li>In the case of migration of an existing app written in Express, allows to slowly break down the app into API Gateway endpoints.</li>
<li>Internal handling of routing rather than relying on the API Gateway request/response model (proxying to Express router).</li>
</ol>

<p>What would be the benefit of such approach, in case I am missing something?</p>

<p><strong>Some resources promoting this approach:</strong></p>

<ul>
<li><a href=""https://medium.freecodecamp.org/express-js-and-aws-lambda-a-serverless-love-story-7c77ba0eaa35"" rel=""noreferrer"">Express.js and AWS Lambdaa serverless love story (Slobodan Stojanovi, freeCodeCamp)</a></li>
<li><a href=""https://github.com/awslabs/aws-serverless-express"" rel=""noreferrer"">awslabs/aws-serverless-express (GitHub)</a></li>
<li><a href=""https://serverless.com/blog/serverless-express-rest-api/"" rel=""noreferrer"">Deploy a REST API using Serverless, Express and Node.js (Alex DeBrie, Serverless Framework Blog)</a></li>
</ul>
",1786557.0,,1786557.0,,2019-04-01 13:20:54,2019-04-05 07:24:22,"Serverless - Running an Express instance in a Lambda function, good or bad?",<aws-lambda><serverless-framework><serverless><aws-serverless>,1,1,3.0,,,CC BY-SA 4.0,While learning the Serverless Framework I came across several tutorials showing how to run an Express instance in a Lambda  This seems to me like an overkill and against the purpose of Lambda functions  The approach usually involves running an Express instance in the Lambda and proxying API Gateway requests to the Express router for internal handling  To me the trivial approach is to just create an API in API Gateway and route individual requests to a Lambda for handling  Am I missing something  Taking into account that Lambdas execution time is 15 minutes  isnt just spinning up the Express instance quite expensive in terms of memory  Also  limited to 100 concurrent Lambda executions would create a bottleneck  no  Wouldnt an EC2 instance be a better fit in such case  Using a Lambda like this seems like an overkill  The only two benefits I see in running an Express instance in a Lambda are:  In the case of migration of an existing app written in Express  allows to slowly break down the app into API Gateway endpoints  Internal handling of routing rather than relying on the API Gateway request/response model (proxying to Express router)   What would be the benefit of such approach  in case I am missing something  Some resources promoting this approach:      
55458492,1,,,2019-04-01 15:20:01,,2,270,"<p>I have a couple of Lambda functions that require access to other services I have in AWS inside a VPC. So I have added these Lambda functions to the same VPC which makes them lose internet access. </p>

<p>One of the service I need access to is a SQS queue, which is not in the VPC as it doesn't support it.</p>

<p>I found two solutions to my problem:</p>

<ul>
<li>Create a NAT gateway and allow my function to access the internet</li>
<li>Create a VPC endpoint for SQS</li>
</ul>

<p>I wanted to go with the latter. When I did that, the Lambda worked fine, but I couldn't deploy new code to my Beanstalk instances. That should have nothing to do with it! I spent hours to realize Beanstalk uses SQS to send messages to my instances to perform the deploy.</p>

<p>Questions:</p>

<ul>
<li>Is it a better solution to use a VPC endpoint rather than a NAT gateway? (it seems to be cheaper)</li>
<li>If so, how can I keep my Beanstalk instances working correctly?</li>
</ul>
",7095424.0,,1133490.0,,2019-04-01 17:04:40,2019-04-01 17:04:40,Create AWS VPC Endpoint for SQS,<aws-lambda><amazon-sqs><amazon-vpc>,0,0,,,,CC BY-SA 4.0,I have a couple of Lambda functions that require access to other services I have in AWS inside a VPC  So I have added these Lambda functions to the same VPC which makes them lose internet access   One of the service I need access to is a SQS queue  which is not in the VPC as it doesnt support it  I found two solutions to my problem:  Create a NAT gateway and allow my function to access the internet Create a VPC endpoint for SQS  I wanted to go with the latter  When I did that  the Lambda worked fine  but I couldnt deploy new code to my Beanstalk instances  That should have nothing to do with it  I spent hours to realize Beanstalk uses SQS to send messages to my instances to perform the deploy  Questions:  Is it a better solution to use a VPC endpoint rather than a NAT gateway  (it seems to be cheaper) If so  how can I keep my Beanstalk instances working correctly   
55683373,1,,,2019-04-15 06:08:22,,0,25,"<p>I have created a web application (demo: <a href=""http://demo.dhruval.tech"" rel=""nofollow noreferrer"">http://demo.dhruval.tech</a> or <a href=""http://helloco.us-east-1.elasticbeanstalk.com/userRegistration.jsp"" rel=""nofollow noreferrer"">http://helloco.us-east-1.elasticbeanstalk.com/userRegistration.jsp</a>) of Shopping cart hosted on AWS elastic beanstalk. I want to develop Alexa skill for this web application which takes credentials from a user via Alexa voice and validates them. After this, Alexa will ask which items the user wants to add in cart and checkout.</p>

<p>A web application is running successfully on AWS. I have tried to use AWS Lambda and Alexa skill kit but a bit confused about how to integrate them in Java hibernate. I also have tried to use AWS S3 to generate and store bill of user which user can download manually via clicking on generated URL or via selecting option to send the bill to a mailbox.</p>

<p>There will be two ways to order from the web application.</p>

<ol>
<li>Using web browser 2. Using Alexa skill voice command</li>
</ol>

<p>When a user opts for the 2nd option, Alexa will ask for credentials and validates them. After validation Alexa will ask which items the user wants to add in cart. Later on, it will check out. </p>

<p>The web application is running smoothly except that bill generation. Need suggestions for how to link Alexa with Java Hibernate.</p>
",6643601.0,,,,,2019-04-15 06:08:22,Creating alexa skill of web application in hibernate framework hosted on AWS elastics beanstalk,<java><hibernate><amazon-s3><aws-lambda><alexa-skills-kit>,0,2,,,,CC BY-SA 4.0,I have created a web application (demo:  or ) of Shopping cart hosted on AWS elastic beanstalk  I want to develop Alexa skill for this web application which takes credentials from a user via Alexa voice and validates them  After this  Alexa will ask which items the user wants to add in cart and checkout  A web application is running successfully on AWS  I have tried to use AWS Lambda and Alexa skill kit but a bit confused about how to integrate them in Java hibernate  I also have tried to use AWS S3 to generate and store bill of user which user can download manually via clicking on generated URL or via selecting option to send the bill to a mailbox  There will be two ways to order from the web application   Using web browser 2  Using Alexa skill voice command  When a user opts for the 2nd option  Alexa will ask for credentials and validates them  After validation Alexa will ask which items the user wants to add in cart  Later on  it will check out   The web application is running smoothly except that bill generation  Need suggestions for how to link Alexa with Java Hibernate  
43034788,1,43036151.0,,2017-03-26 21:31:23,,1,832,"<p>I hope to build a mobile app that sends credit card information to an aws-lambda microservice, which then submits that information to stripe. I'm concerned about PCI compliance/security, and I'm wondering if there is something I'm missing. The following is my plan:</p>

<p>1) Users sign in using PCI compliant passwords - and are assigned unique ids and get cognito access keys.</p>

<p>2) Users enter payment information in the mobile app. The app then sends that credit card data via POST request using HTTPS to a cognito authenticated aws-lambda instance (api gateway is used to create endpoints). </p>

<p>3) Upon a successful post request the app deletes the local credit card data.</p>

<p>4) The lambda instance decrypts encrypted stripe secret access keys using KMS.</p>

<p>5) The lambda instance uses Stripe NodeJS sdk to send the data to stripe and stores stripe tokens in databases. </p>

<p>6) At no point does the Lambda instance save ANY credit card data - it ONLY writes Stripe tokens to the database.</p>

<p>Is there anything I'm missing here? Is there something I should be concerned about?</p>

<p>EDIT:</p>

<p>Additional Info:
Credit card details are collected within the app and stored in the app state until they are deleted. The https POST does not use Stripe tools because I'm using React Native. </p>
",3112506.0,,3112506.0,,2017-03-26 23:12:15,2017-03-27 00:23:12,"AWS Lambda stripe payment backend, PCI concerns?",<stripe-payments><aws-lambda><pci-compliance><aws-cognito>,1,9,1.0,,,CC BY-SA 3.0,I hope to build a mobile app that sends credit card information to an aws-lambda microservice  which then submits that information to stripe  Im concerned about PCI compliance/security  and Im wondering if there is something Im missing  The following is my plan: 1) Users sign in using PCI compliant passwords - and are assigned unique ids and get cognito access keys  2) Users enter payment information in the mobile app  The app then sends that credit card data via POST request using HTTPS to a cognito authenticated aws-lambda instance (api gateway is used to create endpoints)   3) Upon a successful post request the app deletes the local credit card data  4) The lambda instance decrypts encrypted stripe secret access keys using KMS  5) The lambda instance uses Stripe NodeJS sdk to send the data to stripe and stores stripe tokens in databases   6) At no point does the Lambda instance save ANY credit card data - it ONLY writes Stripe tokens to the database  Is there anything Im missing here  Is there something I should be concerned about  EDIT: Additional Info: Credit card details are collected within the app and stored in the app state until they are deleted  The https POST does not use Stripe tools because Im using React Native   
55859414,1,55860127.0,,2019-04-26 00:57:17,,0,2214,"<p>Setting response headers with API Gateway with AWS Lambda proxy integration is straight forward and looks like this:</p>

<pre><code>import zlib from 'zlib';

exports.handler = async (event, context, callback) =&gt; {
  const body = zlib.gzipSync(JSON.stringify({ data: 'mock' }));

  const headers = {};
  headers['Content-Type'] = 'application/json';
  headers['Content-Encoding'] = 'gzip';

  const responseObject = {
    statusCode: 200,
    headers,
    body: body.toString('base64'),
    isBase64Encoded: true
  };

  return callback(null, responseObject);
}
</code></pre>

<p>Everything is returned gzipped as expected. Because we set the content-encoding the browser decompresses the response.</p>

<p>Question is how can headers be similarly set when the Lambda function is invoked from the browser directly using AWS SDK JS? API Gateway is the service which implements the headers in the previous setup, without having API Gateway in front of AWS Lambda headers are ignored and being generically set to:</p>

<pre><code>access-control-allow-origin: *
access-control-expose-headers: x-amzn-RequestId,x-amzn-ErrorType,x-amzn-ErrorMessage,Date,x-amz-log-result,x-amz-function-error
content-length: 1242
content-type: application/json
date: Fri, 26 Apr 2019 00:36:35 GMT
status: 200
x-amz-executed-version: $LATEST
x-amzn-remapped-content-length: 0
x-amzn-requestid: &lt;REDACTED&gt;
x-amzn-trace-id: &lt;REDACTED&gt;
</code></pre>

<p>AWS SDK JS browser invoke code looks something like this:</p>

<pre><code>import AWS from 'aws-sdk';

AWS.config.region = 'us-east-1'; // Region
AWS.config.credentials = new AWS.CognitoIdentityCredentials({
    IdentityPoolId: '&lt;SOME IDENTITY&gt;',
});

const AWSLambda = new AWS.Lambda({region: REGION, apiVersion: '2015-03-31'});

const parameters = {
    FunctionName : 'MyFunctionName',
    InvocationType : 'RequestResponse',
    LogType : 'None',
    Payload: JSON.stringify({msg: 'hello lambda'})
};

(async () =&gt; {
  const response = await AWSLambda.invoke(shopParameters).promise();
  console.log(response);
})();
</code></pre>

<p>The returned response is the response object above as a string with the generic headers. Browser does not decompress the gzipped content, presumably because the content-encoding header is not being set. AWS Lambda when invoked from the browser, treats the whole Lambda response object as the response and performs no transformations that happen with API Gateway. For instance, API Gateway would pick up on the response object structure and map response object headers to the response before sending to the client.</p>

<p>Is there no way to set AWS Lambda headers without API Gateway? Or is the only option to decompress the gzip content manually on the client using something like <a href=""https://github.com/nodeca/pako"" rel=""nofollow noreferrer"">https://github.com/nodeca/pako</a> (sigh). </p>

<p>Idea to not use API Gateway came from AWS docs found here, like to avoid the API Gateway costs: <a href=""https://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/browser-invoke-lambda-function-example.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/browser-invoke-lambda-function-example.html</a></p>

<p>Any guidance, expertise, thoughts are much appreciated!</p>
",1079731.0,,1079731.0,,2019-04-26 02:15:37,2019-04-26 03:44:13,AWS Lambda set response headers when invoke originates from Browser AWS SDK,<javascript><amazon-web-services><aws-lambda><aws-sdk-js>,2,1,,,,CC BY-SA 4.0,Setting response headers with API Gateway with AWS Lambda proxy integration is straight forward and looks like this:  Everything is returned gzipped as expected  Because we set the content-encoding the browser decompresses the response  Question is how can headers be similarly set when the Lambda function is invoked from the browser directly using AWS SDK JS  API Gateway is the service which implements the headers in the previous setup  without having API Gateway in front of AWS Lambda headers are ignored and being generically set to:  AWS SDK JS browser invoke code looks something like this:  The returned response is the response object above as a string with the generic headers  Browser does not decompress the gzipped content  presumably because the content-encoding header is not being set  AWS Lambda when invoked from the browser  treats the whole Lambda response object as the response and performs no transformations that happen with API Gateway  For instance  API Gateway would pick up on the response object structure and map response object headers to the response before sending to the client  Is there no way to set AWS Lambda headers without API Gateway  Or is the only option to decompress the gzip content manually on the client using something like  (sigh)   Idea to not use API Gateway came from AWS docs found here  like to avoid the API Gateway costs:  Any guidance  expertise  thoughts are much appreciated  
55870453,1,56409180.0,,2019-04-26 15:11:08,,11,1117,"<p>I have set up a simple intent </p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>{
  ""interactionModel"": {
    ""languageModel"": {
      ""invocationName"": ""viva bank"",
      ""intents"": [
        ...builtin intents...{
          ""name"": ""ask"",
          ""slots"": [{
            ""name"": ""question"",
            ""type"": ""AMAZON.SearchQuery""
          }],
          ""samples"": [
            ""when {question}"",
            ""how to {question}"",
            ""what {question}""
          ]
        }
      ],
      ""types"": []
    }
  }
}</code></pre>
</div>
</div>
</p>

<p>But when I ask a question it gives me a generic error response like this:</p>

<p>Me: alexa ask viva bank when is the late fee charged</p>

<p>Alexa: Sorry, I don't know that.</p>

<p>Here is my lambda code, but I don't think it is getting that far.</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>'use strict';

const Alexa = require('ask-sdk-core');
var https = require('https');
var querystring = require('querystring');


const APP_ID = 'amzn1.ask.skill.1234';

const AskIntentHandler = {
  canHandle(handlerInput) {
    return !!handlerInput.requestEnvelope.request.intent.slots['question'].value;
  },
  handle(handlerInput) {
    var question = handlerInput.requestEnvelope.request.intent.slots['question'].value;
    console.log('mydata:', question);
    var responseString = '';
    const subscription_key = 'XXXX';

    var data = {
      simplequery: question,
      channel: 'Alexa'
    };
    var get_options = {
      headers: {
        'Subscription-Key': subscription_key
      }
    };

    https.get('https://fakeapi.com/' + querystring.stringify(data), get_options, (res) =&gt; {
      console.log('statusCode:', res.statusCode);
      console.log('headers:', res.headers);

      res.on('data', (d) =&gt; {
        responseString += d;
      });

      res.on('end', function(res) {
        var json_hash = JSON.parse(responseString);
        // grab the first answer returned as text and have Alexa read it
        const speechOutput = json_hash['results'][0]['content']['text'];
        console.log('==&gt; Answering: ', speechOutput);
        // speak the output
        return handlerInput.responseBuilder.speak(speechOutput).getResponse();
      });
    }).on('error', (e) =&gt; {
      console.error(e);
      return handlerInput.responseBuilder.speak(""I'm sorry I ran into an error"").getResponse();
    });
  }

};

exports.handler = (event, context) =&gt; {
  const alexa = Alexa.handler(event, context);
  alexa.APP_ID = APP_ID;
  alexa.registerHandlers(AskIntentHandler);
  alexa.execute();
};</code></pre>
</div>
</div>
</p>

<p>I'm really just looking to create a very simple pass through, where a question is asked to Alexa, and then I pipe that to an external API and have Alexa read the response.</p>
",3311489.0,,7198636.0,,2019-05-06 07:27:14,2019-06-01 18:02:28,Alexa ask a question and get response from external API,<javascript><aws-lambda><alexa-skills-kit><alexa-slot>,2,3,,,,CC BY-SA 4.0,I have set up a simple intent       But when I ask a question it gives me a generic error response like this: Me: alexa ask viva bank when is the late fee charged Alexa: Sorry  I dont know that  Here is my lambda code  but I dont think it is getting that far       Im really just looking to create a very simple pass through  where a question is asked to Alexa  and then I pipe that to an external API and have Alexa read the response  
55871395,1,55875871.0,,2019-04-26 16:11:35,,0,50,"<p>Lets say I have an app where <code>users</code> can make <code>posts</code>. I store these in a single DynamoDB table using the following design:</p>

<pre><code>+--------+--------+---------------------------+
| PK     | SK     | (Attributes)              |
+-----------------+---------------------------+
| UserId | UserId | username, profile, etc... |  &lt;-- user item
| UserId | PostId | body, timestamp, etc...   |  &lt;-- post item
+--------+--------+---------------------------+
</code></pre>

<p>When a <code>user</code> makes a <code>post</code>, my Lambda function receives the following data:</p>

<pre><code>{
  ""userId"": &lt;UserId&gt;"",
  ""body"": &lt;Body&gt;,
  etc...
}
</code></pre>

<p>My question is, should I first verify that the <code>user</code> exists before adding the <code>post</code> to the table by using <code>dynamodb.get({PK: userId, SK: userId)</code>? This would make sure there won't be any orphaned <code>posts</code>, but also the function will require both a read and write unit. </p>

<p>One idea I have is to just write the <code>post</code>, potentially allowing orphaned <code>posts</code>. Then, I could have another Lambda function that runs periodically to find and remove any orphans.</p>

<p>This is obviously a simple case, but imagine a more complex system where objects have multiple relationships. It seems it could easily get very costly to check for relationship existence in these cases.</p>
",7250252.0,,,,,2019-04-26 23:07:48,Should I validate relationships in DynamoDB?,<rest><aws-lambda><nosql><amazon-dynamodb>,1,2,,,,CC BY-SA 4.0,Lets say I have an app where  can make   I store these in a single DynamoDB table using the following design:  When a  makes a   my Lambda function receives the following data:  My question is  should I first verify that the  exists before adding the  to the table by using   This would make sure there wont be any orphaned   but also the function will require both a read and write unit   One idea I have is to just write the   potentially allowing orphaned   Then  I could have another Lambda function that runs periodically to find and remove any orphans  This is obviously a simple case  but imagine a more complex system where objects have multiple relationships  It seems it could easily get very costly to check for relationship existence in these cases  
55951606,1,55952990.0,,2019-05-02 11:36:58,,1,1331,"<p>I am trying to implement a vue.js server(-less) side rendered webshop-a-like-site via nuxt.js on AWS Lambda backed with Cloudflare.</p>

<p>I prefer Cloudflare over Cloudfront because of http/3, image optimization features, safety against attacks, brotli and some more features that Cloudflare provides out-of-the-box.</p>

<p>Unfortunately i couldn't find any ressources if anyone did this before and what to take care of to work properly.</p>

<p>Right now my setup is like:</p>

<pre><code>User -&gt; Route53 -&gt; AWS API Gateway -&gt; AWS Lambda
 -&gt; S3 (for static files)
 -&gt; another AWS Lambda for dynamic data from Elasticsearch indexes
</code></pre>

<p>I am not sure where to properly integrate Cloudflare.</p>

<p>`I found a blogposts and threads about: </p>

<ol>
<li>using Cloudflare Workers instead of AWS API Gateway 
<a href=""https://news.ycombinator.com/item?id=16747420"" rel=""nofollow noreferrer"">https://news.ycombinator.com/item?id=16747420</a></li>
<li>Creating a CNAME for Lambda provided by Cloudfront, but I am not sure if this triggers another Roundtrip to Cloudfront and additional cost? <a href=""https://forums.aws.amazon.com/thread.jspa?threadID=261297"" rel=""nofollow noreferrer"">https://forums.aws.amazon.com/thread.jspa?threadID=261297</a></li>
<li>Connecting a Subdomain to API-Gateway
<a href=""https://medium.com/@bobthomas295/combining-aws-serverless-with-cloudflare-sub-domains-338a1b7b2bd"" rel=""nofollow noreferrer"">https://medium.com/@bobthomas295/combining-aws-serverless-with-cloudflare-sub-domains-338a1b7b2bd</a> </li>
<li>Another solution could be that I build the nuxt.js directly in a Cloudflare Worker, but I am not sure of any downsides of this solution, since CPU time is very limited in Pro Plan?
`</li>
</ol>

<p>Furthermore I've read an article about the need of securing the API-Gateway against attackers by only allowing Cloudflare IPs.</p>

<p>Did anyone of you already setup Vue + Nuxt with Cloudflare ? Am open to any other suggestions or ideas.</p>

<p>Thanks a lot!
Philipp</p>
",10563210.0,,,,,2019-05-02 12:56:27,Deploying SSR Nuxt.js on AWS Lambda with Cloudflare?,<amazon-web-services><vue.js><nuxt.js><cloudflare><serverless-framework>,1,0,,,,CC BY-SA 4.0,I am trying to implement a vue js server(-less) side rendered webshop-a-like-site via nuxt js on AWS Lambda backed with Cloudflare  I prefer Cloudflare over Cloudfront because of http/3  image optimization features  safety against attacks  brotli and some more features that Cloudflare provides out-of-the-box  Unfortunately i couldnt find any ressources if anyone did this before and what to take care of to work properly  Right now my setup is like:  I am not sure where to properly integrate Cloudflare  `I found a blogposts and threads about:   using Cloudflare Workers instead of AWS API Gateway   Creating a CNAME for Lambda provided by Cloudfront  but I am not sure if this triggers another Roundtrip to Cloudfront and additional cost   Connecting a Subdomain to API-Gateway   Another solution could be that I build the nuxt js directly in a Cloudflare Worker  but I am not sure of any downsides of this solution  since CPU time is very limited in Pro Plan  `  Furthermore Ive read an article about the need of securing the API-Gateway against attackers by only allowing Cloudflare IPs  Did anyone of you already setup Vue + Nuxt with Cloudflare   Am open to any other suggestions or ideas  Thanks a lot  Philipp 
55958489,1,,,2019-05-02 18:47:17,,0,300,"<p>I am currently building a serverless application based in AWS Lambda that creates CloudFront distributions on behalf of users.  Currently, when a user calls my 'delete' operation, my API Lambda function disables the CloudFront Distribution. However, the distributions are never cleaned up and deleted because I need to wait for the disable to complete first. Given Lambda's limit of 15 minutes I can't just wait for the disable to finish deploying, and that would be cost-inefficient even if I could.</p>

<p>I realize I could have a Lambda function periodically poll my CloudFront distributions and clean them up, but I'm hoping to do this in an event-driven way so that it occurs as close to real-time as possible and I don't need to use any compute when there's nothing to delete.</p>

<p>I tried setting a CloudWatch Event to trigger on UpdateDistribution calls, but that triggers when the distribution begins to disable rather than when it finishes, so that doesn't really fix the issue where I need to wait for the deploy.</p>

<p>Any suggestions on how to accomplish this? Is it even possible?</p>
",11255696.0,,,,,2019-05-03 03:59:39,Automatically Clean Up CloudFront Distributions after Disabling,<amazon-web-services><aws-lambda><amazon-cloudfront>,2,0,,,,CC BY-SA 4.0,I am currently building a serverless application based in AWS Lambda that creates CloudFront distributions on behalf of users   Currently  when a user calls my delete operation  my API Lambda function disables the CloudFront Distribution  However  the distributions are never cleaned up and deleted because I need to wait for the disable to complete first  Given Lambdas limit of 15 minutes I cant just wait for the disable to finish deploying  and that would be cost-inefficient even if I could  I realize I could have a Lambda function periodically poll my CloudFront distributions and clean them up  but Im hoping to do this in an event-driven way so that it occurs as close to real-time as possible and I dont need to use any compute when theres nothing to delete  I tried setting a CloudWatch Event to trigger on UpdateDistribution calls  but that triggers when the distribution begins to disable rather than when it finishes  so that doesnt really fix the issue where I need to wait for the deploy  Any suggestions on how to accomplish this  Is it even possible  
56067806,1,56089957.0,,2019-05-09 21:26:19,,1,336,"<p>Let's say in my client I have a list of PKs :</p>

<pre><code>PKs = [uuid1, uuid2, uuid3, uuid4, ...]
</code></pre>

<p>And I need to get the objects that have these corresponding PKs.</p>

<p>I can think of 3 ways :</p>

<p><strong>Transaction</strong></p>

<p>Using TransactGetItems, I can fetch 10 items at a time, so I would just get each item individually untill I get them all</p>

<p><strong>Batch get Items</strong></p>

<p>Same as Transaction, but not transactional and I can fetch 25 items at a time.</p>

<p><strong>Query with filter (probably messy)</strong></p>

<p>I could instead have a GSI that has a immutable attribute as Partition key, and set the original Partition key as a attribute, and I could just chain a bunch of ""ORs"" in the condition expression.</p>

<p>For example (boto3):</p>

<pre><code>table.query(
    KeyConditionExpression=Key('gsi1_pk').eq('metadata'),
    #Bunch of ORs togheter
    FilterExpression=Attr('pk').eq('uuid1') | Attr('pk').eq('uuid2') ...
    Index='GSI1-Index1'
)
</code></pre>

<p>Now, according to the pricing page :</p>

<blockquote>
  <p>DynamoDB charges one read request unit for each strongly consistent
  read (up to 4 KB), two read request units for each transactional read,
  and one-half read request unit for each eventually consistent read</p>
</blockquote>

<p>I'm not sure what 1 read is, does it considers every object returned or every object scanned? Will each different request be 1 RCU at minimum or do they sum untill it reaches 1 RCU?</p>

<p>Which of the 3 examples above would be the cheapest following DynamoDB pricing system? Is there another way of doing this?</p>

<p>Bonus points for the calculation.</p>
",3529833.0,,3529833.0,,2019-05-10 12:29:57,2019-05-11 11:34:02,DynamoDB - Most efficient/cheap way of getting a objects from a list of partition keys?,<nosql><amazon-dynamodb><serverless><dynamodb-queries>,1,0,,,,CC BY-SA 4.0,Lets say in my client I have a list of PKs :  And I need to get the objects that have these corresponding PKs  I can think of 3 ways : Transaction Using TransactGetItems  I can fetch 10 items at a time  so I would just get each item individually untill I get them all Batch get Items Same as Transaction  but not transactional and I can fetch 25 items at a time  Query with filter (probably messy) I could instead have a GSI that has a immutable attribute as Partition key  and set the original Partition key as a attribute  and I could just chain a bunch of ORs in the condition expression  For example (boto3):  Now  according to the pricing page :  DynamoDB charges one read request unit for each strongly consistent   read (up to 4 KB)  two read request units for each transactional read    and one-half read request unit for each eventually consistent read  Im not sure what 1 read is  does it considers every object returned or every object scanned  Will each different request be 1 RCU at minimum or do they sum untill it reaches 1 RCU  Which of the 3 examples above would be the cheapest following DynamoDB pricing system  Is there another way of doing this  Bonus points for the calculation  
57054559,1,,,2019-07-16 09:52:23,,0,755,"<p>I have multiple CSV files containing data for different tables, with different file sizes varying from 1 MB to 1.5 GB. I want to process the data (replace/remove values of columns) row by row and then load the data to existing Redshift tables. This is once a day batch processing.</p>

<ol>
<li><strong>AWS Lambda</strong>:

<ul>
<li>Lambda has limitations of memory, hence I was not able to run process for  large CSV files.</li>
</ul></li>
<li><strong>EC2</strong>: I already have EC2 instance where I am running python script to transform and load the data to redshift. 

<ul>
<li>I have keep EC2 running all the time, which has all python scripts which I want to run for all tables and environment created (installing <code>python, psycopg lib</code> etc), leads to more cost.</li>
</ul></li>
<li><strong>AWS Batch</strong>: 

<ul>
<li>I created a container image which has all the setup to run the python scripts, and pushed it to ECR.</li>
<li>I then set up AWS Batch job, which can take this container image and run it through ECS.</li>
<li>This is more optimized, I only pay for EC2 used and ECR image storage.</li>
<li>But all the development and unit testing I will have to do on my personal desktop and then push a container, no inline AWS service to test.</li>
</ul></li>
<li><strong>AWS Workspaces</strong>:

<ul>
<li>I am not much familiar with AWS Workspaces, but need inputs, can this also be used as aws batch to start and stop instance when required and run python scripts on that, edit or test scripts.</li>
<li>Also, Can I schedule it to run everyday at defined time?</li>
</ul></li>
</ol>

<p>I need a inputs on which service is best suitable, optimized solution for such use-case? Or It would also be great if anyone suggests a better way to use above services I mentioned in better way.</p>
",2151118.0,,2151118.0,,2019-07-16 20:34:27,2022-01-20 09:43:02,AWS-Batch vs EC2 vs AWS Workspaces for running batch scripts to load data to Redshift,<amazon-web-services><amazon-ec2><aws-lambda><aws-batch><amazon-workspaces>,1,3,,,,CC BY-SA 4.0,I have multiple CSV files containing data for different tables  with different file sizes varying from 1 MB to 1 5 GB  I want to process the data (replace/remove values of columns) row by row and then load the data to existing Redshift tables  This is once a day batch processing   AWS Lambda:   Lambda has limitations of memory  hence I was not able to run process for  large CSV files   EC2: I already have EC2 instance where I am running python script to transform and load the data to redshift     I have keep EC2 running all the time  which has all python scripts which I want to run for all tables and environment created (installing  etc)  leads to more cost   AWS Batch:    I created a container image which has all the setup to run the python scripts  and pushed it to ECR  I then set up AWS Batch job  which can take this container image and run it through ECS  This is more optimized  I only pay for EC2 used and ECR image storage  But all the development and unit testing I will have to do on my personal desktop and then push a container  no inline AWS service to test   AWS Workspaces:   I am not much familiar with AWS Workspaces  but need inputs  can this also be used as aws batch to start and stop instance when required and run python scripts on that  edit or test scripts  Also  Can I schedule it to run everyday at defined time    I need a inputs on which service is best suitable  optimized solution for such use-case  Or It would also be great if anyone suggests a better way to use above services I mentioned in better way  
40118293,1,,,2016-10-18 21:22:02,,0,403,"<p>I have an API Gateway endpoint at some url, like this:</p>

<pre><code>https://api.myapp.com/myendpoint
</code></pre>

<p>The people and/or services that are going to be accessing this endpoint need to pass particular parameters and values to the endpoint. Like this:</p>

<pre><code>https://api.myapp.com/myendpoint?token=123456
</code></pre>

<p>Is it possible to limit access to the endpoint if the <code>token</code> parameter is missing OR if the <code>token</code> value is not a specific pre-determined value? Can I setup my endpoint to simply ignore calls that don't have the proper token?</p>

<p>I'm planning on using Lambda as the backend. Do I have to deal with this in my Lambda function? Ultimately, I'm trying to avoid unnecessary Lambda and API Gateway usage costs by random individuals making bogus calls to the endpoint. So if I can have API Gateway simply ignore these calls without spinning up Lambda that would be ideal.</p>

<p>If I am able to have API Gateway ignore these calls, do I still get billed for usage when bogus calls are made to the endpoint(s) that are missing the token?</p>

<p>The reason I'm asking is because the 3rd party service that is going to access this endpoint does not have any options for passing authentication parameters in headers or using AWS Cognito, etc. So I'm just trying to think of a simple way to limit access.</p>
",172350.0,,13070.0,,2016-10-18 22:13:50,2016-10-19 22:03:53,Limt AWS API Gateway endpoint with GET parameters,<amazon-web-services><authentication><aws-lambda><endpoint><aws-api-gateway>,1,4,,,,CC BY-SA 3.0,I have an API Gateway endpoint at some url  like this:  The people and/or services that are going to be accessing this endpoint need to pass particular parameters and values to the endpoint  Like this:  Is it possible to limit access to the endpoint if the  parameter is missing OR if the  value is not a specific pre-determined value  Can I setup my endpoint to simply ignore calls that dont have the proper token  Im planning on using Lambda as the backend  Do I have to deal with this in my Lambda function  Ultimately  Im trying to avoid unnecessary Lambda and API Gateway usage costs by random individuals making bogus calls to the endpoint  So if I can have API Gateway simply ignore these calls without spinning up Lambda that would be ideal  If I am able to have API Gateway ignore these calls  do I still get billed for usage when bogus calls are made to the endpoint(s) that are missing the token  The reason Im asking is because the 3rd party service that is going to access this endpoint does not have any options for passing authentication parameters in headers or using AWS Cognito  etc  So Im just trying to think of a simple way to limit access  
56413651,1,,,2019-06-02 09:05:49,,1,192,"<p>This is my first question so I appologize if it's not the best quality.</p>

<p>I have a use case: User creates a monitoring task which sends an http request to a website every X hours. User can have thousands of these tasks and can add/modify and delete them. When a user creates a task, django signals create a Celery periodic task which then is running periodically.</p>

<p>I'm searching for a more scalable solution using AWS. I've read about using Lambda + Cloudwatch Events. </p>

<p>My question is: how do I approach this to let my users create tens of thousands of these tasks in the cheapest / most scalable way?</p>

<p>Thank you for reading my question!
Peter</p>
",11589221.0,,174777.0,,2019-06-02 21:16:53,2019-06-02 21:16:53,What is a scalable way of creating cron jobs on Amazon Web Services?,<python><django><amazon-web-services><aws-lambda><amazon-cloudwatch>,1,2,2.0,,,CC BY-SA 4.0,This is my first question so I appologize if its not the best quality  I have a use case: User creates a monitoring task which sends an http request to a website every X hours  User can have thousands of these tasks and can add/modify and delete them  When a user creates a task  django signals create a Celery periodic task which then is running periodically  Im searching for a more scalable solution using AWS  Ive read about using Lambda + Cloudwatch Events   My question is: how do I approach this to let my users create tens of thousands of these tasks in the cheapest / most scalable way  Thank you for reading my question  Peter 
57063109,1,,,2019-07-16 18:06:32,,0,44,"<p>I've made a simple scheduler in Go that is meant to be long-running and using channels, when the ticker's channel is ready, I poll my database for emails to be sent out, which sends the email to the send channel and when it is ready, the email is sent out. For now the tick interval is every 10 minutes. This scheduler connects to the pg database once on startup.</p>

<p>Example:</p>

<pre><code>for {
    select {
     case &lt;-s.ticker.C:
      // query db, sends each email to s.send channel. Could just send them
      // without the channel though.
      go poll()
     case email := s.send:
      send(email) // sends the email via some email service
    }
}
</code></pre>

<p>My question is if would it be more safe/efficient to use this Go program which connects to the database once, or setup a Lambda function as a cronjob every 10 minutes? I've never used Lambda, but I believe there would be a new database connection every time it is run, which also involves fetching credentials.</p>

<p>Cost-wise assume that it doesn't matter, I'm only curious as to which would be more safe/efficient. Any information is appreciated!</p>
",5289157.0,,174777.0,,2019-07-17 03:49:44,2019-07-17 03:49:44,Golang creating resource-efficient emailer,<amazon-web-services><go><aws-lambda><serverless>,0,5,,,,CC BY-SA 4.0,Ive made a simple scheduler in Go that is meant to be long-running and using channels  when the tickers channel is ready  I poll my database for emails to be sent out  which sends the email to the send channel and when it is ready  the email is sent out  For now the tick interval is every 10 minutes  This scheduler connects to the pg database once on startup  Example:  My question is if would it be more safe/efficient to use this Go program which connects to the database once  or setup a Lambda function as a cronjob every 10 minutes  Ive never used Lambda  but I believe there would be a new database connection every time it is run  which also involves fetching credentials  Cost-wise assume that it doesnt matter  Im only curious as to which would be more safe/efficient  Any information is appreciated  
57070054,1,57141958.0,,2019-07-17 06:50:00,,0,232,"<p>I made some research about Containers, Serverless, and Virtual machines all of these has its own benefits like cost, deployments, reliability, etc. but I am still confused when to use these, and what kind of situations.</p>
",5327471.0,,,,,2019-07-22 08:53:15,Containers vs Serverless vs Virtual Machines,<containers><virtual-machine><serverless>,1,0,1.0,2019-07-25 12:27:17,,CC BY-SA 4.0,I made some research about Containers  Serverless  and Virtual machines all of these has its own benefits like cost  deployments  reliability  etc  but I am still confused when to use these  and what kind of situations  
57078937,1,69385757.0,,2019-07-17 15:02:19,,1,3034,"<p>I have a requirement to build a basic ""3 failed login attempts and your account gets locked"" functionality. The project uses AWS Cognito for Authentication, and the Cognito PreAuth and PostAuth triggers to run a Lambda function look like they will help here.</p>

<p>So the basic flow is to increment a counter in the PreAuth lambda, check it and block login there, or reset the counter in the PostAuth lambda (so successful logins dont end up locking the user out). Essentially it boils down to:</p>

<p><strong>PreAuth Lambda</strong></p>

<pre><code>if failed-login-count &gt; LIMIT:
    block login
else:
    increment failed-login-count
</code></pre>

<p><strong>PostAuth Lambda</strong></p>

<pre><code>reset failed-login-count to zero
</code></pre>

<p>Now at the moment I am using a dedicated DynamoDB table to store the failed-login-count for a given user. This seems to work fine for now.
Then I figured it'd be neater to use a custom attribute in Cognito (using <code>CognitoIdentityServiceProvider.adminUpdateUserAttributes</code>) so I could throw away the DynamoDB table.</p>

<p>However reading <a href=""https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-dg.pdf"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-dg.pdf</a> the section titled ""Configuring User Pool Attributes"" states:</p>

<blockquote>
  <p>Attributes are pieces of information that help you identify individual users, such as name, email, and phone number. Not all information about your users should be stored in attributes. For example, user data that changes frequently, such as usage statistics or game scores, should be kept in a separate data store, such as Amazon Cognito Sync or Amazon DynamoDB.</p>
</blockquote>

<p>Given that the counter will change on every single login attempt, the docs would seem to indicate I shouldn't do this...</p>

<p>But can anyone tell me why? Or if there would be some negative consequence of doing so?
As far as I can see, Cognito billing is purely based on storage (i.e. number of users), and not operations, whereas Dynamo charges for read/write/storage.
Could it simply be AWS not wanting people to abuse Cognito as a storage mechanism? Or am I being daft?</p>
",293956.0,,,,,2021-09-30 03:31:54,Should I store failed login attempts in AWS Cognito or Dynamo DB?,<amazon-web-services><aws-lambda><amazon-cognito>,3,0,,,,CC BY-SA 4.0,I have a requirement to build a basic 3 failed login attempts and your account gets locked functionality  The project uses AWS Cognito for Authentication  and the Cognito PreAuth and PostAuth triggers to run a Lambda function look like they will help here  So the basic flow is to increment a counter in the PreAuth lambda  check it and block login there  or reset the counter in the PostAuth lambda (so successful logins dont end up locking the user out)  Essentially it boils down to: PreAuth Lambda  PostAuth Lambda  Now at the moment I am using a dedicated DynamoDB table to store the failed-login-count for a given user  This seems to work fine for now  Then I figured itd be neater to use a custom attribute in Cognito (using ) so I could throw away the DynamoDB table  However reading  the section titled Configuring User Pool Attributes states:  Attributes are pieces of information that help you identify individual users  such as name  email  and phone number  Not all information about your users should be stored in attributes  For example  user data that changes frequently  such as usage statistics or game scores  should be kept in a separate data store  such as Amazon Cognito Sync or Amazon DynamoDB   Given that the counter will change on every single login attempt  the docs would seem to indicate I shouldnt do this    But can anyone tell me why  Or if there would be some negative consequence of doing so  As far as I can see  Cognito billing is purely based on storage (i e  number of users)  and not operations  whereas Dynamo charges for read/write/storage  Could it simply be AWS not wanting people to abuse Cognito as a storage mechanism  Or am I being daft  
56672041,1,56672147.0,,2019-06-19 16:16:46,,2,378,"<p>I am writing an Azure Durable Function to process various bulk operations. Te code can get 1000 operations in a file and it's breaking those down to call the same activity function 1000 times.</p>

<p>The problem is that this can flood an API that the activity function uses up to the point that our activity function gets a 429 - Too Many Requests from the API. We are thinking of reading the Retry-After header offered and putting the thread to sleep for that period of time. </p>

<p>In this case, we're wondering if Azure will bill us for the seconds we're waiting for. Also, would this time count towards the timeout for the Azure function?</p>
",7581591.0,,,,,2019-06-19 16:25:18,Is an Azure Function on a Consumption plan billed for time spent waiting for Thread.Sleep()?,<azure><azure-functions><serverless>,1,0,,,,CC BY-SA 4.0,I am writing an Azure Durable Function to process various bulk operations  Te code can get 1000 operations in a file and its breaking those down to call the same activity function 1000 times  The problem is that this can flood an API that the activity function uses up to the point that our activity function gets a 429 - Too Many Requests from the API  We are thinking of reading the Retry-After header offered and putting the thread to sleep for that period of time   In this case  were wondering if Azure will bill us for the seconds were waiting for  Also  would this time count towards the timeout for the Azure function  
57083043,1,57084352.0,,2019-07-17 19:37:59,,0,1211,"<p>I am trying to get the cost of the previous day using cost explorer while using <code>boto</code> and <code>lambda</code>. But I get and error </p>

<blockquote>
  <p>"" ""errorMessage"": ""An error occurred (ValidationException) when
  calling the GetCostAndUsage operation: "" and the error type is
  ""ClientError"".</p>
</blockquote>

<p>I have specified the region to <code>us-east-1</code>. Also my policy is </p>

<pre><code>{
    ""Version"": ""2012-10-17"",
    ""Statement"": [
        {
            ""Effect"": ""Allow"",
            ""Action"": [
                ""ce:*""
            ],
            ""Resource"": [
                ""*""
            ]
        }
    ]
}
</code></pre>

<p>My Code is below </p>

<pre><code>ce = boto3.client('ce')

cost = ce.get_cost_and_usage(TimePeriod={'Start': '2019-7-15', 'End': '2019-7-17'}, Granularity = 'DAILY')

print(cost)
</code></pre>
",11799458.0,,11799458.0,,2019-07-17 19:43:01,2019-07-17 21:37:33,Getting error while using get_cost_and_usage in lambda with boto,<python><amazon-web-services><aws-lambda><boto3>,1,0,,,,CC BY-SA 4.0,I am trying to get the cost of the previous day using cost explorer while using  and   But I get and error    errorMessage: An error occurred (ValidationException) when   calling the GetCostAndUsage operation:  and the error type is   ClientError   I have specified the region to   Also my policy is   My Code is below   
56687133,1,,,2019-06-20 13:31:37,,0,301,"<p>i use AWS Api Gateway and a Lambda function to process Paypal (Ipn) instant notification messages. how can i secure my api gateway to only allow access to the message sent by Paypal which are then passed on to my lambda function. at the moment the api is open to anyone to access and i am afraid of malicious activity triggering the api and lambda function and thus incurring costs on my behalf.</p>

<p>i have secured my other Apis using Cognito and the associated lambda functions using roles and permission policies but don't know how to handle the calls on my ipn api as these will be unauthorized.</p>
",4391398.0,,,,,2019-06-21 15:48:22,how to secure AWS Api Gateway and Lambda for Paypal Ipn messages,<amazon-web-services><api><aws-lambda><aws-api-gateway><paypal-ipn>,1,0,,,,CC BY-SA 4.0,i use AWS Api Gateway and a Lambda function to process Paypal (Ipn) instant notification messages  how can i secure my api gateway to only allow access to the message sent by Paypal which are then passed on to my lambda function  at the moment the api is open to anyone to access and i am afraid of malicious activity triggering the api and lambda function and thus incurring costs on my behalf  i have secured my other Apis using Cognito and the associated lambda functions using roles and permission policies but dont know how to handle the calls on my ipn api as these will be unauthorized  
56274163,1,,,2019-05-23 11:25:02,,0,890,"<p>I am currently using AWS EC2 for my workloads. </p>

<p>Now I want to convert the EC2 server to the Serverless Platform i.e(API Gateway and Lambda). </p>

<p>I have also followed different blogs and I am ready to go with the serverless. But, my one concern is on pricing. </p>

<p>How can I predict per month cost for the serverless according to my use of EC2? Will the EC2 Cloudwatch metrics help me to calculate the costing? </p>

<p>How can I make cost comparison? </p>
",9923849.0,,174777.0,,2019-05-24 00:27:29,2019-06-02 10:18:25,AWS EC2 vs Serverless Cost Comparison,<amazon-web-services><amazon-ec2><aws-lambda>,1,1,,,,CC BY-SA 4.0,I am currently using AWS EC2 for my workloads   Now I want to convert the EC2 server to the Serverless Platform i e(API Gateway and Lambda)   I have also followed different blogs and I am ready to go with the serverless  But  my one concern is on pricing   How can I predict per month cost for the serverless according to my use of EC2  Will the EC2 Cloudwatch metrics help me to calculate the costing   How can I make cost comparison   
39590227,1,39605751.0,,2016-09-20 09:19:52,,0,729,"<p>So I have created a lambda function (the purpose of which doesn't matter anymore) and have tested that it works when run from my laptop.  However the bigger problem is that I cannot get it to run off of a test event or on a schedule in AWS. </p>

<p>When I try to run it from AWS I get a 300s timeout error. </p>

<p>The following is included for your consideration:</p>

<ul>
<li>Function</li>
<li>Logs</li>
<li>Trigger Event</li>
<li>Policy</li>
<li>VPC Related Configuration</li>
</ul>

<p>If anyone can tell me what the issue might be, I would appreciate it as I have been searching for the solution for about 3 days.</p>

<p>FUNCTION:</p>

<pre><code>from __future__ import print_function

def lambda_handler(event, context):
    if event[""account""] == ""123456789012"":
        print(""Passed!!"")
    return event['time']


import boto3
import datetime

def find_auto_scaling_instances():
    """"""
    Find Auto-scaling Instances
    """"""
    client=boto3.client(""autoscaling"")
    auto_scaling=client.describe_auto_scaling_groups()
    dict={}
    for group in auto_scaling[""AutoScalingGroups""]:
        print(""hello"")
        auto_scaling_group_name=group[""AutoScalingGroupName""]
        number_of_instances=len(group[""Instances""])
        if number_of_instances == 0:
            continue
        else:
            for i in range(number_of_instances):
                if group[""Instances""][i][""LifecycleState""] == ""InService"":
                    instance_id=group[""Instances""][i][""InstanceId""]
                    dict[auto_scaling_group_name]=instance_id
                    break
    return dict

def find_staging_instances():
    """"""
    Find Static Instances
    """"""
    stg_list=[]
    tag_list=[""Y-qunie-stepsrv-1a"",""S-StepSvr""]
    for i in range(num_of_instances):
        print(""hello2"")
        target=instances[i][""Instances""][0]
        number_of_tags=len(target[""Tags""])
        for tag in range(number_of_tags):
            if target[""Tags""][tag][""Value""] in tag_list:
                stg_list+=[target[""InstanceId""]]

    return stg_list

def volumes_per_instance():
    """"""
    Find the EBS associated with the Instances
    """"""
    instance_disk={}
    for i in range(num_of_instances):
        print(""hello3"")
        target=instances[i][""Instances""][0]
        if target[""InstanceId""] in instance_list:
            instance_disk[target[""InstanceId""]]=[]
            for disk in range(len(target[""BlockDeviceMappings""])):
                print(""hello4"")
                instance_disk[target[""InstanceId""]]+=\
                target[""BlockDeviceMappings""][disk][""Ebs""][""VolumeId""]]

    return instance_disk

#Group instances together and prepare to process
instance_in_asgroup_dict=find_auto_scaling_instances()

as_instance_list=[]
for group in instance_in_asgroup_dict:
    print(""hello5"")
    as_instance_list+=[instance_in_asgroup_dict[group]]

client=boto3.client(""ec2"")
instances=client.describe_instances()[""Reservations""]
num_of_instances=len(instances)

staging_instances=find_staging_instances()

instance_list=[]
instance_list+=as_instance_list
instance_list+=staging_instances

#Gather Disk Information
inst_disk_set=volumes_per_instance()

date=str(datetime.datetime.now()).replace("" \
"",""_"").replace("":"","""").replace(""."","""")

#Create Disk Images
as_image={}
image=[]
for instance in instance_list:
    print(""hello6"")
    if instance in as_instance_list:
        as_image[instance]=client.create_image(
            InstanceId=instance,
            Name=instance+""-""+date+""-AMI"",
            Description=""AMI for instance ""+instance+""."",
            NoReboot=True
        )
    else:
        image+=[client.create_image(
            InstanceId=instance,
            Name=instance+""-""+date+""-AMI"",
            Description=""AMI for instance ""+instance+""."",
            NoReboot=True
        )]
</code></pre>

<p>LOGS:</p>

<pre><code>18:03:30
START RequestId: 0ca9e0a3-7f11-11e6-be11-6974d9213102 Version: $LATEST

18:08:30
END RequestId: 0ca9e0a3-7f11-11e6-be11-6974d9213102

18:08:30
REPORT RequestId: 0ca9e0a3-7f11-11e6-be11-6974d9213102  Duration: 300001.99 ms  Billed Duration: 300000 ms Memory Size: 128 MB  Max Memory Used: 24 MB

18:08:30
2016-09-20T09:08:30.544Z 0ca9e0a3-7f11-11e6-be11-6974d9213102 Task timed out after 300.00 seconds
</code></pre>

<p>TRIGGER_EVENT:</p>

<pre><code>{
  ""account"": ""123456789012"",
  ""region"": ""us-east-1"",
  ""detail"": {},
  ""detail-type"": ""Scheduled Event"",
  ""source"": ""aws.events"",
  ""time"": ""1970-01-01T00:00:00Z"",
  ""id"": ""cdc73f9d-aea9-11e3-9d5a-835b769c0d9c"",
  ""resources"": [
    ""arn:aws:events:us-east-1:123456789012:rule/my-schedule""
  ]
}
</code></pre>

<p><strong>EDIT-1</strong></p>

<p>IAM POLICY:</p>

<p>From my understanding all I need to allow VPC Access to my function is to add the following privilages to the lambda function's assigned policy. </p>

<ul>
<li>ec2:CreateNetworkInterface</li>
<li>ec2:DeleteNetworkInterface</li>
<li>ec2:DescribeNetworkInterfaces</li>
</ul>

<blockquote>
<pre><code>{
    ""Version"": ""2012-10-17"",
    ""Statement"": [
        {
            ""Effect"": ""Allow"",
            ""Action"": [
                ""logs:CreateLogGroup"",
                ""logs:CreateLogStream"",
                ""logs:PutLogEvents"",
                ""ec2:CreateNetworkInterface"",
                ""ec2:DescribeNetworkInterfaces"",
                ""ec2:DeleteNetworkInterface""
            ],
            ""Resource"": ""*""
        },
        {
            ""Action"": [
                ""ec2:DescribeInstances"",
                ""ec2:CreateImage"",
                ""autoscaling:DescribeAutoScalingGroups""
            ],
            ""Effect"": ""Allow"",
            ""Resource"": ""*""
        },
        {
            ""Effect"": ""Allow"",
            ""Action"": [
                ""logs:CreateLogGroup"",
                ""logs:CreateLogStream"",
                ""logs:PutLogEvents""
            ],
            ""Resource"": ""arn:aws:logs:*:*:*""
        }
    ] }
</code></pre>
</blockquote>

<p><strong>EDIT 2</strong></p>

<p>CONFIGURATION:</p>

<p>The subnets, security groups and VPC attached to the function.</p>

<p><a href=""https://i.stack.imgur.com/HwQfN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HwQfN.png"" alt=""Configurations""></a></p>

<p><strong>EDIT 3 (CONCLUSION)</strong></p>

<p>Mark Gave an excellent answer, informing me that I had set my function up to run inside a VPC yet I was not accessing resources within the VPC.  Rather, I was accessing the Amazon API endpoint which required that I have access to the internet or the transaction would timeout.</p>

<p>As such, there were two options available to fix this situation. </p>

<ul>
<li>Remove my VPC settings</li>
<li>Create a NAT Gateway inside my VPC</li>
</ul>

<p>I chose the one that costs the least money.</p>
",3699468.0,,3699468.0,,2016-09-21 01:56:45,2016-09-21 01:56:45,AWS Lambda function using test trigger or schedule (Timeout),<python><amazon-web-services><aws-lambda><boto3>,1,4,,,,CC BY-SA 3.0,So I have created a lambda function (the purpose of which doesnt matter anymore) and have tested that it works when run from my laptop   However the bigger problem is that I cannot get it to run off of a test event or on a schedule in AWS   When I try to run it from AWS I get a 300s timeout error   The following is included for your consideration:  Function Logs Trigger Event Policy VPC Related Configuration  If anyone can tell me what the issue might be  I would appreciate it as I have been searching for the solution for about 3 days  FUNCTION:  LOGS:  TRIGGER_EVENT:  EDIT-1 IAM POLICY: From my understanding all I need to allow VPC Access to my function is to add the following privilages to the lambda functions assigned policy    ec2:CreateNetworkInterface ec2:DeleteNetworkInterface ec2:DescribeNetworkInterfaces     EDIT 2 CONFIGURATION: The subnets  security groups and VPC attached to the function   EDIT 3 (CONCLUSION) Mark Gave an excellent answer  informing me that I had set my function up to run inside a VPC yet I was not accessing resources within the VPC   Rather  I was accessing the Amazon API endpoint which required that I have access to the internet or the transaction would timeout  As such  there were two options available to fix this situation    Remove my VPC settings Create a NAT Gateway inside my VPC  I chose the one that costs the least money  
56402673,1,,,2019-05-31 23:24:10,,4,2288,"<p>I want to have a lambda function to run batch payment jobs.</p>

<p>When the user send the batch job. I want the user to see the progress of batch payment. So I want the Lambda function to send the messages back to the client. The user needs to see what payment was successful too.</p>

<p>I expect the lambda function will take around 3-5 minutes to run.</p>

<p>What should I use to for Lambda to communicate with the client side code? Sockets? The client side is written in Vuejs.</p>

<p>Thanks</p>
",2564566.0,,174777.0,,2019-05-31 23:50:03,2019-05-31 23:53:51,Aws Lambda display progress,<amazon-web-services><vue.js><aws-lambda>,1,0,,,,CC BY-SA 4.0,I want to have a lambda function to run batch payment jobs  When the user send the batch job  I want the user to see the progress of batch payment  So I want the Lambda function to send the messages back to the client  The user needs to see what payment was successful too  I expect the lambda function will take around 3-5 minutes to run  What should I use to for Lambda to communicate with the client side code  Sockets  The client side is written in Vuejs  Thanks 
56520042,1,,,2019-06-10 03:07:17,,-1,34,"<p>I am building an app that allows people to share items with other people in the community. I wanted to use AWS as my platform.</p>

<p>My idea was to use react Native for the app. AWS Cognito for the authentication. AWS lambda for the server calls. Relational database for storing data about the items and user data such as geolocation. Dynamodb for real-time chat, requests for borrowing and transaction data between users. My primary focus is low cost and I was thinking of using PostgresSQL for relational database.</p>

<p>What do you guys think of my database choices. Of course the PostgresSQL database on rds. Is there a flaw in database plan so far? Any help would be greatly appreciated.</p>
",6011355.0,,,,,2019-06-10 12:46:56,AWS platform. Picking the right technologies,<postgresql><react-native><aws-lambda><amazon-dynamodb><amazon-cognito>,1,0,,,,CC BY-SA 4.0,I am building an app that allows people to share items with other people in the community  I wanted to use AWS as my platform  My idea was to use react Native for the app  AWS Cognito for the authentication  AWS lambda for the server calls  Relational database for storing data about the items and user data such as geolocation  Dynamodb for real-time chat  requests for borrowing and transaction data between users  My primary focus is low cost and I was thinking of using PostgresSQL for relational database  What do you guys think of my database choices  Of course the PostgresSQL database on rds  Is there a flaw in database plan so far  Any help would be greatly appreciated  
56530171,1,56534349.0,,2019-06-10 16:29:43,,1,64,"<p>I have a key that is being shared among different services and it is currently stored in an s3 bucket inside a text file. 
My goal is to read that variable and pass it to my lambda service through cloudformation. 
for an ec2 instance it was easy because I could download the file and read it, and that was easily achievable by putting the scripts inside my cloudformation json file. But I don't have any idea how to do it for my lambdas....! </p>

<p>I tried to put my credentials in gitlab pipeline but because of the access permissions it doesn't let gitlab pass it on, so my best and least expensive option right now is to do it in cloud formation.</p>
",9188930.0,,174777.0,,2019-06-10 22:40:48,2019-06-10 22:40:48,How to import a text from s3 to lambda through cloudformation?,<amazon-web-services><amazon-s3><aws-lambda><amazon-cloudformation>,1,0,,,,CC BY-SA 4.0,I have a key that is being shared among different services and it is currently stored in an s3 bucket inside a text file   My goal is to read that variable and pass it to my lambda service through cloudformation   for an ec2 instance it was easy because I could download the file and read it  and that was easily achievable by putting the scripts inside my cloudformation json file  But I dont have any idea how to do it for my lambdas       I tried to put my credentials in gitlab pipeline but because of the access permissions it doesnt let gitlab pass it on  so my best and least expensive option right now is to do it in cloud formation  
57043464,1,,,2019-07-15 16:03:12,,2,228,"<p>My goal is to store data received through an SNS subscription in Amazon Redshift. I have successfully managed to store the data using a lambda function, simply through creating a Redshift connection and calling a prepared insert statement. While the lambda does reuse the connection, I use connection pooling to ensure that I don't lose the connection and end up spending 5-10 seconds reconnecting.</p>

<p>This process has worked perfectly well for smaller scale tests, and I don't come close to using all of my lambda concurrency. But, I'm afraid that this flow may cause problems when I begin to subscribe to a considerably more active SNS (up to 100 records/sec).</p>

<p>While I know the aws documentation and many online forums recommend firehose because of its scalability, one of my number one priorities is cost reduction, and firehose seems extremely costly (the price of firehose + cost of intermediate s3 bucket). Does anyone have any experience with trying to bypass firehose by programmatically storing in Redshift/other databases?</p>
",9801628.0,,,,,2021-01-13 08:14:32,Is Amazon Kinesis Firehose necessary to scale?,<aws-lambda><amazon-redshift><amazon-kinesis><amazon-kinesis-firehose>,1,10,,,,CC BY-SA 4.0,My goal is to store data received through an SNS subscription in Amazon Redshift  I have successfully managed to store the data using a lambda function  simply through creating a Redshift connection and calling a prepared insert statement  While the lambda does reuse the connection  I use connection pooling to ensure that I dont lose the connection and end up spending 5-10 seconds reconnecting  This process has worked perfectly well for smaller scale tests  and I dont come close to using all of my lambda concurrency  But  Im afraid that this flow may cause problems when I begin to subscribe to a considerably more active SNS (up to 100 records/sec)  While I know the aws documentation and many online forums recommend firehose because of its scalability  one of my number one priorities is cost reduction  and firehose seems extremely costly (the price of firehose + cost of intermediate s3 bucket)  Does anyone have any experience with trying to bypass firehose by programmatically storing in Redshift/other databases  
57591791,1,,,2019-08-21 12:36:37,,-1,80,"<p>I have a function that writes the cost of the account in a CSV. Now I would like to use assumed role to query all other accounts. Unfortunately he only writes the costs of the account where the Lambda function is located. What do I do wrong to write all further costs into the CSV file? Here is the code as I thought it would be. I left the function of the CSV out of the picture, because it works perfectly.</p>

<pre><code>import boto3
import re
import csv
import os

def lambda_handler(event,context):

    #get data from environment
    start = os.environ['Start']
    end = os.environ['End']

    response = client.get_cost_and_usage(
        TimePeriod={
            'Start': start,
            'End': end
        },
        Granularity='MONTHLY',
        Metrics=['BlendedCost'],
        GroupBy=[
            {
                'Type': 'TAG',
                'Key': 'Project











</code></pre>
",11793317.0,,11793317.0,,2019-08-21 16:57:17,2019-08-21 16:57:17,how do i get the information over api of the assumed role?,<python-3.x><amazon-s3><aws-lambda><aws-sdk>,1,0,,,,CC BY-SA 4.0,I have a function that writes the cost of the account in a CSV  Now I would like to use assumed role to query all other accounts  Unfortunately he only writes the costs of the account where the Lambda function is located  What do I do wrong to write all further costs into the CSV file  Here is the code as I thought it would be  I left the function of the CSV out of the picture  because it works perfectly   
56923955,1,56924125.0,,2019-07-07 16:17:50,,1,3984,"<p>I have a Python 3.6 AWS Lambda Function I am building out to query Cost Explorer, and a few other services. I want to write out the string response I am returning into a JSON object I can either upload to S3 or into DynamoDB.</p>

<p>A working example of the Function is below</p>

<pre class=""lang-py prettyprint-override""><code>import boto3

def handler(event,context):
    client = boto3.client('ce')
    response = client.get_cost_forecast(
    TimePeriod={
        'Start': '2019-06-01', ## Start must be one day after creation date of function
        'End': '2020-07-01'
    },
    Metric='UNBLENDED_COST',
    Granularity='MONTHLY',
    PredictionIntervalLevel=90 ## 51 - 99 Range
    )
    return str (response)
</code></pre>

<p>That returns just fine in the console, now I tried this to put the Response into the Body of an object I wanted to write into S3</p>

<pre class=""lang-py prettyprint-override""><code>import boto3

def handler(event,context):
    client = boto3.client('ce')
    s3 = boto3.resource('s3')
    object = s3.Object('my-bucket', 'my/path/object.json')
    response = client.get_cost_forecast(
    ---ce python stuff---
    object.put(Body=response)
    return str (response)
</code></pre>

<p>That is not working as expected, and I am getting this error from CloudWatch</p>

<pre><code>Parameter validation failed:
Invalid type for parameter Body, value: {'Total': {'Amount': '...RESPONSE...'}, 'RetryAttempts': 0}}, type: &lt;class 'dict'&gt;, valid types: &lt;class 'bytes'&gt;, &lt;class 'bytearray'&gt;, file-like object: ParamValidationError
Traceback (most recent call last):
File ""/var/task/lambda_function.py"", line 17, in handler
object.put(Body=response)
File ""/var/task/boto3/resources/factory.py"", line 520, in do_action
response = action(self, *args, **kwargs)
File ""/var/task/boto3/resources/action.py"", line 83, in __call__
response = getattr(parent.meta.client, operation_name)(**params)
File ""/var/task/botocore/client.py"", line 357, in _api_call
return self._make_api_call(operation_name, kwargs)
File ""/var/task/botocore/client.py"", line 634, in _make_api_call
api_params, operation_model, context=request_context)
File ""/var/task/botocore/client.py"", line 682, in _convert_to_request_dict
api_params, operation_model)
File ""/var/task/botocore/validate.py"", line 297, in serialize_to_request
raise ParamValidationError(report=report.generate_report())
botocore.exceptions.ParamValidationError: Parameter validation failed:
Invalid type for parameter Body, value: {'Total': {'Amount': '...RESPONSE...'}, 'RetryAttempts': 0}}, type: &lt;class 'dict'&gt;, valid types: &lt;class 'bytes'&gt;, &lt;class 'bytearray'&gt;, file-like object
</code></pre>

<p>I thought I could just stuff that into the object and write it out. Is there a better way to do what I am trying to do?</p>
",11721362.0,,,,,2019-07-07 16:40:34,Write out Boto3 Response in JSON Object and Upload to S3 in AWS Lambda Function,<python><json><amazon-s3><aws-lambda><boto3>,1,0,,,,CC BY-SA 4.0,I have a Python 3 6 AWS Lambda Function I am building out to query Cost Explorer  and a few other services  I want to write out the string response I am returning into a JSON object I can either upload to S3 or into DynamoDB  A working example of the Function is below  That returns just fine in the console  now I tried this to put the Response into the Body of an object I wanted to write into S3  That is not working as expected  and I am getting this error from CloudWatch  I thought I could just stuff that into the object and write it out  Is there a better way to do what I am trying to do  
56812833,1,,,2019-06-28 20:21:55,,2,141,"<p>I am creating a lambda with c# several different ways (serverless, lambda, with/without tests) and I end up with a lot of items in the publishing artifacts .zip file that really don't need to be there. If I'm deploying the Lambda and there is a test project in the solution, I right click on the <code>project</code> -> <code>Publish to AWS</code>, follow the prompts and it ends up zipping the <code>xunit</code> libraries along with many localized versions of <code>Microsoft.TestPlatform</code> and <code>Microsoft.VisualStudio.TestPlatform</code> artifacts. The output will look similar to the following:</p>

<pre><code>... zipping: xunit.abstractions.dll
... zipping: xunit.assert.dll
... zipping: xunit.core.dll
... zipping: xunit.execution.dotnet.dll
... zipping: xunit.runner.reporters.netcoreapp10.dll
... zipping: xunit.runner.utility.netcoreapp10.dll
... zipping: xunit.runner.visualstudio.dotnetcore.testadapter.dll
... zipping: cs/Microsoft.TestPlatform.CommunicationUtilities.resources.dll
... zipping: cs/Microsoft.TestPlatform.CoreUtilities.resources.dll
... zipping: cs/Microsoft.TestPlatform.CrossPlatEngine.resources.dll
... zipping: cs/Microsoft.VisualStudio.TestPlatform.Common.resources.dll
... zipping: cs/Microsoft.VisualStudio.TestPlatform.ObjectModel.resources.dll
... zipping: de/Microsoft.TestPlatform.CommunicationUtilities.resources.dll
... zipping: de/Microsoft.TestPlatform.CoreUtilities.resources.dll
... zipping: de/Microsoft.TestPlatform.CrossPlatEngine.resources.dll
... zipping: de/Microsoft.VisualStudio.TestPlatform.Common.resources.dll
... zipping: de/Microsoft.VisualStudio.TestPlatform.ObjectModel.resources.dll
... zipping: es/Microsoft.TestPlatform.CommunicationUtilities.resources.dll
... zipping: es/Microsoft.TestPlatform.CoreUtilities.resources.dll
... zipping: es/Microsoft.TestPlatform.CrossPlatEngine.resources.dll
... zipping: es/Microsoft.VisualStudio.TestPlatform.Common.resources.dll
... zipping: es/Microsoft.VisualStudio.TestPlatform.ObjectModel.resources.dll
... zipping: fr/Microsoft.TestPlatform.CommunicationUtilities.resources.dll
... zipping: fr/Microsoft.TestPlatform.CoreUtilities.resources.dll
... zipping: fr/Microsoft.TestPlatform.CrossPlatEngine.resources.dll
... zipping: fr/Microsoft.VisualStudio.TestPlatform.Common.resources.dll
... zipping: fr/Microsoft.VisualStudio.TestPlatform.ObjectModel.resources.dll
... zipping: it/Microsoft.TestPlatform.CommunicationUtilities.resources.dll
... zipping: it/Microsoft.TestPlatform.CoreUtilities.resources.dll
... zipping: it/Microsoft.TestPlatform.CrossPlatEngine.resources.dll
... zipping: it/Microsoft.VisualStudio.TestPlatform.Common.resources.dll
... zipping: it/Microsoft.VisualStudio.TestPlatform.ObjectModel.resources.dll
... zipping: ja/Microsoft.TestPlatform.CommunicationUtilities.resources.dll
... zipping: ja/Microsoft.TestPlatform.CoreUtilities.resources.dll
... zipping: ja/Microsoft.TestPlatform.CrossPlatEngine.resources.dll
... zipping: ja/Microsoft.VisualStudio.TestPlatform.Common.resources.dll
... zipping: ja/Microsoft.VisualStudio.TestPlatform.ObjectModel.resources.dll
... zipping: ko/Microsoft.TestPlatform.CommunicationUtilities.resources.dll
... zipping: ko/Microsoft.TestPlatform.CoreUtilities.resources.dll
... zipping: ko/Microsoft.TestPlatform.CrossPlatEngine.resources.dll
... zipping: ko/Microsoft.VisualStudio.TestPlatform.Common.resources.dll
... zipping: ko/Microsoft.VisualStudio.TestPlatform.ObjectModel.resources.dll
... zipping: pl/Microsoft.TestPlatform.CommunicationUtilities.resources.dll
... zipping: pl/Microsoft.TestPlatform.CoreUtilities.resources.dll
... zipping: pl/Microsoft.TestPlatform.CrossPlatEngine.resources.dll
... zipping: pl/Microsoft.VisualStudio.TestPlatform.Common.resources.dll
... zipping: pl/Microsoft.VisualStudio.TestPlatform.ObjectModel.resources.dll
... zipping: pt-BR/Microsoft.TestPlatform.CommunicationUtilities.resources.dll
... zipping: pt-BR/Microsoft.TestPlatform.CoreUtilities.resources.dll
... zipping: pt-BR/Microsoft.TestPlatform.CrossPlatEngine.resources.dll
... zipping: pt-BR/Microsoft.VisualStudio.TestPlatform.Common.resources.dll
... zipping: pt-BR/Microsoft.VisualStudio.TestPlatform.ObjectModel.resources.dll
... zipping: ru/Microsoft.TestPlatform.CommunicationUtilities.resources.dll
... zipping: ru/Microsoft.TestPlatform.CoreUtilities.resources.dll
... zipping: ru/Microsoft.TestPlatform.CrossPlatEngine.resources.dll
... zipping: ru/Microsoft.VisualStudio.TestPlatform.Common.resources.dll
... zipping: ru/Microsoft.VisualStudio.TestPlatform.ObjectModel.resources.dll
... zipping: tr/Microsoft.TestPlatform.CommunicationUtilities.resources.dll
... zipping: tr/Microsoft.TestPlatform.CoreUtilities.resources.dll
... zipping: tr/Microsoft.TestPlatform.CrossPlatEngine.resources.dll
... zipping: tr/Microsoft.VisualStudio.TestPlatform.Common.resources.dll
... zipping: tr/Microsoft.VisualStudio.TestPlatform.ObjectModel.resources.dll
... zipping: zh-Hans/Microsoft.TestPlatform.CommunicationUtilities.resources.dll
... zipping: zh-Hans/Microsoft.TestPlatform.CoreUtilities.resources.dll
... zipping: zh-Hans/Microsoft.TestPlatform.CrossPlatEngine.resources.dll
... zipping: zh-Hans/Microsoft.VisualStudio.TestPlatform.Common.resources.dll
... zipping: zh-Hans/Microsoft.VisualStudio.TestPlatform.ObjectModel.resources.dll
... zipping: zh-Hant/Microsoft.TestPlatform.CommunicationUtilities.resources.dll
... zipping: zh-Hant/Microsoft.TestPlatform.CoreUtilities.resources.dll
... zipping: zh-Hant/Microsoft.TestPlatform.CrossPlatEngine.resources.dll
... zipping: zh-Hant/Microsoft.VisualStudio.TestPlatform.Common.resources.dll
... zipping: zh-Hant/Microsoft.VisualStudio.TestPlatform.ObjectModel.resources.dll
</code></pre>

<ol>
<li>Is there a way to limit what files go into the zip folder that gets sent to AWS?  </li>
<li>Is there a way to exclude localized libraries? </li>
<li>Can I exclude other projects that are not dependencies? </li>
<li>Can I keep all my projects to be in the same solution and be able to publish using the built in mechanisms without sending extraneous projects?</li>
</ol>

<p>The problem I have is that it inflates the size of the deploy package and the files are never used, so it impacts my s3 storage costs, causes slower lambda cold start times, and inefficiencies .</p>

<p>Searching the internet, <a href=""https://docs.microsoft.com/en-us/aspnet/web-forms/overview/deployment/advanced-enterprise-web-deployment/excluding-files-and-folders-from-deployment"" rel=""nofollow noreferrer"">Excluding Files and Folders from Deployment</a> looked promising, but it didn't make any difference for me (perhaps I did something wrong?).</p>
",1558083.0,,,,,2019-07-10 22:42:55,How do you limit or control publishing artifacts for the AWS Visual Studio Toolkit?,<aws-lambda><publish><visual-studio-2019><aws-visual-studio-toolkit>,1,1,,,,CC BY-SA 4.0,I am creating a lambda with c# several different ways (serverless  lambda  with/without tests) and I end up with a lot of items in the publishing artifacts  zip file that really dont need to be there  If Im deploying the Lambda and there is a test project in the solution  I right click on the  -&gt;   follow the prompts and it ends up zipping the  libraries along with many localized versions of  and  artifacts  The output will look similar to the following:   Is there a way to limit what files go into the zip folder that gets sent to AWS    Is there a way to exclude localized libraries   Can I exclude other projects that are not dependencies   Can I keep all my projects to be in the same solution and be able to publish using the built in mechanisms without sending extraneous projects   The problem I have is that it inflates the size of the deploy package and the files are never used  so it impacts my s3 storage costs  causes slower lambda cold start times  and inefficiencies   Searching the internet   looked promising  but it didnt make any difference for me (perhaps I did something wrong )  
56926793,1,,,2019-07-07 23:08:15,,0,955,"<p>I am writing a Lambda Function in Python 3.6 to query out specific conditions from the Cost Explorer API, it will eventually be invoked by API Gateway so I want to be able to send back a pared down response, as well as take that same response and persist it into S3.</p>

<p>I have the overall functionality working correctly, I was hoping to shortcut parsing and just crawl the response, but that was not working with Glue and Athena. The basic functioning code is below:</p>

<pre class=""lang-py prettyprint-override""><code>import boto3
import json

def handler(event,context):
    client = boto3.client('ce')
    s3 = boto3.resource('s3')
    object = s3.Object('s3-bucket', 'path/object.json')
    response = client.get_cost_and_usage(
    TimePeriod={
        'Start': '2019-01-01',
        'End': '2019-07-01'
    },
    Granularity='MONTHLY',
    Metrics=[
        'UnblendedCost',
    ],
    GroupBy=[
        {
            'Type': 'DIMENSION',
            'Key': 'SERVICE'
        },
    ],
    )
    object.put(Body=json.dumps(response).encode())
    return str (response)
</code></pre>

<p>It gives a response like this according to <a href=""https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/ce.html#CostExplorer.Client.get_cost_and_usage"" rel=""nofollow noreferrer"">the docs</a></p>

<pre><code>{
    'NextPageToken': 'string',
    'GroupDefinitions': [
        {
            'Type': 'DIMENSION'|'TAG',
            'Key': 'string'
        },
    ],
    'ResultsByTime': [
        {
            'TimePeriod': {
                'Start': 'string',
                'End': 'string'
            },
            'Total': {
                'string': {
                    'Amount': 'string',
                    'Unit': 'string'
                }
            },
            'Groups': [
                {
                    'Keys': [
                        'string',
                    ],
                    'Metrics': {
                        'string': {
                            'Amount': 'string',
                            'Unit': 'string'
                        }
                    }
                },
            ],
            'Estimated': True|False
        },
    ]
}
</code></pre>

<p>Which looks something like this when I run my function <strong>(VS Code did this spacing)</strong>:</p>

<pre><code>{
    ""GroupDefinitions"": [
        {
            ""Type"": ""DIMENSION"",
            ""Key"": ""SERVICE""
        }
    ],
    ""ResultsByTime"": [
        {
            ""TimePeriod"": {
                ""Start"": ""2019-01-01"",
                ""End"": ""2019-02-01""
            },
            ""Total"": {
                ""UnblendedCost"": {
                    ""Amount"": ""0"",
                    ""Unit"": ""USD""
                }
            },
            ""Groups"": [],
            ""Estimated"": true
        },
        {
            ""TimePeriod"": {
                ""Start"": ""2019-02-01"",
                ""End"": ""2019-03-01""
            },
            ""Total"": {
                ""UnblendedCost"": {
                    ""Amount"": ""0"",
                    ""Unit"": ""USD""
                }
            },
            ""Groups"": [],
            ""Estimated"": true
        },
        {
            ""TimePeriod"": {
                ""Start"": ""2019-03-01"",
                ""End"": ""2019-04-01""
            },
            ""Total"": {
                ""UnblendedCost"": {
                    ""Amount"": ""0"",
                    ""Unit"": ""USD""
                }
            },
            ""Groups"": [],
            ""Estimated"": false
        },
        {
            ""TimePeriod"": {
                ""Start"": ""2019-04-01"",
                ""End"": ""2019-05-01""
            },
            ""Total"": {},
            ""Groups"": [
                {
                    ""Keys"": [
                        ""AWS CloudTrail""
                    ],
                    ""Metrics"": {
                        ""UnblendedCost"": {
                            ""Amount"": ""0.032953"",
                            ""Unit"": ""USD""
                        }
                    }
                },
                {
                    ""Keys"": [
                        ""AWS CodeCommit""
                    ],
                    ""Metrics"": {
                        ""UnblendedCost"": {
                            ""Amount"": ""0"",
                            ""Unit"": ""USD""
                        }
                    }
                },
                {
                    ""Keys"": [
                        ""AWS Config""
                    ],
                    ""Metrics"": {
                        ""UnblendedCost"": {
                            ""Amount"": ""10.148"",
                            ""Unit"": ""USD""
                        }
                    }
                },
                {
                    ""Keys"": [
                        ""AWS Elemental MediaStore""
                    ],
                    ""Metrics"": {
                        ""UnblendedCost"": {
                            ""Amount"": ""0"",
                            ""Unit"": ""USD""
                        }
                    }
                }
            ],
            ""ResponseMetadata"": {
                ""RequestId"": ""1d149b43-3b7b-46cb-973a-6b0e9adfbe14"",
                ""HTTPStatusCode"": 200,
                ""HTTPHeaders"": {
                    ""x-amzn-requestid"": ""1d149b43-3b7b-46cb-973a-6b0e9adfbe14"",
                    ""content-type"": ""application/x-amz-json-1.1"",
                    ""content-length"": ""9310"",
                    ""date"": ""Sun, 07 Jul 2019 00:00:00 GMT""
                },
                ""RetryAttempts"": 0
            }
</code></pre>

<p>I am trying to parse out only the information I get from Groups, both as the printed response when I proxy it back to API Gateway and also when I persist it to S3 (or Dynamo) to save reports and eventually add some analytics layering to this. I modified the end of my function code to this:</p>

<pre class=""lang-py prettyprint-override""><code>...Lambda Code...
object.put(Body=json.dumps(response['ResultsByTime'][0]['Groups']['Keys']).encode())
    return str (response['ResultsByTime'][0]['Groups']['Keys'])
</code></pre>

<p>That did not work, and now I am getting this error in CloudWatch Logs</p>

<pre><code>list indices must be integers or slices, not str: TypeError
Traceback (most recent call last):
File ""/var/task/lambda_function.py"", line 25, in handler
print(response['ResultsByTime'][0]['Groups']['Keys'])
TypeError: list indices must be integers or slices, not str
</code></pre>

<p>Is there something obvious I am doing wrong? Am I not allowed to parse out only a specific array in the body? Thanks for your help in advance!</p>
",11721362.0,,174777.0,,2019-07-08 00:22:06,2019-07-08 00:24:52,Parsing AWS Lambda Python Function Response from Boto3 Cost Explorer Client,<python><json><amazon-web-services><aws-lambda><serverless-framework>,1,0,,,,CC BY-SA 4.0,I am writing a Lambda Function in Python 3 6 to query out specific conditions from the Cost Explorer API  it will eventually be invoked by API Gateway so I want to be able to send back a pared down response  as well as take that same response and persist it into S3  I have the overall functionality working correctly  I was hoping to shortcut parsing and just crawl the response  but that was not working with Glue and Athena  The basic functioning code is below:  It gives a response like this according to   Which looks something like this when I run my function (VS Code did this spacing):  I am trying to parse out only the information I get from Groups  both as the printed response when I proxy it back to API Gateway and also when I persist it to S3 (or Dynamo) to save reports and eventually add some analytics layering to this  I modified the end of my function code to this:  That did not work  and now I am getting this error in CloudWatch Logs  Is there something obvious I am doing wrong  Am I not allowed to parse out only a specific array in the body  Thanks for your help in advance  
56927848,1,,,2019-07-08 02:52:18,,0,871,"<p>I have a React app which calls API gateway, which in turn triggers my Lambda functions. Now for saving cost purpose due to the potentially lets say, tens of millions of requests to the API gateway, I did some research and are looking at to potentially use ALB to invoke my Lambdas rather than API GW. My API GW is simply a Lambda-Proxy integration.</p>

<p>My question is with API GW I can add API keys and custom authorizers etc, but for ALB, how do I add a bit of authentication at the ALB layer, say only allow the invocation of my Lambda functions only from the client that I trust? Note my client is a static React app with no server behind it! I dont need anything too fancy but just want to reject requests other than my trusted request origins. Inside Lambda to cover browser I will just add CORS to response header. But at ALB level, how do I achieve what I required?</p>

<p>Looking forward to getting some shed of lights here!</p>

<p>Thanks</p>
",2610721.0,,,,,2021-08-17 09:45:33,"Use Application Load Balancer (ALB) to trigger Lambda functions, how to add a bit of authentication at the ALB level?",<amazon-web-services><aws-lambda><serverless-framework><aws-serverless>,1,4,,,,CC BY-SA 4.0,I have a React app which calls API gateway  which in turn triggers my Lambda functions  Now for saving cost purpose due to the potentially lets say  tens of millions of requests to the API gateway  I did some research and are looking at to potentially use ALB to invoke my Lambdas rather than API GW  My API GW is simply a Lambda-Proxy integration  My question is with API GW I can add API keys and custom authorizers etc  but for ALB  how do I add a bit of authentication at the ALB layer  say only allow the invocation of my Lambda functions only from the client that I trust  Note my client is a static React app with no server behind it  I dont need anything too fancy but just want to reject requests other than my trusted request origins  Inside Lambda to cover browser I will just add CORS to response header  But at ALB level  how do I achieve what I required  Looking forward to getting some shed of lights here  Thanks 
58361050,1,,,2019-10-13 06:15:53,,2,469,"<p>I am using AWS IoT. I want to throttle the connections and messages from a particular device.
( mainly to prevent costs )</p>

<p>Is there any way to achieve this?</p>

<p>AWS IoT device defender can be used for addressing security vulnerabilities, detect anamolies, etc.</p>

<p>But I wan to set up some threshold ( e.g. 100 messages per day), after which the messages from the same device should be rejected.</p>
",1520421.0,,,,,2019-12-27 05:19:37,"AWS IoT : Throttling connections, messages from a device",<amazon-web-services><aws-lambda><aws-iot><aws-iot-analytics>,1,0,1.0,,,CC BY-SA 4.0,I am using AWS IoT  I want to throttle the connections and messages from a particular device  ( mainly to prevent costs ) Is there any way to achieve this  AWS IoT device defender can be used for addressing security vulnerabilities  detect anamolies  etc  But I wan to set up some threshold ( e g  100 messages per day)  after which the messages from the same device should be rejected  
58362746,1,,,2019-10-13 10:35:26,,0,58,"<p>I raised a similar question recently <a href=""https://stackoverflow.com/questions/57608422/why-is-my-lambda-unable-to-access-the-internet"">here</a> which was resolved, however I've change the  setup slightly.</p>

<p>The Nat Gateway was costing a small fortune so I switched it out for a Nat Instance using EC2. I followed <a href=""https://hackernoon.com/dealing-with-an-aws-billing-surprise-beware-the-defaults-d8a95f6635a2"" rel=""nofollow noreferrer"">this</a> guide.</p>

<p>Now my Lambda is successfully connecting to both the RDS instance and also has internet connectivity which is great. The problem I now have is I'm unable to connect to RDS Postgres through port 5432 from my local machine and I'm a bit stumped.</p>

<p><a href=""https://imgur.com/a/Y891Vmi"" rel=""nofollow noreferrer"">Screenshots</a> here, let me know if you need anything else.</p>

<p>Cheers!</p>
",4449455.0,,,,,2019-10-13 10:35:26,How can I connect to RDS from my local machine?,<amazon-web-services><aws-lambda><amazon-rds>,0,5,1.0,,,CC BY-SA 4.0,I raised a similar question recently  which was resolved  however Ive change the  setup slightly  The Nat Gateway was costing a small fortune so I switched it out for a Nat Instance using EC2  I followed  guide  Now my Lambda is successfully connecting to both the RDS instance and also has internet connectivity which is great  The problem I now have is Im unable to connect to RDS Postgres through port 5432 from my local machine and Im a bit stumped   here  let me know if you need anything else  Cheers  
39208258,1,,,2016-08-29 14:04:33,,17,6437,"<p>AWS Lambda logging on CloudWatch may become an huge hidden cost if you have a lot of them, because there are no way to tell AWS to stop logging on CloudWatch platform.
The only way I have found to do that is to manage a custom IAM policy (associated with every lambda) and explicitally deny access to the <strong>logs:...</strong> actions:</p>

<pre><code>{
        ""Sid"": ""DisableAllLogs"",
        ""Resource"": ""*"",
        ""Action"": [
            ""logs:CreateLogGroup"",
            ""logs:CreateLogStream"",
            ""logs:PutLogEvents""
        ],
        ""Effect"": ""Deny""
}
</code></pre>

<p>Now I'm trying to fine graining the policy to let only some lambda to log. To do that I'm using the <strong>Condition</strong> parameters of the policy:</p>

<pre><code>{
        ""Sid"": ""EnableLogsForWantedLambdaTriggers"",
        ""Resource"": ""*"",
        ""Condition"": {
            ""ArnEquals"": {
                ""aws:SourceArn"": ""arn:aws:lambda:REGION:ACCOUNT-ID:function:FUNCTION-NAME""
            }
        },
        ""Action"": [
            ""logs:CreateLogGroup"",
            ""logs:CreateLogStream"",
            ""logs:PutLogEvents""
        ],
        ""Effect"": ""Allow""
}
</code></pre>

<p>but in this way no log is sent to CloudWatch. I think that the source ARN is wrong but I can't figure out to find the correct one.</p>

<p>Any clues?</p>
",2692902.0,,2692902.0,,2016-08-29 14:09:42,2016-09-02 07:52:20,How to stop AWS Lambda function to log on CloudWatch,<amazon-web-services><aws-lambda>,1,3,,,,CC BY-SA 3.0,AWS Lambda logging on CloudWatch may become an huge hidden cost if you have a lot of them  because there are no way to tell AWS to stop logging on CloudWatch platform  The only way I have found to do that is to manage a custom IAM policy (associated with every lambda) and explicitally deny access to the logs:    actions:  Now Im trying to fine graining the policy to let only some lambda to log  To do that Im using the Condition parameters of the policy:  but in this way no log is sent to CloudWatch  I think that the source ARN is wrong but I cant figure out to find the correct one  Any clues  
58569706,1,,,2019-10-26 09:48:33,,3,771,"<p>due to huge costs in our environment, I have a task to create a lambda to tag all log groups like corresponding resources (the source of these log groups). However, I am facing a challenge to identify the resource arn of log groups. There are many logs in our environment like logs for lambda, logs for elastic-beanstalk, logs for ec2. But how can I match the log group with the corresponding resource? I would appreciate any help very much!</p>
",6791551.0,,,,,2019-10-26 13:31:52,How to find for aws log groups the corresponding ressource?,<amazon-web-services><aws-lambda><boto3><amazon-cloudwatch><aws-cloudwatch-log-insights>,1,1,0.0,,,CC BY-SA 4.0,due to huge costs in our environment  I have a task to create a lambda to tag all log groups like corresponding resources (the source of these log groups)  However  I am facing a challenge to identify the resource arn of log groups  There are many logs in our environment like logs for lambda  logs for elastic-beanstalk  logs for ec2  But how can I match the log group with the corresponding resource  I would appreciate any help very much  
59984990,1,,,2020-01-30 11:27:36,,2,214,"<h1>What is a shadow launch?</h1>

<p>Shadow launch means that you launch the new version but it not yet gets live traffic. The traffic of the old version gets 1:1 copied into the the new one and so you can measure the performance and results of the new function against the old one without influencing the live system.</p>

<h1>Tried approach</h1>

<p>I already tried building something like that myself with a ""Proxy"" function that async calls the new function and dumps the results and sync calls the old one returning its results. </p>

<p>The problem are side effects: If both functions do something e.g. in DynamoDB there could be an invalid state in the DB because of the shadow launch.</p>

<h1>Goal</h1>

<p>I want to test new code versions of a particular lambda function. </p>

<h1>Intended usage Example</h1>

<p>I have a function that calculates Fibbonacci <a href=""https://gist.github.com/meghakrishnamurthy/331bd9addab3dbb1b6a23802b1c6845e#file-fibonacci-java-L43-L48"" rel=""nofollow noreferrer"">recursive</a> and stores the result with a timestamp in DynamoDB in my live system. </p>

<p>This function is called via <a href=""https://aws.amazon.com/step-functions/"" rel=""nofollow noreferrer"">AWS Step Functions</a> as one in a chain of many.</p>

<p>As it should be more efficient to calculate it <a href=""https://gist.github.com/meghakrishnamurthy/331bd9addab3dbb1b6a23802b1c6845e#file-fibonacci-java-L20-L33"" rel=""nofollow noreferrer"">iterative</a> I implement the function with this new algorithm. </p>

<p>Now I want to test the assumption that the new function is more performant than the old one.
So I shadow deploy it AWS lambda to get the actually live traffic as testing data.</p>

<h2>Problem</h2>

<p>With the current tooling I found it is only possible to execute the function twice and thereby create two entries in DynamoDB which I consider an unwanted side effect as my data in the DynamoDB is in an unintended invalid state.</p>

<p>If I mock the external services in the new function, it may always be more performant than the old one as the time-expensive external services are not access and thereby my result would be incorrect.</p>

<h1>Question</h1>

<p>Is it possible to shadow launch a new version of an AWS lambda function without these side effects? </p>
",7533177.0,,7533177.0,,2020-02-10 13:15:23,2020-02-14 20:33:56,Is a shadow launch in AWS Lambda possible?,<amazon-web-services><aws-lambda><aws-sdk><aws-sdk-js>,3,8,,,,CC BY-SA 4.0,What is a shadow launch  Shadow launch means that you launch the new version but it not yet gets live traffic  The traffic of the old version gets 1:1 copied into the the new one and so you can measure the performance and results of the new function against the old one without influencing the live system  Tried approach I already tried building something like that myself with a Proxy function that async calls the new function and dumps the results and sync calls the old one returning its results   The problem are side effects: If both functions do something e g  in DynamoDB there could be an invalid state in the DB because of the shadow launch  Goal I want to test new code versions of a particular lambda function   Intended usage Example I have a function that calculates Fibbonacci  and stores the result with a timestamp in DynamoDB in my live system   This function is called via  as one in a chain of many  As it should be more efficient to calculate it  I implement the function with this new algorithm   Now I want to test the assumption that the new function is more performant than the old one  So I shadow deploy it AWS lambda to get the actually live traffic as testing data  Problem With the current tooling I found it is only possible to execute the function twice and thereby create two entries in DynamoDB which I consider an unwanted side effect as my data in the DynamoDB is in an unintended invalid state  If I mock the external services in the new function  it may always be more performant than the old one as the time-expensive external services are not access and thereby my result would be incorrect  Question Is it possible to shadow launch a new version of an AWS lambda function without these side effects   
59863241,1,,,2020-01-22 15:37:12,,1,945,"<p>I have some tables in dynamodb and I simply want to take a cost variable, of a service and create a function that adds (like sum(column)) up all from one id and returns the result. how can I do it </p>
",5376323.0,,174777.0,,2020-01-23 00:34:42,2021-06-08 18:10:26,how to sum values in function lambda in dynamodb in AWS,<amazon-web-services><aws-lambda><amazon-dynamodb>,2,0,,,,CC BY-SA 4.0,I have some tables in dynamodb and I simply want to take a cost variable  of a service and create a function that adds (like sum(column)) up all from one id and returns the result  how can I do it  
55702144,1,55703107.0,,2019-04-16 06:48:44,,1,35,"<p>I've just figured out a big mistake I had while creating the dynamodb structure.
I've created 11 tables, whereas one of them is the table mostly refereed to and the others are complementary tables.
For example, I have a table where I hold names (together with other info) called ""Names"" and another table called ""NamesMappings"" holding all these names added to the ""Names"" table so that each time a user wants to add a name to the ""Names"" table he first tries to put the name in ""NamesMappings"" and only if it succeed (therefore this name doesn't exist) he can add the name into the ""Names"" table. This procedure helps if the name is not unique and is not the primary key in the ""Names"" table and with this technique I don't have to search inside the ""Names"" table if the name exists, but instead I can try to add it to the ""NamesMappings"" table and only if it succeed I know this is a unique name.</p>

<p>First of all, I would like to ask you if this is a common approach or there is a better one?</p>

<p>Next, I figured out that with this design I soon reached to 11 tables each has 5 provisioned capacity of read and write which leads to overall 55 provisioned read and write under the free-tier. Then I understood why I get all these payments each month, because as the number of tables is getting bigger, and I leave the provisioned capacity as default (both read/write capacity are 5) I get more and more provisioned capacity.</p>

<p>So, what should be my conclusion from this understanding? Should I try to reduce the number of tables even if it takes more effort to preform scanning and querying inside the table? Or should I split the table same as I do but reduce the capacity of these mappings tables used only for indication if an item exists or not in another table?</p>
",1779053.0,,,,,2019-04-16 07:49:16,DynamoDB Throughput vs Search time,<aws-lambda><amazon-dynamodb><throughput><capacity><aws-billing>,1,0,,,,CC BY-SA 4.0,Ive just figured out a big mistake I had while creating the dynamodb structure  Ive created 11 tables  whereas one of them is the table mostly refereed to and the others are complementary tables  For example  I have a table where I hold names (together with other info) called Names and another table called NamesMappings holding all these names added to the Names table so that each time a user wants to add a name to the Names table he first tries to put the name in NamesMappings and only if it succeed (therefore this name doesnt exist) he can add the name into the Names table  This procedure helps if the name is not unique and is not the primary key in the Names table and with this technique I dont have to search inside the Names table if the name exists  but instead I can try to add it to the NamesMappings table and only if it succeed I know this is a unique name  First of all  I would like to ask you if this is a common approach or there is a better one  Next  I figured out that with this design I soon reached to 11 tables each has 5 provisioned capacity of read and write which leads to overall 55 provisioned read and write under the free-tier  Then I understood why I get all these payments each month  because as the number of tables is getting bigger  and I leave the provisioned capacity as default (both read/write capacity are 5) I get more and more provisioned capacity  So  what should be my conclusion from this understanding  Should I try to reduce the number of tables even if it takes more effort to preform scanning and querying inside the table  Or should I split the table same as I do but reduce the capacity of these mappings tables used only for indication if an item exists or not in another table  
39312837,1,,,2016-09-04 01:43:03,,4,294,"<p>We have a .NET client application that uploads files to S3. There is an event notification registered on the bucket which triggers a Lambda to process the file. If we need to do maintenance, then we suspend our processing by removing the event notification and adding it back later when we're ready to resume processing.</p>

<p>To process the backlog of files that have queued up in S3 during the period the event notification was disabled, we write a record to a kinesis stream with the S3 key to each file, and we have an event mapping that lets Lambda consume each kinesis record. This works great for us because it allows us to control our concurrency when we are processing a large backlog by controlling the number of shards in the stream. We were originally using SNS but when we had thousands of files that needed to be reprocessed SNS would keep starting Lambdas until we hit our concurrent executions threshold, which is why we switched to Kinesis.</p>

<p>The problem we're facing right now is that the cost of kinesis is killing us, even though we barely use it. We get 150 - 200 files uploaded per minute, and our lambda takes about 15 seconds to process each one. If we suspend processing for a few hours we end up with thousands of files to process. We could easily reprocess them with a 128 shard stream, however that would cost us $1,400 / month. The current cost for running our Lambda each month is less than $300. It seems terrible that we have to increase our COGS by 400% just to be able to control our concurrency level during a recovery scenario.</p>

<p>I could attempt to keep the stream size small by default and then resize it on the fly before we re-process a large backlog, however resizing a stream from 1 shard up to 128 takes an incredibly long time. If we're trying to recover from an unplanned outage then we can't afford to sit around waiting for the stream to resize before we can use it. So my questions are:</p>

<ol>
<li><p>Can anyone recommend an alternative pattern to using kinesis shards for being able to control the upper bound on the number of concurrent lambdas draining a queue?</p></li>
<li><p>Is there something I am missing which would allow us to use Kinesis more cost efficiently?</p></li>
</ol>
",6792096.0,,881229.0,,2016-09-07 21:17:23,2016-12-06 04:29:58,Controlling Lambda + Kinesis Costs,<amazon-web-services><aws-lambda><amazon-kinesis>,1,0,1.0,,,CC BY-SA 3.0,We have a  NET client application that uploads files to S3  There is an event notification registered on the bucket which triggers a Lambda to process the file  If we need to do maintenance  then we suspend our processing by removing the event notification and adding it back later when were ready to resume processing  To process the backlog of files that have queued up in S3 during the period the event notification was disabled  we write a record to a kinesis stream with the S3 key to each file  and we have an event mapping that lets Lambda consume each kinesis record  This works great for us because it allows us to control our concurrency when we are processing a large backlog by controlling the number of shards in the stream  We were originally using SNS but when we had thousands of files that needed to be reprocessed SNS would keep starting Lambdas until we hit our concurrent executions threshold  which is why we switched to Kinesis  The problem were facing right now is that the cost of kinesis is killing us  even though we barely use it  We get 150 - 200 files uploaded per minute  and our lambda takes about 15 seconds to process each one  If we suspend processing for a few hours we end up with thousands of files to process  We could easily reprocess them with a 128 shard stream  however that would cost us $1 400 / month  The current cost for running our Lambda each month is less than $300  It seems terrible that we have to increase our COGS by 400% just to be able to control our concurrency level during a recovery scenario  I could attempt to keep the stream size small by default and then resize it on the fly before we re-process a large backlog  however resizing a stream from 1 shard up to 128 takes an incredibly long time  If were trying to recover from an unplanned outage then we cant afford to sit around waiting for the stream to resize before we can use it  So my questions are:  Can anyone recommend an alternative pattern to using kinesis shards for being able to control the upper bound on the number of concurrent lambdas draining a queue  Is there something I am missing which would allow us to use Kinesis more cost efficiently   
61224097,1,,,2020-04-15 08:05:40,,1,181,"<p>I am using AWS and SES to send Emails and Sms through lambda function using NodeJs and I need to make more than 1k or 500 REST API calls, the average call takes 3sec to execute single lambda function request so I need process multiple requests in the single request so that we can save billing because of every single request lambda trigger bill generated.</p>

<ol>
<li><p>AWS SQS receive message limit that 1 to 10 message receive
<a href=""https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_ReceiveMessage.html"" rel=""nofollow noreferrer"">SQS Limit Number of message </a></p>

<p>My question is how can we proceed with multiple messages on once AWS lambda function trigger so that we can save billing because every API request one MySQL connection created if I do this for 1k it create 1k mysql connection and it crash MySQL while executing </p></li>
</ol>
",11066573.0,,,,,2020-04-15 09:24:38,AWS Lambda and SQS Trigger,<javascript><node.js><amazon-web-services><aws-lambda><amazon-sqs>,2,1,,,,CC BY-SA 4.0,I am using AWS and SES to send Emails and Sms through lambda function using NodeJs and I need to make more than 1k or 500 REST API calls  the average call takes 3sec to execute single lambda function request so I need process multiple requests in the single request so that we can save billing because of every single request lambda trigger bill generated   AWS SQS receive message limit that 1 to 10 message receive  My question is how can we proceed with multiple messages on once AWS lambda function trigger so that we can save billing because every API request one MySQL connection created if I do this for 1k it create 1k mysql connection and it crash MySQL while executing   
61241060,1,,,2020-04-16 01:03:34,,1,260,"<p>I have a lambda function accessing a S3 bucket using <code>aws-sdk</code></p>

<p>There are a high number of operations(requests) to the S3 bucket, which is increasing considerably the cost to use lambda </p>

<p>I was hoping that the requests use the <code>s3://</code> protocol but there are going over the internet</p>

<p>I understand that one solution could be:</p>

<ul>
<li>Attach the Lambda to a VPC </li>
<li>Create a VPC endpoint to S3</li>
<li>Update the route tables of the VPC </li>
</ul>

<p>Is there a simpler way to do so? </p>
",549808.0,,,,,2020-04-29 04:29:08,Request to S3 from Lambda without leaving AWS Cloud,<amazon-web-services><amazon-s3><aws-lambda><amazon-vpc>,2,1,1.0,,,CC BY-SA 4.0,I have a lambda function accessing a S3 bucket using  There are a high number of operations(requests) to the S3 bucket  which is increasing considerably the cost to use lambda  I was hoping that the requests use the  protocol but there are going over the internet I understand that one solution could be:  Attach the Lambda to a VPC  Create a VPC endpoint to S3 Update the route tables of the VPC   Is there a simpler way to do so   
61268446,1,,,2020-04-17 09:41:40,,1,112,"<p>We are currently an AWS customer and have developed a few internal serverless apps that integrate with some of our business applications. Many of the functions we developed need to access private resources in our VPC. The userbase is small hence the frequency of function invocation are small and the cost is single digits generally.</p>

<p>We are evaluating moving to Azure. One aspect that I am stuck, is the cost of running similar apps to Azure. There are a number of the question here addressing the limitation of the Consumption Plan (<a href=""https://stackoverflow.com/questions/41341180/azure-function-access-private-resources"">link</a> and <a href=""https://stackoverflow.com/questions/48155262/vnet-integration-for-azure-function-using-consumption-plan/48160347#48160347"">link</a>) and the breaking down of, the confusing, cost structure of <a href=""https://stackoverflow.com/questions/60232579/understanding-of-azure-functions-premium-plan-billing"">Azure Functions Premium</a> needed for VNet (to access private resources). </p>

<p>The difficulty I have is it appears that our cost jumps from a few dollars to $100+ for similar functionality.  Are there alternative ways/designs that can be used without having to resort to Azure Function Premium? Have any experienced similar issue and how did you address them? </p>
",10353671.0,,,,,2020-04-21 09:21:40,Azure Functions Accessing Private Resources - Migrating from AWS,<azure><aws-lambda><azure-functions><vnet>,1,0,,,,CC BY-SA 4.0,We are currently an AWS customer and have developed a few internal serverless apps that integrate with some of our business applications  Many of the functions we developed need to access private resources in our VPC  The userbase is small hence the frequency of function invocation are small and the cost is single digits generally  We are evaluating moving to Azure  One aspect that I am stuck  is the cost of running similar apps to Azure  There are a number of the question here addressing the limitation of the Consumption Plan ( and ) and the breaking down of  the confusing  cost structure of  needed for VNet (to access private resources)   The difficulty I have is it appears that our cost jumps from a few dollars to $100+ for similar functionality   Are there alternative ways/designs that can be used without having to resort to Azure Function Premium  Have any experienced similar issue and how did you address them   
61292542,1,,,2020-04-18 16:28:26,,1,414,"<p>I'm dealing with the CloudFormation limitation of 200 resources per stack. It seems the solution is to split my service (serverless.yml file) into multiple files. I have tried automated approaches and <a href=""https://stackoverflow.com/questions/61280210/how-to-deploy-a-serverless-project-bigger-than-one-cloudformation-stack-while-la"">they don't work for me</a>. So, I'm looking into manual ones. But I don't know how.</p>

<p>This is a sample file that I have:</p>

<pre><code>service:                        serverless-test

provider:
  name:                         aws
  runtime:                      nodejs12.x
  endpointType:                 REGIONAL

plugins:
- serverless-aws-alias

functions:
  authorizerFunc:
    handler:                    code.authorizer

  users:
    handler:                    code.users
    events:
      - http:
          path:                 /user
          integration:          lambda
          authorizer:           authorizerFunc
          method:               get
          cors:                 true
          request:
            passThrough:        WHEN_NO_TEMPLATES
            template:
              application/json: '{ ""action"": ""list_users"" }'
      - http:
          path:                 /user
          integration:          lambda
          authorizer:           authorizerFunc
          method:               post
          cors:                 true
          request:
            passThrough:        WHEN_NO_TEMPLATES
            template:
              application/json: '{ ""action"": ""create_user"", ""payload"": $input.body }'

  posts:
    handler:                    code.posts
    events:
      - http:
          path:                 /post
          integration:          lambda
          authorizer:           authorizerFunc
          method:               get
          cors:                 true
          request:
            passThrough:        WHEN_NO_TEMPLATES
            template:
              application/json: '{ ""action"": ""list_posts"" }'
      - http:
          path:                 /post
          integration:          lambda
          authorizer:           authorizerFunc
          method:               post
          cors:                 true
          request:
            passThrough:        WHEN_NO_TEMPLATES
            template:
              application/json: '{ ""action"": ""create_post"", ""payload"": $input.body }'
</code></pre>

<p>Can someone please help me split this file into 2 or 3? Feel free to split it in any way you like (as long as the resulting files have fewer resources individually). It's just that the JS code should remain untouched. Also, please pay close attention to the <code>serverless-aws-alias</code> plugin. That is a crucial part of my service. Of course, the intention is that the deployment of multiple files should be identical to deploying this single file.</p>
",866082.0,,,,,2020-04-19 17:22:16,How to split the API Gateway services in a serverless project,<amazon-web-services><serverless-framework>,1,0,,,,CC BY-SA 4.0,Im dealing with the CloudFormation limitation of 200 resources per stack  It seems the solution is to split my service (serverless yml file) into multiple files  I have tried automated approaches and   So  Im looking into manual ones  But I dont know how  This is a sample file that I have:  Can someone please help me split this file into 2 or 3  Feel free to split it in any way you like (as long as the resulting files have fewer resources individually)  Its just that the JS code should remain untouched  Also  please pay close attention to the  plugin  That is a crucial part of my service  Of course  the intention is that the deployment of multiple files should be identical to deploying this single file  
37931010,1,37944541.0,,2016-06-20 20:09:52,,19,5021,"<p>Is scheduling a lambda function to get called every 20 mins with CloudWatch the best way to get rid of lambda cold start times? (not completely get rid of)... </p>

<p>Will this get pricey or is there something I am missing because I have it set up right now and I think it is working. </p>

<p>Before my cold start time would be like 10 seconds and every subsequent call would complete in like 80 ms. Now every call no matter how frequent is around 80 ms. Is this a good method until say your userbase grows, then you can turn this off? </p>

<p>My second option is just using beanstalk and having a server running 24/7 but that sounds expensive so I don't prefer it.</p>
",6085310.0,,65732.0,,2018-03-10 22:19:38,2021-04-07 16:43:27,Lambda cold start possible solution?,<amazon-web-services><amazon-ec2><aws-sdk><aws-lambda>,7,1,2.0,,,CC BY-SA 3.0,Is scheduling a lambda function to get called every 20 mins with CloudWatch the best way to get rid of lambda cold start times  (not completely get rid of)     Will this get pricey or is there something I am missing because I have it set up right now and I think it is working   Before my cold start time would be like 10 seconds and every subsequent call would complete in like 80 ms  Now every call no matter how frequent is around 80 ms  Is this a good method until say your userbase grows  then you can turn this off   My second option is just using beanstalk and having a server running 24/7 but that sounds expensive so I dont prefer it  
61312110,1,,,2020-04-19 21:52:50,,0,126,"<p>I'm creating my application with the as much serverless as possible premise. </p>

<p>Long story short, 2 services cannot be implemented as lambda functions, hence I bet on ECS tasks with EC2 autoscaling groups, due to GPU requirements, etc. </p>

<p>After doing my homework on the Lambda + VPC resources lesson I was shocked there's no easy and pleasant way to expose VPC services extension of AWS services. So the official approach stands for incorporating a lambda function into a VPC plus establishing a NAT gateway/instance or VPC endpoints in order to reach the internet and AWS services. Moreover, I can read this is not recommended and should be treated as the final solution. It slows down lambda and increases cold starts.</p>

<p>Generally, I need access to the internet and reach other AWS services from the lambda, which must make requests to ECS tasks. Those tasks are crucial contributors to my flow I'd like them to be easily callable from lambda functions. I'm not sure if VPC lambdas would make sense if I need to pay for NAT, which is comparatively expensive. Maybe I missed something.</p>

<p>Is it possible to avoid incorporating lambdas into VPC and still be able to call ECS services? If not, what is the best way to cuts costs related to NAT?</p>

<p>I'd appreciate any form of help.</p>
",8371146.0,,248823.0,,2020-04-19 23:15:58,2020-04-19 23:15:58,AWS Lambda and ECS Tasks - what is the best way to orchestrate?,<amazon-web-services><aws-lambda><amazon-ecs><serverless><amazon-vpc>,0,2,,,,CC BY-SA 4.0,Im creating my application with the as much serverless as possible premise   Long story short  2 services cannot be implemented as lambda functions  hence I bet on ECS tasks with EC2 autoscaling groups  due to GPU requirements  etc   After doing my homework on the Lambda + VPC resources lesson I was shocked theres no easy and pleasant way to expose VPC services extension of AWS services  So the official approach stands for incorporating a lambda function into a VPC plus establishing a NAT gateway/instance or VPC endpoints in order to reach the internet and AWS services  Moreover  I can read this is not recommended and should be treated as the final solution  It slows down lambda and increases cold starts  Generally  I need access to the internet and reach other AWS services from the lambda  which must make requests to ECS tasks  Those tasks are crucial contributors to my flow Id like them to be easily callable from lambda functions  Im not sure if VPC lambdas would make sense if I need to pay for NAT  which is comparatively expensive  Maybe I missed something  Is it possible to avoid incorporating lambdas into VPC and still be able to call ECS services  If not  what is the best way to cuts costs related to NAT  Id appreciate any form of help  
61324341,1,,,2020-04-20 14:01:15,,0,102,"<p>I need to implement functionality:</p>

<ol>
<li>Receive api request </li>
<li>From the input, create X different cases</li>
<li>For each case, do calculation</li>
<li>Aggregate all results, and send result back to api gateway</li>
</ol>

<p>First guess was using Step Functions, but this service has limit for 32kb of data transferred between steps, which is not working for me. Also, since I have about 10 steps, I assume it would be hard to implement and also expensive to try to use S3 for storage of this inter-steps data.
Second guess was calling multiple lambdas from single lambda, and wait for all responses. Since I use AWS C++ SDK, it seems a bit complicated since there is almost no documentation for c++ or good examples of this case.</p>

<p>The simplest solution for me would be to create multiple threads inside single lambda, but it supports only 2 cores, which is also not working for me, I need at least 50-100.</p>

<p>Do you have any other solution or idea, as simple as possible. Is it possible to use aws batch or sqs for this or something else?</p>
",7305800.0,,100297.0,,2020-07-26 11:44:05,2020-07-26 11:44:05,Calling multiple lambdas and return aggregated result,<c++><amazon-web-services><aws-lambda>,0,5,,,,CC BY-SA 4.0,I need to implement functionality:  Receive api request  From the input  create X different cases For each case  do calculation Aggregate all results  and send result back to api gateway  First guess was using Step Functions  but this service has limit for 32kb of data transferred between steps  which is not working for me  Also  since I have about 10 steps  I assume it would be hard to implement and also expensive to try to use S3 for storage of this inter-steps data  Second guess was calling multiple lambdas from single lambda  and wait for all responses  Since I use AWS C++ SDK  it seems a bit complicated since there is almost no documentation for c++ or good examples of this case  The simplest solution for me would be to create multiple threads inside single lambda  but it supports only 2 cores  which is also not working for me  I need at least 50-100  Do you have any other solution or idea  as simple as possible  Is it possible to use aws batch or sqs for this or something else  
61339400,1,61340109.0,,2020-04-21 08:45:49,,0,22,"<p>I'm a beginner in Serverless and dynamoDB. My use case consists of two tables Trips and Routes. 
Trips table consists of these parameters <code>{id, Route, Cost, Distance, Time}</code>. Routes table consists of these parameters {quantity, Rate, From, To }. </p>

<p>Cost param in the Trips table is calculated by <code>quantity * Rate</code> params from the routes table. Every time a trip is created/edited I fetch the value from the table and store the new value as <code>Cost</code> param of trips table. </p>

<p>The issue arises when someone changes the <code>quantity</code> or <code>rate</code> parameter in the Routes table, how do I propogate this change to Trips table? Currently I'm updating the <code>Cost</code> Parameter everytime someone updates Routes, is there a more efficient way?</p>
",7617328.0,,,,,2020-04-21 09:24:02,What is the correct way to propogate changes in connected DynamoDB tables,<javascript><node.js><amazon-web-services><amazon-dynamodb><serverless>,1,3,,,,CC BY-SA 4.0,Im a beginner in Serverless and dynamoDB  My use case consists of two tables Trips and Routes   Trips table consists of these parameters   Routes table consists of these parameters {quantity  Rate  From  To }   Cost param in the Trips table is calculated by  params from the routes table  Every time a trip is created/edited I fetch the value from the table and store the new value as  param of trips table   The issue arises when someone changes the  or  parameter in the Routes table  how do I propogate this change to Trips table  Currently Im updating the  Parameter everytime someone updates Routes  is there a more efficient way  
61365289,1,,,2020-04-22 12:36:48,,2,69,"<p>From what I know about Azure Functions, and serverless computing in general, is that it provides the benefit of not needing to pay for a server that is constantly running. I.E. you only pay for the compute that was used.</p>

<p>In my case, I am already paying for servers to host a web application. Would it not make sense to use those same servers to host a backend API? </p>

<p>My guess is that the performance of the web application would take a hit, but aside from that, are there any other reasons why the function apps would make sense over a web API?</p>
",2105180.0,,,,,2020-04-22 18:12:29,Azure Function apps or Web API?,<azure><azure-functions><serverless>,1,0,,,,CC BY-SA 4.0,From what I know about Azure Functions  and serverless computing in general  is that it provides the benefit of not needing to pay for a server that is constantly running  I E  you only pay for the compute that was used  In my case  I am already paying for servers to host a web application  Would it not make sense to use those same servers to host a backend API   My guess is that the performance of the web application would take a hit  but aside from that  are there any other reasons why the function apps would make sense over a web API  
38065018,1,,,2016-06-27 23:59:39,,0,595,"<p>Is there a service or framework or any way that would allow me to run Node JS for heavy computations letting me choose the number of cores?</p>

<p>I'll be more specific: let's say I want to run some expensive computation for each of my users and I have 20000 users.
So I want to run the expensive computation for each user on a separate thread/core/computer, so I can finish the computation for all users faster.</p>

<p>But I don't want to deal with low level server configuration, all I'm looking for is something similar to <a href=""https://aws.amazon.com/lambda/"" rel=""nofollow"">AWS Lambda</a> but for high performance computing, i.e., letting me scale as I please (maybe I want 1000 cores).</p>

<p>I did simulate this with AWS Lambda by having a ""master"" lambda that receives the data for all 20000 users and then calls a ""computation"" lambda for each user. Problem is, with AWS Lambda I can't make 20000 requests and wait for their callbacks at the same time (I get a request limit exceeded error).</p>

<p>With some setup I could user <a href=""https://aws.amazon.com/hpc/"" rel=""nofollow"">Amazon HPC</a>, <a href=""https://cloud.google.com/compute/"" rel=""nofollow"">Google Compute Engine</a> or <a href=""https://azure.microsoft.com/en-us/documentation/scenarios/high-performance-computing/"" rel=""nofollow"">Azure</a>, but they only go up to 64 cores, so if I need more than that, I'd still have to setup all the machines I need separately and orchestrate the communication between them with something like <a href=""https://www.open-mpi.org/"" rel=""nofollow"">Open MPI</a>, handling the different low level setups for master and compute instances (accessing via ssh and etc).</p>

<p>So is there any service I can just paste my Node JS code, maybe choose the number of cores and run (not having to care about OS, or how many computers there are in my cluster)?</p>

<p>I'm looking for something that can take that code:</p>

<pre><code>var users = [...];

function expensiveCalculation(user) {
    // ...
    return ...;
}

users.forEach(function(user) {
    Thread.create(function() {
        save(user.id, expensiveCalculation(user));
    });
});
</code></pre>

<p>And run each thread on a separate core so they can run simultaneously (therefore finishing faster).</p>
",1116465.0,,1116465.0,,2016-06-28 20:38:22,2016-06-29 20:28:00,Run Node JS on a multi-core cluster cloud,<node.js><parallel-processing><cluster-computing><aws-lambda>,1,11,,,,CC BY-SA 3.0,Is there a service or framework or any way that would allow me to run Node JS for heavy computations letting me choose the number of cores  Ill be more specific: lets say I want to run some expensive computation for each of my users and I have 20000 users  So I want to run the expensive computation for each user on a separate thread/core/computer  so I can finish the computation for all users faster  But I dont want to deal with low level server configuration  all Im looking for is something similar to  but for high performance computing  i e   letting me scale as I please (maybe I want 1000 cores)  I did simulate this with AWS Lambda by having a master lambda that receives the data for all 20000 users and then calls a computation lambda for each user  Problem is  with AWS Lambda I cant make 20000 requests and wait for their callbacks at the same time (I get a request limit exceeded error)  With some setup I could user    or   but they only go up to 64 cores  so if I need more than that  Id still have to setup all the machines I need separately and orchestrate the communication between them with something like   handling the different low level setups for master and compute instances (accessing via ssh and etc)  So is there any service I can just paste my Node JS code  maybe choose the number of cores and run (not having to care about OS  or how many computers there are in my cluster)  Im looking for something that can take that code:  And run each thread on a separate core so they can run simultaneously (therefore finishing faster)  
61363833,1,61413253.0,,2020-04-22 11:19:09,,1,726,"<p>I am using Spring Boot in my application. While searching for some IAM tools, I actually liked Auth0, but iam not affordable their pricing. So, I found another called <code>AWS Cognito</code>. </p>

<p>Below is Auth0 to restrict our custom access api</p>

<pre><code> https://auth0.com/docs/api-auth/restrict-access-api
</code></pre>

<p>Currently, I am trying to restrict access API using AWS cognito, but I am not finding correct documentation to achieve this. Can anyone please tell me whether restricting api access can be possible using aws cognito.</p>
",12152142.0,,,,,2020-04-24 16:42:44,How to Restrict Custom api access using AWS Cognito,<amazon-web-services><spring-boot><aws-lambda><aws-api-gateway><amazon-cognito>,2,2,1.0,,,CC BY-SA 4.0,I am using Spring Boot in my application  While searching for some IAM tools  I actually liked Auth0  but iam not affordable their pricing  So  I found another called    Below is Auth0 to restrict our custom access api  Currently  I am trying to restrict access API using AWS cognito  but I am not finding correct documentation to achieve this  Can anyone please tell me whether restricting api access can be possible using aws cognito  
39769031,1,,,2016-09-29 11:28:44,,9,4280,"<p>Simple as that, I am using AWS SDK in Node to make a Lambda procedure that is in charge of sending emails according to data it receives.</p>

<p>I would like to 'delay' that email, delivering in a date and time received, not in the specific moment that the function was called. The Date and Time to deliver are parameters received by the function. Any thoughts? I couldn't find much searching on the web.</p>

<p>Thanks in advance!</p>
",2970305.0,,13070.0,,2016-09-29 13:53:16,2020-04-18 01:36:54,AWS SES Schedule sending of email (Node SDK),<node.js><amazon-web-services><aws-lambda><aws-sdk><amazon-ses>,2,1,2.0,,,CC BY-SA 3.0,Simple as that  I am using AWS SDK in Node to make a Lambda procedure that is in charge of sending emails according to data it receives  I would like to delay that email  delivering in a date and time received  not in the specific moment that the function was called  The Date and Time to deliver are parameters received by the function  Any thoughts  I couldnt find much searching on the web  Thanks in advance  
61421777,1,,,2020-04-25 06:23:10,,1,118,"<p>I am using Serverless and DynamoDB and am relatively new to it. My app has a table called Trips. The parameters of the tables are {id, route, cost, selling, type, date, LR, asset } and a bunch of other irrelevant document numbers, where id is generated by uuid.</p>

<p>According to this guide for ordering data in a date range (see 5th row in the table at the end) <a href=""https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-modeling-nosql-B.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-modeling-nosql-B.html</a> I have added a parameter called <code>createdAt</code> which is a random integer from 1 to 20 sent by frontend and made a <code>GSI</code> with <code>HASH key</code> as createdAT and <code>SORT key</code> as <code>date</code>.</p>

<p>I ran 20 promises concurrently for keeping createdAt from 1 to 20.  I am simply combining all the results in one object like this.</p>

<pre><code>await Promise.all(promises).then(function (values) {
      console.log(values);
      values.map((value) =&gt; {
        value.map((v) =&gt; {
          tripdata.push(v);
        });
      });
    });
</code></pre>

<p>Here is the response</p>

<pre><code> [
 [
{
  Cost: 12128,
  date: '2020-04-01',
  RouteShortCode: 'Hazira-Manjusar-Sudeep',
  Selling: 11000,
  Type: 'Export'
},
{
  Cost: 12581,
  date: '2020-04-24',
  RouteShortCode: 'Hazira-Nandesari-Kevin',
  Selling: 10000,
  Type: 'Export'
}
],
[
{
  Cost: 12691,
  date: '2020-04-09',
  RouteShortCode: 'Hazira-Nandesari-Kevin',
  Selling: 10000,
  Type: 'Export'
}
],
[
{
  Cost: 11536,
  date: '2020-04-09',
  RouteShortCode: 'Hazira-Nandesari-Omega',
  Selling: 29000,
  Type: 'Import'
},
{
  Cost: 8973.5,
  date: '2020-04-18',
  RouteShortCode: 'Hazira-Manjusar-Sudeep',
  Selling: 11000,
  Type: 'Export'
}
],
[
{
  Cost: 11665,
  date: '2020-04-20',
  RouteShortCode: 'Hazira-Nandesari-Kevin',
  Selling: 10000,
  Type: 'Export'
}
],
]
</code></pre>

<p>Serverless.yml for tripsTable</p>

<pre><code>tripTable:
  Type: ""AWS::DynamoDB::Table""
  Properties:
    AttributeDefinitions:
      [
        { ""AttributeName"": ""id"", ""AttributeType"": ""S"" },
        { ""AttributeName"": ""date"", ""AttributeType"": ""S"" },
        { ""AttributeName"": ""createdAt"", ""AttributeType"": ""N"" },
        { ""AttributeName"": ""Asset"", ""AttributeType"": ""S"" },
      ]
    # { ""AttributeName"": ""Route"", ""AttributeType"": ""S"" },
    KeySchema:
      [
        { ""AttributeName"": ""date"", ""KeyType"": ""HASH"" },
        { ""AttributeName"": ""id"", ""KeyType"": ""RANGE"" },
      ]
    ProvisionedThroughput:
      ReadCapacityUnits: 5
      WriteCapacityUnits: 5
    StreamSpecification:
      StreamViewType: ""NEW_AND_OLD_IMAGES""
    TableName: ${self:provider.environment.TRIPS}
    GlobalSecondaryIndexes:
      - IndexName: DateVSTrips
        KeySchema:
          - AttributeName: createdAt
            KeyType: HASH
          - AttributeName: date
            KeyType: RANGE
        Projection:

          ProjectionType: ""ALL""
        ProvisionedThroughput:
          ReadCapacityUnits: ""5""
          WriteCapacityUnits: ""5""

    LocalSecondaryIndexes:
      - IndexName: TripsVSRoutes
        KeySchema:
          - AttributeName: date
            KeyType: HASH
          - AttributeName: Asset
            KeyType: RANGE
        Projection:
          ProjectionType: ALL
</code></pre>

<p>But the issue is the entry on 2020/04/01 and 2020/04/24 have the same createdAt parameter, hence after combining all array results the final result is not ordered by date.</p>

<p>Do I need to sort this list again or am i missing something here? If I need to sort it again won't it become much more inefficient?</p>
",7617328.0,,7617328.0,,2020-04-25 08:56:21,2020-04-27 13:37:00,Fetch data for a date range in dynamoDB,<amazon-web-services><database-design><nosql><amazon-dynamodb><serverless>,1,5,,,,CC BY-SA 4.0,I am using Serverless and DynamoDB and am relatively new to it  My app has a table called Trips  The parameters of the tables are {id  route  cost  selling  type  date  LR  asset } and a bunch of other irrelevant document numbers  where id is generated by uuid  According to this guide for ordering data in a date range (see 5th row in the table at the end)  I have added a parameter called  which is a random integer from 1 to 20 sent by frontend and made a  with  as createdAT and  as   I ran 20 promises concurrently for keeping createdAt from 1 to 20   I am simply combining all the results in one object like this   Here is the response  Serverless yml for tripsTable  But the issue is the entry on 2020/04/01 and 2020/04/24 have the same createdAt parameter  hence after combining all array results the final result is not ordered by date  Do I need to sort this list again or am i missing something here  If I need to sort it again wont it become much more inefficient  
61661789,1,,,2020-05-07 15:34:34,,0,550,"<p>I'm in the process of building an application for stripe payments. This application generates a form that passes the data to the Stripe api via nextjs api. I just need to build in some basic authentication so only those submitting their payments via my form have access to the api. How would I go about adding some basic auth to my api without requiring users to login? Would I do this via env variable? I'm fairly new to the nextjs/vercel world and come from the python/django/react realm, so maybe my thoughts on this are backwards... I'm planning on hosting the api on vercel and the react form on a php site. So the react form will essentially push data to the vercel app api. 
(The reason I'm not building the api in php is because I don't know php and because I'm attempting to build something with as little footprint in the current php site as possible.) Any help or guidance on this would be much appreciated!  </p>

<p><strong>My pages/api/customers.js file</strong></p>

<pre><code>import Stripe from 'stripe'

const stripe = new Stripe(process.env.SECRET_KEY)

export default async (req, res) =&gt; {
  if (req.method === 'POST') {
    try {
      const { email, name, address, phone, source } = req.body

      // Check for customer
      const customerExist = await stripe.customers.list(
        {
          email: email,
          limit: 0
        })
      // console.log('customerExist', customerExist.data[0])
      if (customerExist.data.length &lt; 1) {
        const customer = await stripe.customers.create({
          email,
          name,
          address,
          phone,
          source
        })
        res.status(200).send(customer.id)
      } else {
        res.status(200).send(customerExist.data[0].id)
      }
    } catch (err) {
      res.status(500).json({ statusCode: 500, message: err.message })
    }
  } else {
    res.setHeader('Allow', 'POST')
    res.status(405).end('Method Not Allowed')
  }
}
</code></pre>

<p>Part of my checkout form</p>

<pre><code>  // Function to check/create a user account via api
  const checkUserAccount = async (billingDetails, source) =&gt; {
    try {
      const customer = await axios.post('/api/customers', {
        email: billingDetails.email,
        name: billingDetails.name,
        phone: billingDetails.phone,
        address: billingDetails.address,
        source: source
      })
      return customer.data
    } catch (err) {
      console.log(err)
    }
  }
</code></pre>

<p>UPDATE:</p>

<p>Alright, so I added a ""TOKEN"" to my <code>.env</code> file and now require my api to receive that specific token. </p>

<p>I added this to my checkout form: </p>

<pre><code>...
axios.defaults.headers.common.Authorization = process.env.AUTH_TOKEN
axios.defaults.headers.post['Content-Type'] = 'application/x-www-form-urlencoded'
...
</code></pre>

<p>and then added this to the api: </p>

<p><code>if (req.method === 'POST' &amp;&amp; req.headers.authorization === process.env.AUTH_TOKEN)...</code></p>

<p>Since I'm not using a login/logout system, I'm hoping this is enough. Thoughts or feedback are more than welcome. </p>
",7736704.0,,7736704.0,,2020-05-07 17:55:20,2020-05-07 17:55:20,Nextjs api and authentication,<node.js><reactjs><stripe-payments><next.js><vercel>,0,5,,,,CC BY-SA 4.0,Im in the process of building an application for stripe payments  This application generates a form that passes the data to the Stripe api via nextjs api  I just need to build in some basic authentication so only those submitting their payments via my form have access to the api  How would I go about adding some basic auth to my api without requiring users to login  Would I do this via env variable  Im fairly new to the nextjs/vercel world and come from the python/django/react realm  so maybe my thoughts on this are backwards    Im planning on hosting the api on vercel and the react form on a php site  So the react form will essentially push data to the vercel app api   (The reason Im not building the api in php is because I dont know php and because Im attempting to build something with as little footprint in the current php site as possible ) Any help or guidance on this would be much appreciated    My pages/api/customers js file  Part of my checkout form  UPDATE: Alright  so I added a TOKEN to my  file and now require my api to receive that specific token   I added this to my checkout form:   and then added this to the api:   Since Im not using a login/logout system  Im hoping this is enough  Thoughts or feedback are more than welcome   
61662447,1,61662587.0,,2020-05-07 16:05:38,,1,412,"<p>I want to develop a Web API using .NET Core that needs to be able to handle a large number of concurrent requests.  Furthermore, the WebAPI needs to connect to a database.  The Web API will be internal and won't be exposed on Internet.</p>

<p>I'm considering two possibilities:  </p>

<ul>
<li>Hosting an ASP.NET Core application in Kestrel in one or more containers / EC2 instances, with or without IIS.</li>
<li>A serverless solution using AWS Lambda</li>
</ul>

<p>I'm trying to understand what considerations I need to be aware of that will impact the performance and cost of each of these solutions.</p>

<ul>
<li><p>I understand that a Kestrel server with an application that uses async / await patterns can be expected to handle large numbers of concurrent requests.  Database connection pooling means database connections can be efficiently shared between requests.</p></li>
<li><p>In <a href=""https://www.quora.com/Is-the-Aws-Lambda-thread-safe-If-not-then-how-can-we-make-it-safe"" rel=""nofollow noreferrer"">this forum post on AWS Lambda</a> I read that:</p></li>
</ul>

<blockquote>
  <p>I understand this questions more in terms if AWS Lambda calls:
  response = Function(request) are thread-safe.</p>
  
  <p>The answer is yes.</p>
  
  <p>It is because of one very simple reason. <strong>AWS Lambda does not allow for
  another thread to invoke the same lambda instance before the previous
  thread exits.</strong> And since multiple lambda instances of the same function
  do not share resources, this is not an issue also.</p>
</blockquote>

<p>I understood and interpreted this as meaning:</p>

<ul>
<li>Each lambda instance can only handle one request at a time.</li>
<li>This means a large number of instances are needed to handle a large number of concurrent requests.</li>
<li>Each lambda instance will have its own database connection pool (probably with only a single connection).</li>
<li>Therefore the total number of database connections is likely to be higher, bounded by the Lambda function level concurrency limit.</li>
<li>Also as traffic increases and new lambda instances are created to respond to the demand, there will be significant latency for some requests due to the cold start time of each lambda instance.</li>
</ul>

<p>Is this understanding correct?  </p>
",13087.0,,,,,2020-05-07 16:11:39,AWS Lambda vs Kestrel .NET Core performance,<asp.net-core><aws-lambda><kestrel-http-server>,1,1,,,,CC BY-SA 4.0,I want to develop a Web API using  NET Core that needs to be able to handle a large number of concurrent requests   Furthermore  the WebAPI needs to connect to a database   The Web API will be internal and wont be exposed on Internet  Im considering two possibilities:    Hosting an ASP NET Core application in Kestrel in one or more containers / EC2 instances  with or without IIS  A serverless solution using AWS Lambda  Im trying to understand what considerations I need to be aware of that will impact the performance and cost of each of these solutions   I understand that a Kestrel server with an application that uses async / await patterns can be expected to handle large numbers of concurrent requests   Database connection pooling means database connections can be efficiently shared between requests  In  I read that:   I understand this questions more in terms if AWS Lambda calls:   response = Function(request) are thread-safe  The answer is yes  It is because of one very simple reason  AWS Lambda does not allow for   another thread to invoke the same lambda instance before the previous   thread exits  And since multiple lambda instances of the same function   do not share resources  this is not an issue also   I understood and interpreted this as meaning:  Each lambda instance can only handle one request at a time  This means a large number of instances are needed to handle a large number of concurrent requests  Each lambda instance will have its own database connection pool (probably with only a single connection)  Therefore the total number of database connections is likely to be higher  bounded by the Lambda function level concurrency limit  Also as traffic increases and new lambda instances are created to respond to the demand  there will be significant latency for some requests due to the cold start time of each lambda instance   Is this understanding correct    
61664757,1,,,2020-05-07 18:08:50,,1,3172,"<p>I have a nextjs app that I'm using to process stripe payments that I've deployed to Vercel. The app is simply a form with tons of functionality built in but requires the use of an api (which is why I'm using nextjs). What is the best way to get this into a php site? I need to access the nextjs api in order to communicate with stripe. Otherwise I would have just built the react app in the php site and rendered it on the page in question per usual. I've never done this before, so please any recommendations would be huge.</p>

<p>Here are the options I've come up with: </p>

<ol>
<li>iframe. I've never been a huge fan of these, but figured this was an option. I can render my app via iframe on the php site. </li>
<li>use the vercel deployed nextjs app strictly for the api and build the react form into the php site. Then I would simply change the api requests to point to the vercel deployed nextjs app instead of a local api.</li>
</ol>

<p>Are there better more efficient methods? </p>
",7736704.0,,,,,2020-09-16 19:45:57,Port a nextjs application into a php site,<php><reactjs><next.js><vercel>,1,2,1.0,,,CC BY-SA 4.0,I have a nextjs app that Im using to process stripe payments that Ive deployed to Vercel  The app is simply a form with tons of functionality built in but requires the use of an api (which is why Im using nextjs)  What is the best way to get this into a php site  I need to access the nextjs api in order to communicate with stripe  Otherwise I would have just built the react app in the php site and rendered it on the page in question per usual  Ive never done this before  so please any recommendations would be huge  Here are the options Ive come up with:   iframe  Ive never been a huge fan of these  but figured this was an option  I can render my app via iframe on the php site   use the vercel deployed nextjs app strictly for the api and build the react form into the php site  Then I would simply change the api requests to point to the vercel deployed nextjs app instead of a local api   Are there better more efficient methods   
44043162,1,,,2017-05-18 08:59:53,,1,432,"<p>I have my Postgres DB with daily increasing data(approx 500 rows added per day) in following format </p>

<pre><code>Timestamp,Val1, Val2, Val3, Val4, Val5
--------------------------------------------
1494410340000,1360,1362,1359.2,1354.2,28453
1494410340000,1360,1362,1359.2,1354.2,28453
1494410340000,1360,1362,1359.2,1354.2,28453
</code></pre>

<p>Every End of Day, I can write these data to AWS S3 as CSV file
Each CSV file contains data in above format for that day.
10May.csv, 11May.csv
12May.csv and so on.</p>

<p>These files will be hardly 25 KB each.</p>

<p>I want to store the above data in AWS and allow client to directly get filtered N number of rows</p>

<p><strong>For example:</strong> client can request for data between <strong><em>10May 11 A.M.</strong> to <strong>11May 3 P.M</em></strong></p>

<p>Basically I need to mimic this query on multiple CSV files: </p>

<pre><code>select * from allcsvdata where timestamp between Ts1 and Ts2
</code></pre>

<p>Relevant things i have found so far: </p>

<ol>
<li>AWS Athena -> Read csv and query then return result [min charge for
10MB per scan :(]</li>
<li>AWS Gateway -> AWS Lambda fn -> Read file from S3
and return result</li>
</ol>

<p><strong>What would be better approach to this situation. 70% queries will require multiple days data [reading multiple csv files].</strong> </p>

<p>So should I append all data in single file and use Athena ?</p>

<p>Or should I get an EC2 with presto? </p>

<p>Or Any other architecture to suit this need ?</p>

<p>I am open to suggestions, let me know if any other details are required?</p>
",2129748.0,,,,,2017-05-18 16:57:14,Which AWS services should I use for multiple small CSV file data query (cost effective way),<postgresql><amazon-s3><aws-lambda><aws-sdk><amazon-athena>,1,0,,,,CC BY-SA 3.0,I have my Postgres DB with daily increasing data(approx 500 rows added per day) in following format   Every End of Day  I can write these data to AWS S3 as CSV file Each CSV file contains data in above format for that day  10May csv  11May csv 12May csv and so on  These files will be hardly 25 KB each  I want to store the above data in AWS and allow client to directly get filtered N number of rows For example: client can request for data between 10May 11 A M  to 11May 3 P M Basically I need to mimic this query on multiple CSV files:   Relevant things i have found so far:   AWS Athena -&gt; Read csv and query then return result [min charge for 10MB per scan :(] AWS Gateway -&gt; AWS Lambda fn -&gt; Read file from S3 and return result  What would be better approach to this situation  70% queries will require multiple days data [reading multiple csv files]   So should I append all data in single file and use Athena   Or should I get an EC2 with presto   Or Any other architecture to suit this need   I am open to suggestions  let me know if any other details are required  
38216475,1,,,2016-07-06 04:49:14,,4,115,"<p>I'm interning at a small company this summer and have been tasked with parsing log files from a kinesis stream. This has an extremely high throughput, so I've been learning how to do 'real-time' parsing, for lack of a better term, so as to avoid bloating memory and incurring extra costs in lambda.</p>

<p>I went into the project expecting something tedious but manageable, but there are several problems I've encountered:</p>

<ol>
<li><p>Delimiters are 'lost in translation' at some point between the logs being aggregated from many origins to when I receive them. There isn't anything I can easily do like break on tab, 4 spaces, 2 spaces, 3 spaces, colons, commas, etc. because it tends to fracture the logs at unintended points</p></li>
<li><p>There are apparently several thousand types of logs that need to be addressed and analyzed. Many of these are from the ""same"" source (MSWinEventLog, for example), but are still unique in their own right. Windows logs make up around ~85% of random samples. The spreadsheet of log types that was supplied to me records ""0"" instances of many types of logs, but I assume that their presence means they've been observed at some point outside that collection period.</p></li>
</ol>

<p>Everytime I think I find a pattern in a particular type of log, one comes along from a different source and kills it. Field names for the most part are not included, outside of Windows event number specific data, which have colons attached</p>

<p><code>heading1: field1: data1  field with spaces2: data2    field3: data3 with spaces     also_part_of_data3values field4: field5_after4_with_no_value: data5
</code></p>

<p>Right now my approach is somewhat naive and only addressed the Windows events. It employs a combination of regex and ""smart pair"" parsing. Regex to get some fields that I can somewhat rely on to be there, and to identify ""key elements"" when pairing. The pairing is for the parts that are delimited by colons (or '=' in the case of powershell logs). I make a good attempt at separating all fields and values into a list. Then, for each element, I check if it's a key. If it is, I pair it with the next element which <em>should</em> be a value. If the next element is also a key, then the last one is either a header or a field with no value, and is discarded. After pairing a key and value, if the next element does not match 'something:' pattern, then I assumed it's a key with multiple values. Once another key element is encountered the key: value(s) up to that point are added to a dictionary. So, from the above example I'd (hopefully) end up with:</p>

<pre><code>""field1"": ""data1"",
""field with spaces 2"": ""data2"",
""field3"": [""data3 with spaces"", ""also_part_of_data3values""],
""field5"": ""data5""
</code></pre>

<p>This approach sort of works for windows logs. I try and trim unnecessary information (for example, many logs include the description text from Windows about what an event is, that can be 10+ sentences).</p>

<p>My worry is that because the volume of logs is so huge it's really hard to account for every possible log that might go into the parser, especially with their formats not always being the same even within one log type (date / time I've noticed is extremely inconsistent in both order and delimiter). Writing a few thousand Regex doesn't seem practical for one person (me) to do either.</p>

<p>So I guess my question is: from people who have dealt with a similarly messy situation, how can I approach this and fix the duct-taped spaghetti I have now?</p>
",2782805.0,,-1.0,,2017-09-22 17:48:51,2017-09-20 07:58:47,Most intuitive way to parse several thousand different log types (with Python)?,<python><regex><parsing><aws-lambda><bigdata>,2,4,1.0,,,CC BY-SA 3.0,Im interning at a small company this summer and have been tasked with parsing log files from a kinesis stream  This has an extremely high throughput  so Ive been learning how to do real-time parsing  for lack of a better term  so as to avoid bloating memory and incurring extra costs in lambda  I went into the project expecting something tedious but manageable  but there are several problems Ive encountered:  Delimiters are lost in translation at some point between the logs being aggregated from many origins to when I receive them  There isnt anything I can easily do like break on tab  4 spaces  2 spaces  3 spaces  colons  commas  etc  because it tends to fracture the logs at unintended points There are apparently several thousand types of logs that need to be addressed and analyzed  Many of these are from the same source (MSWinEventLog  for example)  but are still unique in their own right  Windows logs make up around ~85% of random samples  The spreadsheet of log types that was supplied to me records 0 instances of many types of logs  but I assume that their presence means theyve been observed at some point outside that collection period   Everytime I think I find a pattern in a particular type of log  one comes along from a different source and kills it  Field names for the most part are not included  outside of Windows event number specific data  which have colons attached  Right now my approach is somewhat naive and only addressed the Windows events  It employs a combination of regex and smart pair parsing  Regex to get some fields that I can somewhat rely on to be there  and to identify key elements when pairing  The pairing is for the parts that are delimited by colons (or = in the case of powershell logs)  I make a good attempt at separating all fields and values into a list  Then  for each element  I check if its a key  If it is  I pair it with the next element which should be a value  If the next element is also a key  then the last one is either a header or a field with no value  and is discarded  After pairing a key and value  if the next element does not match something: pattern  then I assumed its a key with multiple values  Once another key element is encountered the key: value(s) up to that point are added to a dictionary  So  from the above example Id (hopefully) end up with:  This approach sort of works for windows logs  I try and trim unnecessary information (for example  many logs include the description text from Windows about what an event is  that can be 10+ sentences)  My worry is that because the volume of logs is so huge its really hard to account for every possible log that might go into the parser  especially with their formats not always being the same even within one log type (date / time Ive noticed is extremely inconsistent in both order and delimiter)  Writing a few thousand Regex doesnt seem practical for one person (me) to do either  So I guess my question is: from people who have dealt with a similarly messy situation  how can I approach this and fix the duct-taped spaghetti I have now  
39851894,1,39853953.0,,2016-10-04 12:07:16,,0,63,"<p>I have a question reagrding AWS and Lambda functions, as I plan to design a notifications system for my app.</p>

<p>I have events and I want to send a reminder an hour before every event to every user attending. The dates are stored in the data base, as well as are the attending users. So I thought about the Lambda function which would send the notification to the user's device when it reached the right time.</p>

<p>However, is that possible with LAMBDA AWS to write such a function which would periodically look for this? I also wouldn't like to check it every-minute as it is quite costly then (I assume). How else I could design it?</p>

<p>Thank you in advance,
Grzegorz</p>
",6470918.0,,13070.0,,2016-10-04 12:55:19,2016-10-04 13:44:01,Creating a professional notifications system,<android><amazon-web-services><aws-lambda>,1,0,,,,CC BY-SA 3.0,I have a question reagrding AWS and Lambda functions  as I plan to design a notifications system for my app  I have events and I want to send a reminder an hour before every event to every user attending  The dates are stored in the data base  as well as are the attending users  So I thought about the Lambda function which would send the notification to the users device when it reached the right time  However  is that possible with LAMBDA AWS to write such a function which would periodically look for this  I also wouldnt like to check it every-minute as it is quite costly then (I assume)  How else I could design it  Thank you in advance  Grzegorz 
61683190,1,,,2020-05-08 16:03:57,,1,29,"<p>First of all, I hope everyone is safe and healthy in this difficult Covid 19 situation.</p>

<p>Q: What would be ideal &amp; cost effective solution/selection between AWS EC2  or Lambda for below scenario in long term:</p>

<p>Suppose, I have to host a backend (Python based Flask) which contains at most 40 API endpoints which will be used by both react based web app &amp; react native based mobile app.</p>

<p>Out of those 40 , Only 5 api's will be having average  concurrency of max 500 and total API hits could be around ~1500 to ~2000  daily average for all 40 API's.</p>

<p>Idle time could be 8 to 10 hr daily</p>
",9356491.0,,,,,2020-05-08 16:03:57,What would be ideal & cost effective solution/selection between AWS EC2 or Lambda for below scenario in long term,<amazon-web-services><amazon-ec2><aws-lambda>,0,2,,,,CC BY-SA 4.0,First of all  I hope everyone is safe and healthy in this difficult Covid 19 situation  Q: What would be ideal &amp; cost effective solution/selection between AWS EC2  or Lambda for below scenario in long term: Suppose  I have to host a backend (Python based Flask) which contains at most 40 API endpoints which will be used by both react based web app &amp; react native based mobile app  Out of those 40   Only 5 apis will be having average  concurrency of max 500 and total API hits could be around ~1500 to ~2000  daily average for all 40 APIs  Idle time could be 8 to 10 hr daily 
56453496,1,,,2019-06-05 02:17:53,,11,2204,"<p>Quickly introducing my scenario: I have a VPC that contains an API Gateway that redirects its calls to my Lambda functions and then they access both an RDS instance and external API calls (internet access).</p>

<h3>How it's structured</h3>

<p>Due to the fact that the functions need to access the RDS, I've put both RDS and Lambdas in the same VPC, properly securing the RDS without public accessibility. Now, because the Lambdas are in a VPC, they need a NAT Gateway to access the internet (almost all of those functions need to call third parties APIs), and this is where I'm facing an enormous problem.</p>

<h3>The problem</h3>

<p>I have a small project to serve a few users (ranging from 10 to 200 users) and with the serverless setup that I've created, I'm expecting costs to be <strong>from $3.00 to $10.00 each month</strong>. That's the cost <strong>without</strong> a <strong>single</strong> NAT Gateway. Now, and if we add the price of a Gateway, which is <strong><a href=""https://aws.amazon.com/vpc/pricing/"" rel=""noreferrer"">$0.045 per hour</a></strong> - and I'm not even taking into consideration the $0.045 per GB of data transferred -, that's <strong>>$30 per month</strong>. It would be dumb of me to not create another to be Multi-AZ and mitigate possible availability zone failure - so <strong>>$60.00 for 2 NAT Gateways</strong>. </p>

<p>This is not only impractical for me, but wouldn't it also invalidate the point of the whole serverless structure that normally follows an on-demand approach?</p>

<h3>How to solve this?</h3>

<p>One of my alternatives is to move the Lambdas out of the VPC (meaning no VPC) and accessing the RDS through some mechanism without making it publicly accessible - and here is where I'm also failing, how would one securely access the RDS in the scenario where Lambdas functions are outside the RDS VPC?</p>

<p>In the worst case scenario - I know it's bad to expose my RDS to the public - but how big of a vulnerability is exposing it?</p>

<p>Keep in mind that I'm not blaming AWS prices, this is solely focused on finding alternatives to the NAT Gateway one - I appreciate suggestions to solve this case. Also, I'm sorry if I made a totally wrong assumption, I'm new to the AWS ecosystem.</p>
",8558606.0,,,,,2019-06-05 06:51:34,Alternative to AWS Lambda + NAT gateway,<amazon-web-services><aws-lambda><nat><amazon-vpc>,1,3,5.0,,,CC BY-SA 4.0,Quickly introducing my scenario: I have a VPC that contains an API Gateway that redirects its calls to my Lambda functions and then they access both an RDS instance and external API calls (internet access)  How its structured Due to the fact that the functions need to access the RDS  Ive put both RDS and Lambdas in the same VPC  properly securing the RDS without public accessibility  Now  because the Lambdas are in a VPC  they need a NAT Gateway to access the internet (almost all of those functions need to call third parties APIs)  and this is where Im facing an enormous problem  The problem I have a small project to serve a few users (ranging from 10 to 200 users) and with the serverless setup that Ive created  Im expecting costs to be from $3 00 to $10 00 each month  Thats the cost without a single NAT Gateway  Now  and if we add the price of a Gateway  which is  - and Im not even taking into consideration the $0 045 per GB of data transferred -  thats &gt;$30 per month  It would be dumb of me to not create another to be Multi-AZ and mitigate possible availability zone failure - so &gt;$60 00 for 2 NAT Gateways   This is not only impractical for me  but wouldnt it also invalidate the point of the whole serverless structure that normally follows an on-demand approach  How to solve this  One of my alternatives is to move the Lambdas out of the VPC (meaning no VPC) and accessing the RDS through some mechanism without making it publicly accessible - and here is where Im also failing  how would one securely access the RDS in the scenario where Lambdas functions are outside the RDS VPC  In the worst case scenario - I know its bad to expose my RDS to the public - but how big of a vulnerability is exposing it  Keep in mind that Im not blaming AWS prices  this is solely focused on finding alternatives to the NAT Gateway one - I appreciate suggestions to solve this case  Also  Im sorry if I made a totally wrong assumption  Im new to the AWS ecosystem  
57031350,1,,,2019-07-14 21:39:47,,1,226,"<p>Background: I am very new to the AWS Management Console, and I just created a very simple AWS Lambda function(Python 3.7) that deletes EBS Volume Snapshots based on a time limit. I also created a CloudWatch event to trigger the function every hour of the day. This works well for a few volumes, but with additions of volumes, this causes cost / speed issues.</p>

<p>Question: I am trying to update my EBS Snapshot Deletion lambda function code to  find the rate limit of requests to avoid throttling, and use that to create an exponential backoff/retry model for the program(making the program scalable no matter how many snapshots there are). I would assume there is a special API call that can help me with this, but I have not been able to find any concrete information online. Any help would be much appreciated! Pasting my current code below:</p>

<pre><code>import boto3
from datetime import datetime, timezone, timedelta
ec2=boto3.resource('ec2') #resource, higher level
snapshots = ec2.snapshots.filter(OwnerIds=['self']) #all snapshots owned by me, returns list

def lambda_handler(event, context):
# TODO implement
for i in snapshots:#for each snapshot
    start_time=i.start_time #timestamp when snapshot was initiated
    delete_time=datetime.now(tz=timezone.utc)-timedelta(days=1) #Correct time in UTC timezone - 1 days]
    if delete_time&gt;start_time:#if delete time is more than start time (more than a day)
        i.delete() #call method to delete that snapshot
        print ('Snapshot with Id = {snaps} is deleted'.format(snaps=i.snapshot_id)) #pull ID that was deleted
</code></pre>
",11772332.0,,11772332.0,,2020-01-24 23:31:20,2020-01-24 23:31:20,How to go about implementing exponential backoff for an AWS Lamba function that deletes EBS snapshots,<python-3.x><aws-lambda><throttling><rate-limiting><exponential-backoff>,0,0,,,,CC BY-SA 4.0,Background: I am very new to the AWS Management Console  and I just created a very simple AWS Lambda function(Python 3 7) that deletes EBS Volume Snapshots based on a time limit  I also created a CloudWatch event to trigger the function every hour of the day  This works well for a few volumes  but with additions of volumes  this causes cost / speed issues  Question: I am trying to update my EBS Snapshot Deletion lambda function code to  find the rate limit of requests to avoid throttling  and use that to create an exponential backoff/retry model for the program(making the program scalable no matter how many snapshots there are)  I would assume there is a special API call that can help me with this  but I have not been able to find any concrete information online  Any help would be much appreciated  Pasting my current code below:  
61734414,1,,,2020-05-11 16:22:12,,2,169,"<p>I am looking for a solution where the aws api gateway can add few extra headers before forwarding to the actual backend. I can imagine that keeping lambda function as a call back function can be one way to execute it. My calls can be slow, which means, if i use lambda functions in between, i pay a lot.</p>

<p>Is there any other way to do it ?</p>

<p>I am looking for something similar to a 'pre-processor' in Tibco Mashery. Which means, when the request comes, this method/logic is executed by api gateway, which will add the extra headers, and api gateway will forward the request to the actual backend.</p>

<p>Edit:
I need to fill the header dynamically based on the incoming request and some mapping tables in db.</p>

<p>Thanks</p>
",2357083.0,,2357083.0,,2020-05-12 09:06:01,2020-05-12 09:06:01,Can I do some pre-processing in AWS API Gateway before forwarding it to the actual backend?,<amazon-web-services><aws-lambda><aws-api-gateway>,2,1,,,,CC BY-SA 4.0,I am looking for a solution where the aws api gateway can add few extra headers before forwarding to the actual backend  I can imagine that keeping lambda function as a call back function can be one way to execute it  My calls can be slow  which means  if i use lambda functions in between  i pay a lot  Is there any other way to do it   I am looking for something similar to a pre-processor in Tibco Mashery  Which means  when the request comes  this method/logic is executed by api gateway  which will add the extra headers  and api gateway will forward the request to the actual backend  Edit: I need to fill the header dynamically based on the incoming request and some mapping tables in db  Thanks 
44424314,1,,,2017-06-07 23:27:13,,0,294,"<p>I am creating an iOS app that will query my database without doing any object manipulation or calculation, I can do this in several ways:</p>

<ol>
<li><p>Query directly using DynamoDB iOS SDK</p></li>
<li><p>Query using AWS Lambda iOS SDK, and then executing a lambda function that queries DynamoDB directly.</p></li>
</ol>

<p>My question is, which is more execution-time and cost efficient?</p>

<p>My thoughts are that option 1 is the best as it does not require a function or any execution time.</p>

<p>I know that I am charged for the DynamoDB query and I know that I am charged for the execution time of the lambda function. However, I am not sure if I will be charged for using the DynamoDB SDK method or for something else or if I might forgetting another possible cost or thing that I am not taking into account.</p>

<p>Thank you.</p>
",691868.0,,691868.0,,2017-06-07 23:43:11,2017-06-19 22:25:04,Querying using DynamoDB vs AWS Lambda,<amazon-web-services><amazon-dynamodb><aws-lambda><aws-sdk>,3,0,,,,CC BY-SA 3.0,I am creating an iOS app that will query my database without doing any object manipulation or calculation  I can do this in several ways:  Query directly using DynamoDB iOS SDK Query using AWS Lambda iOS SDK  and then executing a lambda function that queries DynamoDB directly   My question is  which is more execution-time and cost efficient  My thoughts are that option 1 is the best as it does not require a function or any execution time  I know that I am charged for the DynamoDB query and I know that I am charged for the execution time of the lambda function  However  I am not sure if I will be charged for using the DynamoDB SDK method or for something else or if I might forgetting another possible cost or thing that I am not taking into account  Thank you  
56580868,1,,,2019-06-13 12:48:04,,0,396,"<p>I have an AWS Lambda function which does a couple of API calls then saves whatever information it gets to a DynamoDB table.</p>

<p>Would it be a good practice for me send a message to an SQS queue after completing whatever the Lambda was doing? The queue will then trigger the Lambda function to start another process again.</p>

<p>So with regards to processing and the costs involved, is this a good idea or not?</p>

<p>The other idea I had was to trigger the Lambda function using a CloudWatch Event but the problem is I want it to start the new process once the old one has completed so if it happens that the Lambda function gets triggered while processing its then going to stuff up my records on DynamoDB.</p>

<p>So if anyone has a better solution or alternative let me know.</p>
",7849709.0,,174777.0,,2019-06-14 03:01:15,2019-06-14 03:01:15,Make AWS Lambda call itself again after completing a task,<amazon-web-services><aws-lambda><amazon-dynamodb><amazon-sqs><amazon-cloudwatch>,0,4,,,,CC BY-SA 4.0,I have an AWS Lambda function which does a couple of API calls then saves whatever information it gets to a DynamoDB table  Would it be a good practice for me send a message to an SQS queue after completing whatever the Lambda was doing  The queue will then trigger the Lambda function to start another process again  So with regards to processing and the costs involved  is this a good idea or not  The other idea I had was to trigger the Lambda function using a CloudWatch Event but the problem is I want it to start the new process once the old one has completed so if it happens that the Lambda function gets triggered while processing its then going to stuff up my records on DynamoDB  So if anyone has a better solution or alternative let me know  
56591312,1,,,2019-06-14 04:17:56,,2,3668,"<p>I want to stream HTTP requests into BigQuery, in real time (or near real time).  </p>

<p>Ideally, I would like to use a tool that provides an endpoint to stream HTTP requests to and allows me to write simple Node such that:
1.  I can add the appropriate insertId so BigQuery can dedupe requests if necessary and
2.  I can batch the data so I don't send a single row at a time (which would result in unnecessary GCP costs)</p>

<p>I have tried using AWS Lambdas or Google Cloud Functions but the necessary setup for this problem on those platforms far exceeds the needs of the use case here.  I assume many developers have this same problem and there must be a better solution.</p>
",10333367.0,,,,,2019-06-14 12:26:06,What is the best way to stream data in real time into Big Query (using Node)?,<node.js><aws-lambda><google-bigquery><google-cloud-functions><cloudflare-workers>,2,2,,,,CC BY-SA 4.0,I want to stream HTTP requests into BigQuery  in real time (or near real time)    Ideally  I would like to use a tool that provides an endpoint to stream HTTP requests to and allows me to write simple Node such that: 1   I can add the appropriate insertId so BigQuery can dedupe requests if necessary and 2   I can batch the data so I dont send a single row at a time (which would result in unnecessary GCP costs) I have tried using AWS Lambdas or Google Cloud Functions but the necessary setup for this problem on those platforms far exceeds the needs of the use case here   I assume many developers have this same problem and there must be a better solution  
56596762,1,,,2019-06-14 10:55:41,,0,75,"<p>I'm designing a website solution in AWS containing only 2 sites: the main dashboard showing company data in a graph (index site) and single pages for each company.</p>

<p><strong>Pre-requisites:</strong></p>

<ul>
<li>serverless.</li>
<li>cost-effective solution based on-demand traffic.</li>
<li>everything can be publicly accessed. not login required.</li>
<li>website in HTML using JS.</li>
<li>the data is changed 1 time per day.</li>
</ul>

<p><strong>Functionalities:</strong></p>

<ul>
<li><p>Main dashboard (a graph in JS) reading dynamic data (last 30 reports). The data can be read from a file: txt, json or maybe a database: dynamoDB. As it has 
to be publicly accessed, I don't think using a database will be a<br>
good strategy.</p></li>
<li><p>Company page.The users can publicly post comments and rate with starts.</p></li>
</ul>

<p><strong>How do you personally would design this solution?</strong></p>

<p>My first approach is: dashboard site reading data from a txt file, company site use a DynamoDB to save/show comments and ratings.</p>
",,user9780441,,user9780441,2019-06-14 15:44:13,2019-06-19 08:46:16,Solution design for a serverless hosted in AWS,<amazon-web-services><serverless>,2,4,,,,CC BY-SA 4.0,Im designing a website solution in AWS containing only 2 sites: the main dashboard showing company data in a graph (index site) and single pages for each company  Pre-requisites:  serverless  cost-effective solution based on-demand traffic  everything can be publicly accessed  not login required  website in HTML using JS  the data is changed 1 time per day   Functionalities:  Main dashboard (a graph in JS) reading dynamic data (last 30 reports)  The data can be read from a file: txt  json or maybe a database: dynamoDB  As it has  to be publicly accessed  I dont think using a database will be a good strategy  Company page The users can publicly post comments and rate with starts   How do you personally would design this solution  My first approach is: dashboard site reading data from a txt file  company site use a DynamoDB to save/show comments and ratings  
62065707,1,,,2020-05-28 13:16:41,,2,666,"<p>I am looking into different FaaS providers and am interested how much disk space I have per function. A function in AWS Lambda has 512 MB of disk space available (see <a href=""https://docs.aws.amazon.com/lambda/latest/dg/gettingstarted-limits.html"" rel=""nofollow noreferrer"">here</a>) and Azure functions have up to 1000 GB of disk space, depending on the pricing model (see <a href=""https://docs.microsoft.com/en-us/azure/azure-functions/functions-scale"" rel=""nofollow noreferrer"">here</a>).</p>

<p>How much does a GCP Function have? </p>

<p>When searching the documentation, I could only find that functions do have disk storage, but not how much (see <a href=""https://cloud.google.com/functions/docs/concepts/exec#file_system"" rel=""nofollow noreferrer"">here</a>).</p>
",5345715.0,,5495381.0,,2020-05-28 13:51:51,2022-01-11 20:51:13,How much disk space does a GCP Function have?,<google-cloud-platform><aws-lambda><azure-functions><disk><faas>,3,0,,,,CC BY-SA 4.0,I am looking into different FaaS providers and am interested how much disk space I have per function  A function in AWS Lambda has 512 MB of disk space available (see ) and Azure functions have up to 1000 GB of disk space  depending on the pricing model (see )  How much does a GCP Function have   When searching the documentation  I could only find that functions do have disk storage  but not how much (see )  
44456814,1,,,2017-06-09 11:37:16,,0,555,"<p>I implemeted a code that deletes multiple records in a table and when that deletion is completed then goes to another table to delete it's record. But the thing is I am not deleting all records of a table just making query to some of the records that have range key (id = '1423') and all records related to this key I get and using foreach loop I then delete the records. Same I am doing in other table as well.</p>

<p>KeyPoints about my code : In both table ""id"" is not the primary key and also in both Table A and Table B I have this same ""id"" = 1423.</p>

<p>Code snippet :</p>

<pre><code>    data: function(tableName, id){
     return new Promise(function(resolve, reject){
     var scanParams = {
      TableName: tableName,
      KeyConditionExpression: ""#getId = :p"",
      ExpressionAttributeNames:{
       ""#getId"": ""id""
      },
      FilterExpression: '#getId = :v1' ,
      ExpressionAttributeValues:{
       "":v1"" : id
      }
     };  
    AWS.docClient.scan(scanParams, function(err, data) {
     if (!_.isNil(err)) {
      console.error(""Error :"", JSON.stringify(err, null, 2));
      reject(err)
     } else {
        if(_.size(data.Items) &gt; 0){
          data.Items.forEach(function(obj, i){
          var params = {
            TableName: scanParams.TableName,
            Key: {""hashKey"":obj[hashKey]},
            ReturnValues: 'NONE', // optional (NONE | ALL_OLD)
            ReturnConsumedCapacity: 'NONE', // optional (NONE | TOTAL | INDEXES)
            ReturnItemCollectionMetrics: 'NONE', // optional (NONE | SIZE)
          }
          AWS.docClient.delete(params, function(err, data) {
            if (err) {
              console.error(""Unable to delete items :"", JSON.stringify(err, null, 2));
              reject(err)
            } else {
                console.log('Deleted the records in the table')
                resolve('success')
              }// successful response
          });
        })
      } else {
          console.log('No record in table')
          resolve('success')
        }
      }
    })
  })
 }
</code></pre>

<p>I am calling this data function in other two files just passing different table name in the parameters, i.e., here : function(tableName, id) and id is same. Also wanted to know that are there any loopholes in asynchronous way and is it better than synchronous method that I am using. Any performance issue or operation cost.</p>
",7200588.0,,7200588.0,,2017-06-09 11:42:23,2017-06-14 04:05:30,Deleting Records in multiple tables of dynamo db asynchronously,<node.js><asynchronous><promise><amazon-dynamodb><aws-lambda>,1,0,,,,CC BY-SA 3.0,I implemeted a code that deletes multiple records in a table and when that deletion is completed then goes to another table to delete its record  But the thing is I am not deleting all records of a table just making query to some of the records that have range key (id = 1423) and all records related to this key I get and using foreach loop I then delete the records  Same I am doing in other table as well  KeyPoints about my code : In both table id is not the primary key and also in both Table A and Table B I have this same id = 1423  Code snippet :  I am calling this data function in other two files just passing different table name in the parameters  i e   here : function(tableName  id) and id is same  Also wanted to know that are there any loopholes in asynchronous way and is it better than synchronous method that I am using  Any performance issue or operation cost  
57165405,1,57166721.0,,2019-07-23 13:34:53,,3,2857,"<p>This really wasn't clear for me in the Docs. And the console configuration is very confusing.</p>

<p>Will a Docker Cluster running in Fargate mode behind a Load Balancer shutdown and not charge me while it's not being used?</p>

<p>What about cold starts? Do I need to care about this in Fargate like in Lambda?</p>

<p>Is it less horizontal than Lambda? A lambda hooked to API Gateway will spawn a new function for every concurrent request, will Fargate do this too? Or will the load balancer decide it?</p>

<p>I've been running Flask/Django applications in Lambda for some time (Using Serverless/Zappa), are there any benefits in migrating them to Fargate? </p>

<p>It seems to be that it is more expensive than Lambda but if the Lambda limitations are not a problem then Lambda should always be the better choice right?</p>
",3529833.0,,,,,2019-07-23 14:44:41,Is AWS Fargate true serverless like Lambda? Does it automatically shutsdown when it finishes the task?,<amazon-web-services><aws-lambda><amazon-ecs><serverless><aws-fargate>,2,1,3.0,,,CC BY-SA 4.0,This really wasnt clear for me in the Docs  And the console configuration is very confusing  Will a Docker Cluster running in Fargate mode behind a Load Balancer shutdown and not charge me while its not being used  What about cold starts  Do I need to care about this in Fargate like in Lambda  Is it less horizontal than Lambda  A lambda hooked to API Gateway will spawn a new function for every concurrent request  will Fargate do this too  Or will the load balancer decide it  Ive been running Flask/Django applications in Lambda for some time (Using Serverless/Zappa)  are there any benefits in migrating them to Fargate   It seems to be that it is more expensive than Lambda but if the Lambda limitations are not a problem then Lambda should always be the better choice right  
44468486,1,44475925.0,,2017-06-10 00:57:31,,0,178,"<p>My system architecture looks like as follows:-</p>

<p>SNS -> AWS Lambda -> Dynamo Db</p>

<p>So, SNS is publishing messages to which AWS Lambda function is the subscriber and then AWS Lambda pushes the data into Dynamo Db. In this, I am doing some transformation of messages in AWS Lambda. For the transformation, I have to fetch some rules from some place. These rules are basically the mapping between fields of the original messages to fields to transformed messages.</p>

<pre><code>Eg. 

Say, Original Message looks like below:-
{""id"": 1,
""name"":""dsadas"",
""house"":""dsads dsadsa"",
""speciality"":""asjdsa""
}

and my mapping is something like:-
{""id"":""id"",
""house"":""home"",
""speciality"":""area""
}
</code></pre>

<p>So, basically I am saying that id should be mapped to id, house to home and so on. </p>

<p>So, I want to keep this mapping at some places like Dynamo Db or some config service. I do not want to directly keep it in aws lambda code as there is a chance that I might have to change. But keeping it in Dynamo Db will be very costly in terms of latency I think because I will make a call on each message request. So, can anyone suggest, any aws resource which can be used for keeping these configs which is very fast and normally used for keeping configuration.</p>
",1642289.0,,,,,2017-06-10 17:32:52,AWS Lambda - Where to Keep the configuration,<amazon-web-services><amazon-dynamodb><aws-lambda>,1,1,1.0,,,CC BY-SA 3.0,My system architecture looks like as follows:- SNS -&gt; AWS Lambda -&gt; Dynamo Db So  SNS is publishing messages to which AWS Lambda function is the subscriber and then AWS Lambda pushes the data into Dynamo Db  In this  I am doing some transformation of messages in AWS Lambda  For the transformation  I have to fetch some rules from some place  These rules are basically the mapping between fields of the original messages to fields to transformed messages   So  basically I am saying that id should be mapped to id  house to home and so on   So  I want to keep this mapping at some places like Dynamo Db or some config service  I do not want to directly keep it in aws lambda code as there is a chance that I might have to change  But keeping it in Dynamo Db will be very costly in terms of latency I think because I will make a call on each message request  So  can anyone suggest  any aws resource which can be used for keeping these configs which is very fast and normally used for keeping configuration  
57187771,1,57188088.0,,2019-07-24 16:46:43,,2,6029,"<p>I have an AWS account with a Postgres RDS database that represents the production environment for an app.  We have another team that is building an analytics infrastructure in a different AWS account.  They need to be able to pull data from our production database to hydrate their reports.</p>

<p>From my research so far, it seems there are a couple options:</p>

<ol>
<li><p>Create a bash script that runs on a CRON schedule that uses <code>pg_dump</code> and <code>pg_restore</code> and stash that on an EC2 instance in one of the accounts.</p></li>
<li><p>Automate the process of creating a Snapshot on a schedule and then ship that to the other accounts S3 bucket.  Then create a Lambda (or other script) that triggers when the snapshot is placed in the S3 bucket and restore it.  Downside to this is we'd have to create a new RDS instance with each restore (since you can't restore a Snapshot to an existing instance), which changes the FQDN of the database (which we can mitigate using Route53 and a CNAME that gets updated, but this is complicated).</p></li>
<li><p>Create a read-replica in the origin AWS account and open up security for that instance so they can just access it directly (but then my account is responsible for all the costs associated with hosting and accessing it).</p></li>
</ol>

<p>None of these seem like good options.  Is there some other way to accomplish this?</p>
",2719094.0,,,,,2019-07-24 17:11:19,Create an RDS/Postgres Replica in another AWS account?,<postgresql><amazon-web-services><aws-lambda><amazon-rds>,1,0,1.0,,,CC BY-SA 4.0,I have an AWS account with a Postgres RDS database that represents the production environment for an app   We have another team that is building an analytics infrastructure in a different AWS account   They need to be able to pull data from our production database to hydrate their reports  From my research so far  it seems there are a couple options:  Create a bash script that runs on a CRON schedule that uses  and  and stash that on an EC2 instance in one of the accounts  Automate the process of creating a Snapshot on a schedule and then ship that to the other accounts S3 bucket   Then create a Lambda (or other script) that triggers when the snapshot is placed in the S3 bucket and restore it   Downside to this is wed have to create a new RDS instance with each restore (since you cant restore a Snapshot to an existing instance)  which changes the FQDN of the database (which we can mitigate using Route53 and a CNAME that gets updated  but this is complicated)  Create a read-replica in the origin AWS account and open up security for that instance so they can just access it directly (but then my account is responsible for all the costs associated with hosting and accessing it)   None of these seem like good options   Is there some other way to accomplish this  
62093781,1,,,2020-05-29 19:59:13,,2,608,"<p>I want to run a Tensorflow 2 LinearClassifier on AWS Lambda. I have a trained LinearClassifier  tensorflow model. I want to run only the prediction in AWS Lambda, because of cost efficeny.</p>

<p>Currently I'm not sure how to handle it.</p>

<ul>
<li>I tried to use normal TF2, but it is too big for AWS Lambda Layers. All descriptions you find are with 
older TF 1.1x versions and Python 2.x.</li>
<li>Then I tried TensorflowJS, but it looks like the LinearClassifier using functions which are not implemented in TFJS.</li>
<li>Last I tried to user Tensorflow Lite, but there I currently have problems to convert my model to TFLite.</li>
</ul>

<p><strong>Do you have an idea, how I can get TF 2 Estimation of LinearClassifier running on AWS Lambda?</strong></p>

<p>Normally I think not all of the 1.5 GB Tensorflow modules are used for the simple prediction of an existing model. So is there a possibility to only get the files which are used for running the script?</p>
",13343416.0,,,,,2021-02-25 12:44:52,Run Tensorflow 2 prediction on AWS Lambda,<python><amazon-web-services><tensorflow><aws-lambda>,1,2,,,,CC BY-SA 4.0,I want to run a Tensorflow 2 LinearClassifier on AWS Lambda  I have a trained LinearClassifier  tensorflow model  I want to run only the prediction in AWS Lambda  because of cost efficeny  Currently Im not sure how to handle it   I tried to use normal TF2  but it is too big for AWS Lambda Layers  All descriptions you find are with  older TF 1 1x versions and Python 2 x  Then I tried TensorflowJS  but it looks like the LinearClassifier using functions which are not implemented in TFJS  Last I tried to user Tensorflow Lite  but there I currently have problems to convert my model to TFLite   Do you have an idea  how I can get TF 2 Estimation of LinearClassifier running on AWS Lambda  Normally I think not all of the 1 5 GB Tensorflow modules are used for the simple prediction of an existing model  So is there a possibility to only get the files which are used for running the script  
57827691,1,,,2019-09-06 19:39:43,,3,483,"<p><strong>Kindly note answers need to work in an AWS Lambda - as such this is not a generic MySQL question.</strong></p>

<p>I have a pandas dataframe in AWS Lambda. It contains 50k rows and I need to load this into Amazon Aurora (MySQL). </p>

<p>Using <code>df.to_sql (even with method='multi')</code> is woefully slow.</p>

<p>The best way to do it would be to save a .csv, and then use the superfast MYSQL     <code>LOAD DATA LOCAL INFILE</code> command.</p>

<p>I save the df to a csv in lambdas \tmp\foo.csv but then <code>con.execute(""LOAD DATA LOCAL INFILE..."")</code> Doesn't work.</p>

<p>I get an SQL error</p>

<blockquote>
  <p>Access denied for user...</p>
</blockquote>

<p>I log in to the db as root, but can't grant. The MySQL flag is set to local-infile=1.</p>

<p>So, in summary: How can I FAST load 50K rows from a csv to MySQL Aurora db in AWS Lambda?</p>

<p><strong>Discounted Solution</strong></p>

<p>One solution I've already discounted is to export the csv to a S3 bucket, and then use the Amazon's  ""LOAD DATA FROM S3"". This seems terribly inefficient (not to mention all that S3 transfer/storage cost).</p>
",8297764.0,,4420967.0,,2019-09-08 06:02:06,2019-09-08 06:02:06,Fast Load 50k rows from csv or pandas to Amazon Aurora (MySQL),<mysql><pandas><amazon-web-services><aws-lambda><amazon-aurora>,0,2,,,,CC BY-SA 4.0,Kindly note answers need to work in an AWS Lambda - as such this is not a generic MySQL question  I have a pandas dataframe in AWS Lambda  It contains 50k rows and I need to load this into Amazon Aurora (MySQL)   Using  is woefully slow  The best way to do it would be to save a  csv  and then use the superfast MYSQL      command  I save the df to a csv in lambdas \tmp\foo csv but then  Doesnt work  I get an SQL error  Access denied for user     I log in to the db as root  but cant grant  The MySQL flag is set to local-infile=1  So  in summary: How can I FAST load 50K rows from a csv to MySQL Aurora db in AWS Lambda  Discounted Solution One solution Ive already discounted is to export the csv to a S3 bucket  and then use the Amazons  LOAD DATA FROM S3  This seems terribly inefficient (not to mention all that S3 transfer/storage cost)  
58286081,1,,,2019-10-08 12:09:33,,4,1247,"<p>I'm just learning AWS CDK after playing around with Serverless for a bit.</p>

<p>Serverless has a component to <a href=""https://github.com/serverless-components/website"" rel=""nofollow noreferrer"">deploy a static website</a>, which uses S3 and CloudFront. It updates an existing CloudFront distribution if it <a href=""https://github.com/serverless-components/domain/blob/master/utils.js#L395"" rel=""nofollow noreferrer"">finds one</a> for the same domain. Presumably the reason why it does this is so you don't have to wait 40 minutes while the CloudFront distribution is set up. I can't think of any other reason for it, e.g. it would seem to cost the same.</p>

<p>So how do you search for and re-use an existing CloudFront distribution in CDK? Should you actually just create a new one?</p>
",632636.0,,,,,2021-04-01 21:23:00,Re-use existing CloudFront distribution with AWS CDK,<amazon-web-services><amazon-cloudfront><serverless-framework><serverless><aws-cdk>,2,0,,,,CC BY-SA 4.0,Im just learning AWS CDK after playing around with Serverless for a bit  Serverless has a component to   which uses S3 and CloudFront  It updates an existing CloudFront distribution if it  for the same domain  Presumably the reason why it does this is so you dont have to wait 40 minutes while the CloudFront distribution is set up  I cant think of any other reason for it  e g  it would seem to cost the same  So how do you search for and re-use an existing CloudFront distribution in CDK  Should you actually just create a new one  
57220692,1,,,2019-07-26 13:05:27,,1,605,"<p>I would like to get the usage cost report of each instance in my aws account form a period of time.</p>

<p>I'm able to get linked_account_id and service in the output but I need instance_id as well. Please help</p>

<pre><code>import argparse
import boto3
import datetime

cd = boto3.client('ce', 'ap-south-1')

results = []

token = None
while True:
    if token:
        kwargs = {'NextPageToken': token}
    else:
        kwargs = {}
    data = cd.get_cost_and_usage(
    TimePeriod={'Start': '2019-01-01', 'End':  '2019-06-30'},
    Granularity='MONTHLY',
    Metrics=['BlendedCost','UnblendedCost'],
    GroupBy=[
                {'Type': 'DIMENSION', 'Key': 'LINKED_ACCOUNT'},
                {'Type': 'DIMENSION', 'Key': 'SERVICE'}
             ], **kwargs)
    results += data['ResultsByTime']
    token = data.get('NextPageToken')
    if not token:
        break
print('\t'.join(['Start_date', 'End_date', 'LinkedAccount', 'Service', 'blended_cost','unblended_cost', 'Unit', 'Estimated']))
for result_by_time in results:
    for group in result_by_time['Groups']:
        blended_cost = group['Metrics']['BlendedCost']['Amount']
        unblended_cost = group['Metrics']['UnblendedCost']['Amount']
        unit = group['Metrics']['UnblendedCost']['Unit']
        print(result_by_time['TimePeriod']['Start'], '\t',
        result_by_time['TimePeriod']['End'],'\t',
        '\t'.join(group['Keys']), '\t',
        blended_cost,'\t',
        unblended_cost, '\t',
        unit, '\t',
        result_by_time['Estimated'])
</code></pre>
",8937036.0,,,,,2021-12-22 11:00:24,Aws cost and usage for all the instances,<amazon-web-services><aws-lambda><boto3>,2,0,,,,CC BY-SA 4.0,I would like to get the usage cost report of each instance in my aws account form a period of time  Im able to get linked_account_id and service in the output but I need instance_id as well  Please help  
57224135,1,57527480.0,,2019-07-26 16:46:22,,4,278,"<p>I have a serverless app running as google cloud function triggered by bucket object finalize.
at the end of the function logic I want to call another action (also function) after exactly one minute (or T time).
currently couldn't come up with any way to call another action in one minute and had to use sleep in my app.</p>

<p>the problem with sleep is that I have 60 seconds that the cloud function cost money while no real work is being done.</p>

<p>any suggestion on how to execute something from cloud function in T time  so I can just exit from function and save money?</p>

<p>keeping in mind I would like to keep it serverless and using GCP.</p>
",1595262.0,,,,,2019-08-16 15:40:00,how to schedule something from cloud function,<go><google-cloud-platform><sleep><serverless><serverless-architecture>,1,5,,,,CC BY-SA 4.0,I have a serverless app running as google cloud function triggered by bucket object finalize  at the end of the function logic I want to call another action (also function) after exactly one minute (or T time)  currently couldnt come up with any way to call another action in one minute and had to use sleep in my app  the problem with sleep is that I have 60 seconds that the cloud function cost money while no real work is being done  any suggestion on how to execute something from cloud function in T time  so I can just exit from function and save money  keeping in mind I would like to keep it serverless and using GCP  
40724369,1,,,2016-11-21 16:02:44,,0,1484,"<p>We are using SQS queues for asynchronous messages and need Lambda functions to do some transformations and logging of the messages on certain queues.
After a lot of research I have decided I will go for a recursive Lambda function as the messages are not really critical and going through using SNS or SWF in between seems overly complicated (and I'm hoping Amazon soon will add a Lambda trigger for SQS).</p>

<p>The maximum execution duration per request for a Lambda function is supposed to be 300 seconds (5 minutes) so I figured to invoke the Lambda over-and-over and then have a Cloudwatch trigger set to 5 minutes to re-trigger the Lambda for another 5 minute run.</p>

<p>However the Lambda just keep running (without the Cloudwatch trigger). I tested it yesterday and was surprised when it kept going past the 300 seconds and now it has been running for over 24 hours...</p>

<p>So, question is, how come it keeps running?
I assume that each time I invoke it Lambda considers it a new request. As the SQS long-poll time-out is 20 seconds and I invoke also after time-out (in case of no new message) it keeps on going as new requests, right?</p>

<p>Also, if I add the Cloudwatch trigger at 5 minutes interval, will I then start multiple instances of the same Lambda function?</p>

<p>(And yes, I am aware I am being billed for the run time but it is still cheaper than an EC2 instance even running 24/7)</p>

<p><strong>EDIT</strong>:
Adding Cloudwatch logs that shows the invokation and recursive running:</p>

<blockquote>
  <p>15:48:22
  <strong>START</strong> RequestId: ee3f71df-b001-11e6-a0d6-bffc6057d58c Version: $LATEST</p>
  
  <p>15:48:42
  2016-11-21T15:48:42.188Z    ee3f71df-b001-11e6-a0d6-bffc6057d58c    Calling again... and again...</p>
  
  <p>15:48:42
  END RequestId: ee3f71df-b001-11e6-a0d6-bffc6057d58c</p>
  
  <p>15:48:42
  REPORT RequestId: ee3f71df-b001-11e6-a0d6-bffc6057d58c  Duration: 20115.93 ms   Billed Duration: 20200 ms Memory Size: 128 MB   Max Memory Used: 37 MB</p>
  
  <p>15:48:42
  <strong>START</strong> RequestId: fa443a44-b001-11e6-bea9-4fe2d7bd8fe7 Version: $LATEST</p>
  
  <p>15:49:02
  2016-11-21T15:49:02.386Z    fa443a44-b001-11e6-bea9-4fe2d7bd8fe7    Calling again... and again...</p>
  
  <p>15:49:02
  END RequestId: fa443a44-b001-11e6-bea9-4fe2d7bd8fe7</p>
  
  <p>15:49:02
  REPORT RequestId: fa443a44-b001-11e6-bea9-4fe2d7bd8fe7  Duration: 20156.93 ms   Billed Duration: 20200 ms Memory Size: 128 MB   Max Memory Used: 37 MB</p>
  
  <p>15:49:02
  <strong>START</strong> RequestId: 0647caad-b002-11e6-adc9-73ebc92281fd Version: $LATEST</p>
  
  <p>15:49:22
  2016-11-21T15:49:22.601Z    0647caad-b002-11e6-adc9-73ebc92281fd    Calling again... and again...</p>
  
  <p>15:49:22
  END RequestId: 0647caad-b002-11e6-adc9-73ebc92281fd</p>
  
  <p>15:49:22
  REPORT RequestId: 0647caad-b002-11e6-adc9-73ebc92281fd  Duration: 20179.49 ms   Billed Duration: 20200 ms Memory Size: 128 MB   Max Memory Used: 37 MB</p>
</blockquote>
",1254707.0,,1254707.0,,2016-11-21 16:54:46,2018-04-16 22:01:13,AWS Lambda recursive invocation works too good?,<node.js><amazon-web-services><aws-lambda>,1,7,1.0,,,CC BY-SA 3.0,We are using SQS queues for asynchronous messages and need Lambda functions to do some transformations and logging of the messages on certain queues  After a lot of research I have decided I will go for a recursive Lambda function as the messages are not really critical and going through using SNS or SWF in between seems overly complicated (and Im hoping Amazon soon will add a Lambda trigger for SQS)  The maximum execution duration per request for a Lambda function is supposed to be 300 seconds (5 minutes) so I figured to invoke the Lambda over-and-over and then have a Cloudwatch trigger set to 5 minutes to re-trigger the Lambda for another 5 minute run  However the Lambda just keep running (without the Cloudwatch trigger)  I tested it yesterday and was surprised when it kept going past the 300 seconds and now it has been running for over 24 hours    So  question is  how come it keeps running  I assume that each time I invoke it Lambda considers it a new request  As the SQS long-poll time-out is 20 seconds and I invoke also after time-out (in case of no new message) it keeps on going as new requests  right  Also  if I add the Cloudwatch trigger at 5 minutes interval  will I then start multiple instances of the same Lambda function  (And yes  I am aware I am being billed for the run time but it is still cheaper than an EC2 instance even running 24/7) EDIT: Adding Cloudwatch logs that shows the invokation and recursive running:  15:48:22   START RequestId: ee3f71df-b001-11e6-a0d6-bffc6057d58c Version: $LATEST 15:48:42   2016-11-21T15:48:42 188Z    ee3f71df-b001-11e6-a0d6-bffc6057d58c    Calling again    and again    15:48:42   END RequestId: ee3f71df-b001-11e6-a0d6-bffc6057d58c 15:48:42   REPORT RequestId: ee3f71df-b001-11e6-a0d6-bffc6057d58c  Duration: 20115 93 ms   Billed Duration: 20200 ms Memory Size: 128 MB   Max Memory Used: 37 MB 15:48:42   START RequestId: fa443a44-b001-11e6-bea9-4fe2d7bd8fe7 Version: $LATEST 15:49:02   2016-11-21T15:49:02 386Z    fa443a44-b001-11e6-bea9-4fe2d7bd8fe7    Calling again    and again    15:49:02   END RequestId: fa443a44-b001-11e6-bea9-4fe2d7bd8fe7 15:49:02   REPORT RequestId: fa443a44-b001-11e6-bea9-4fe2d7bd8fe7  Duration: 20156 93 ms   Billed Duration: 20200 ms Memory Size: 128 MB   Max Memory Used: 37 MB 15:49:02   START RequestId: 0647caad-b002-11e6-adc9-73ebc92281fd Version: $LATEST 15:49:22   2016-11-21T15:49:22 601Z    0647caad-b002-11e6-adc9-73ebc92281fd    Calling again    and again    15:49:22   END RequestId: 0647caad-b002-11e6-adc9-73ebc92281fd 15:49:22   REPORT RequestId: 0647caad-b002-11e6-adc9-73ebc92281fd  Duration: 20179 49 ms   Billed Duration: 20200 ms Memory Size: 128 MB   Max Memory Used: 37 MB  
58467621,1,58501266.0,,2019-10-19 20:15:10,,0,112,"<p>I'm new to OAuth2 and cloud-functions/serverless.</p>

<p>So I was wondering whether it makes sense to create cloud-functions to handle OAuth2 requests.</p>

<p>My Idea:</p>

<ol>
<li>User sends auth request to and API Gateway (to prevent cloud-function abuse, as of my understanding, or how else should that be prevented? Cloudflare?)</li>
<li>Gateway redirects request to cloud-function</li>
<li>Cloud-function stores user authentication in DB</li>
<li>User is now authenticated.</li>
<li>The authenticated user can now request actual data, like profile, through other cloud-functions.</li>
<li>Response with data to the user.</li>
</ol>

<p>Is this a correct understanding of how OAuth works? If so, does this make sense, or would a usual server be cheaper to handle OAuth?</p>
",9191773.0,,,,,2019-10-22 09:35:23,Cloudfunction/Serverless OAuth2 Client,<oauth><oauth-2.0><google-cloud-functions><serverless>,1,2,,,,CC BY-SA 4.0,Im new to OAuth2 and cloud-functions/serverless  So I was wondering whether it makes sense to create cloud-functions to handle OAuth2 requests  My Idea:  User sends auth request to and API Gateway (to prevent cloud-function abuse  as of my understanding  or how else should that be prevented  Cloudflare ) Gateway redirects request to cloud-function Cloud-function stores user authentication in DB User is now authenticated  The authenticated user can now request actual data  like profile  through other cloud-functions  Response with data to the user   Is this a correct understanding of how OAuth works  If so  does this make sense  or would a usual server be cheaper to handle OAuth  
58498009,1,,,2019-10-22 06:10:45,,0,210,"<p>Consider my Lambda package size is 100 mb (unzipped) and on its invocation, it uses say x mb. So will I be charged for the memory slot w.r.t (x + 100) mb OR only x mb?</p>
",4006496.0,,,,,2019-10-25 10:55:23,Is the AWS Lambda package size considered in memory allocated?,<amazon-web-services><aws-lambda>,2,0,,,,CC BY-SA 4.0,Consider my Lambda package size is 100 mb (unzipped) and on its invocation  it uses say x mb  So will I be charged for the memory slot w r t (x + 100) mb OR only x mb  
40961634,1,,,2016-12-04 17:45:37,,0,309,"<p>I was wondering if someone could help me out on the following points:-</p>

<ol>
<li>My aim here is to have a very cost effective architecture using Lambda along with the benefits of Relay to help with querying, caching, optimistic updates, etc. Is this architecture a good idea or am I overlooking something?</li>
<li>Are there any good examples of using Relay with Lambda?</li>
</ol>
",1424625.0,,,,,2017-01-31 13:20:42,Using Relay/GraphQL with AWS Lambda,<aws-lambda><relayjs><relay>,2,0,,,,CC BY-SA 3.0,I was wondering if someone could help me out on the following points:-  My aim here is to have a very cost effective architecture using Lambda along with the benefits of Relay to help with querying  caching  optimistic updates  etc  Is this architecture a good idea or am I overlooking something  Are there any good examples of using Relay with Lambda   
40989800,1,,,2016-12-06 07:15:12,,11,7375,"<p>As per current implementation by AWS, the response payload size returned by a Lambda function cannot exceed <code>6 mb</code>. Is there a provision for increasing this limit to <code>15 mb</code> by requesting a ""Service Limit Increase"". Also, what are the additional charges, if any that would be charged?</p>

<p>Thanks in advance!  </p>
",5143677.0,,,,,2016-12-06 13:57:25,Increase invoke Response payload size in AWS Lambda to 15 mb,<amazon-web-services><aws-lambda>,1,0,1.0,,,CC BY-SA 3.0,As per current implementation by AWS  the response payload size returned by a Lambda function cannot exceed   Is there a provision for increasing this limit to  by requesting a Service Limit Increase  Also  what are the additional charges  if any that would be charged  Thanks in advance    
58734531,1,58734991.0,,2019-11-06 16:22:41,,3,3071,"<p>I am exploring CloudWatch Logs streams to Firehose. As I understand so far, Cloudwatch Subscription Filter is an event that triggers a lambda to digest the CloudWatch logs and send it to a different destination (ElasticSearch or Firehose or ... another custom lambda). Please correct me if I am wrong. </p>

<p>My concern at the case Cloudwatch Logs Stream to Firehose are: </p>

<p>1/ In term of performance + pricing, is there any difference between : </p>

<ul>
<li>Cloudwatch Subscription Filter -> Firehose </li>
<li>Cloudwatch Subscription Filter -> Lambda -> Firehose </li>
</ul>

<p>2/ Which data format does Firehose received from Cloudwatch?</p>

<ul>
<li>Cloudwatch Subscription Filter -> Firehose : I don't know</li>
<li>Cloudwatch Subscription Filter -> Lambda -> Firehose : I think lambda can transform the logs to JSON then put it to Firehose.</li>
</ul>

<p>Any suggestion is appreciated.</p>
",1845475.0,,1219052.0,,2021-03-15 14:04:57,2021-03-15 14:04:57,CloudWatch Subscription Filter: using lambda or direct subscription,<amazon-web-services><aws-lambda><amazon-cloudwatch><amazon-cloudwatchlogs><amazon-kinesis-firehose>,1,4,,,,CC BY-SA 4.0,I am exploring CloudWatch Logs streams to Firehose  As I understand so far  Cloudwatch Subscription Filter is an event that triggers a lambda to digest the CloudWatch logs and send it to a different destination (ElasticSearch or Firehose or     another custom lambda)  Please correct me if I am wrong   My concern at the case Cloudwatch Logs Stream to Firehose are:  1/ In term of performance + pricing  is there any difference between :   Cloudwatch Subscription Filter -&gt; Firehose  Cloudwatch Subscription Filter -&gt; Lambda -&gt; Firehose   2/ Which data format does Firehose received from Cloudwatch   Cloudwatch Subscription Filter -&gt; Firehose : I dont know Cloudwatch Subscription Filter -&gt; Lambda -&gt; Firehose : I think lambda can transform the logs to JSON then put it to Firehose   Any suggestion is appreciated  
58951709,1,,,2019-11-20 10:10:17,,2,513,"<p>Hey I am getting started with the serverless framework, apigateway, lambdas and authorizers.</p>

<p>Here my questions:</p>

<ol>
<li><p>In order to verify a proper JWT token (which seems nowadays the best solution for serverless authentication), I would need a client id. Since the authorizer lambda is not capable(?) of reaching any other parameters of the request into the lambda except for the token itself, this is a difficult task to do. How can I achieve this?</p></li>
<li><p>Since with every authenticated call, an additional lambda is called, my costs are doubled?! May be I am misunderstanding something here, but it seems to me like implementing a method for verifying my token without using the authorizer is cheaper and I don't need to implement the authorizer lambda itself.</p></li>
</ol>
",824612.0,,,,,2019-11-21 15:31:29,Serverless AWS - is it worth using custom authorizers (as a lambda)?,<amazon-web-services><jwt><serverless-framework><lambda-authorizer>,2,8,1.0,,,CC BY-SA 4.0,Hey I am getting started with the serverless framework  apigateway  lambdas and authorizers  Here my questions:  In order to verify a proper JWT token (which seems nowadays the best solution for serverless authentication)  I would need a client id  Since the authorizer lambda is not capable( ) of reaching any other parameters of the request into the lambda except for the token itself  this is a difficult task to do  How can I achieve this  Since with every authenticated call  an additional lambda is called  my costs are doubled   May be I am misunderstanding something here  but it seems to me like implementing a method for verifying my token without using the authorizer is cheaper and I dont need to implement the authorizer lambda itself   
58953674,1,58953743.0,,2019-11-20 11:48:40,,1,40,"<p>I have a AWS Lambda function that I invoke with every 1 minute with >1000 SNS events. This is a problem because my account concurrency is set at 3000, so if I start adding more jobs then eventually I'm going to have >3000 concurrent Lambda instances.</p>

<p>Each job takes around 2-5 seconds to complete which means that within each 1 minute window the concurrency limit will only be threatened within the first 5 seconds and I'll have 0 concurrency for the remaining 55 seconds.</p>

<p>If I set a concurrency limit (e.g. 1000) for the lambda will it handle the first 1000 SNS events and then automatically pick up the remainder once the concurrency frees up? And will I only be charged for the actual runtime rather than time spent waiting for concurrency to reduce?</p>

<p>Otherwise, is there a way that AWS will allow me to spread the load of jobs throughout the 1 minute window so that I can invoke the lambda every ~5 seconds with a subset of the total number of jobs?</p>
",5324979.0,,,,,2019-11-20 11:52:11,AWS Lambda temporal load balancing,<concurrency><aws-lambda>,1,0,,,,CC BY-SA 4.0,I have a AWS Lambda function that I invoke with every 1 minute with &gt;1000 SNS events  This is a problem because my account concurrency is set at 3000  so if I start adding more jobs then eventually Im going to have &gt;3000 concurrent Lambda instances  Each job takes around 2-5 seconds to complete which means that within each 1 minute window the concurrency limit will only be threatened within the first 5 seconds and Ill have 0 concurrency for the remaining 55 seconds  If I set a concurrency limit (e g  1000) for the lambda will it handle the first 1000 SNS events and then automatically pick up the remainder once the concurrency frees up  And will I only be charged for the actual runtime rather than time spent waiting for concurrency to reduce  Otherwise  is there a way that AWS will allow me to spread the load of jobs throughout the 1 minute window so that I can invoke the lambda every ~5 seconds with a subset of the total number of jobs  
41479792,1,,,2017-01-05 07:50:41,,0,50,"<p>I am just looking for ideas on how to solve one specific thing I'd like to build.</p>

<p>Say I have two sets of items. Each item is just a couple of lines of JSON. Any time an item is added to one set I immediately (well, almost) want to process this against the full other set. So item is added to set A: Process against each item in set B. And vice versa.</p>

<p>Items come in through API Gateway + Lambda. Match processing in Lambda from a queue/stream.</p>

<p>What AWS technology would be a good fit? I have no idea and no clear pattern on when or how often the sets change. Also, I want it to be as strongly consistent as possible. And of course, I want it to be as serverless and cost-effective as possible. :)</p>

<p>Options could be:</p>

<ul>
<li>sets stored in Aurora, match processing for a new item in A would need to query the full set B from the database each time</li>
<li>sets stored in DynamoDB, maybe with DynamoDB stream in the background; match processing for a new item in A would need to query the full set B from Dynamo; but spiky load, not a good fit because of unclear read/write provisioning</li>
<li>have each set in its own ""static"" Kinesis stream where match processing reads through items but doesn't trim. Streams to be replaced with fresh sets regularly</li>
</ul>

<p>My pain point is: While processing items from A there might be thousands of items in B to be matched. And I want to avoid having to load the full set B from some database every time I process an item from A. I was thinking about some caching of sets but then would need a good option to invalidate that cache whenever something changes. </p>
",7377898.0,,,,,2017-01-05 07:50:41,Reprocess batches of items over and over again - and the batch might change any time,<amazon-web-services><amazon-dynamodb><aws-lambda><amazon-rds><amazon-aurora>,0,2,,,,CC BY-SA 3.0,I am just looking for ideas on how to solve one specific thing Id like to build  Say I have two sets of items  Each item is just a couple of lines of JSON  Any time an item is added to one set I immediately (well  almost) want to process this against the full other set  So item is added to set A: Process against each item in set B  And vice versa  Items come in through API Gateway + Lambda  Match processing in Lambda from a queue/stream  What AWS technology would be a good fit  I have no idea and no clear pattern on when or how often the sets change  Also  I want it to be as strongly consistent as possible  And of course  I want it to be as serverless and cost-effective as possible  :) Options could be:  sets stored in Aurora  match processing for a new item in A would need to query the full set B from the database each time sets stored in DynamoDB  maybe with DynamoDB stream in the background; match processing for a new item in A would need to query the full set B from Dynamo; but spiky load  not a good fit because of unclear read/write provisioning have each set in its own static Kinesis stream where match processing reads through items but doesnt trim  Streams to be replaced with fresh sets regularly  My pain point is: While processing items from A there might be thousands of items in B to be matched  And I want to avoid having to load the full set B from some database every time I process an item from A  I was thinking about some caching of sets but then would need a good option to invalidate that cache whenever something changes   
59452548,1,59452929.0,,2019-12-23 09:02:10,,7,2033,"<p>AWS allows a Lambda function to be triggered by an SQS queue. With respect to the <a href=""https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html"" rel=""noreferrer"">documentation</a>, Lambda polls the queue and invokes the function synchronously with a batch of messages.</p>

<p>AWS takes the execution time into account while charging for a Lambda function. Does the polling time counted as the execution time and be charged? Or is it free?</p>
",478831.0,,,,,2019-12-23 09:37:39,Does AWS charge polling time of a Lambda function triggered by SQS?,<amazon-web-services><aws-lambda>,1,0,,,,CC BY-SA 4.0,AWS allows a Lambda function to be triggered by an SQS queue  With respect to the   Lambda polls the queue and invokes the function synchronously with a batch of messages  AWS takes the execution time into account while charging for a Lambda function  Does the polling time counted as the execution time and be charged  Or is it free  
41789192,1,,,2017-01-22 09:13:01,,3,2475,"<p>How to build serverless web site on Azure, as ""auto-scaling, pay-per-execution, event-driven app""?</p>

<p>There are tons of good examples how to build serverless-arhitecture web site on AWS Amazon, e.g. <a href=""https://zanon.io/posts/building-serverless-websites-on-aws-tutorial"" rel=""nofollow noreferrer"">https://zanon.io/posts/building-serverless-websites-on-aws-tutorial</a>
It consumes <strong>S3</strong> for HTML and JS, <strong>Lambda</strong> for REST API, <strong>Simple DB</strong> for data.</p>

<p>Microsoft has Azure Functions that is analogue for AWS Lambda, but it is ""serverless computing"", not ""serverless web site"". I can create ""serverless REST API"" with Azure Functions, but what about HTML, JS, CSS for web site, database, etc. ?</p>

<p>I tried Azure App Service, but it lacks ""pay only for what you use"" option, as all plans have Monthly payments, as well as Azure SQL for database. And App Service doesn't seem to be constructed to host serverless-architecture web sites, more for classic ASP.NET web sites that you can easily deploy there.</p>

<p>Also, there is popular library <a href=""https://github.com/serverless/serverless"" rel=""nofollow noreferrer"">https://github.com/serverless/serverless</a> and they even mentioned Azure: ""using AWS Lambda, Azure Functions, Google CloudFunctions &amp; more"", but there is no a single example how to use it for Azure and all Docs are for Amazon AWS.</p>

<p>Thanks!</p>
",1972303.0,,1972303.0,,2017-01-22 09:45:14,2018-07-15 03:15:06,How to build serverless web site on Azure (rather then on AWS Amazon),<azure><aws-lambda><azure-functions><serverless-framework><serverless-architecture>,3,1,2.0,,,CC BY-SA 3.0,How to build serverless web site on Azure  as auto-scaling  pay-per-execution  event-driven app  There are tons of good examples how to build serverless-arhitecture web site on AWS Amazon  e g   It consumes S3 for HTML and JS  Lambda for REST API  Simple DB for data  Microsoft has Azure Functions that is analogue for AWS Lambda  but it is serverless computing  not serverless web site  I can create serverless REST API with Azure Functions  but what about HTML  JS  CSS for web site  database  etc    I tried Azure App Service  but it lacks pay only for what you use option  as all plans have Monthly payments  as well as Azure SQL for database  And App Service doesnt seem to be constructed to host serverless-architecture web sites  more for classic ASP NET web sites that you can easily deploy there  Also  there is popular library  and they even mentioned Azure: using AWS Lambda  Azure Functions  Google CloudFunctions &amp; more  but there is no a single example how to use it for Azure and all Docs are for Amazon AWS  Thanks  
59661316,1,59661556.0,,2020-01-09 09:53:57,,2,48,"<p>I can choose to pay more to have <strong>dedicated</strong> AWS EC2 instances so that my VMs are <strong>physically</strong> isolated from other people's instances.</p>

<p>However, using EC2 also means I bear the responsibility of <strong>maintenance</strong>, either through automation or not.</p>

<p>So I would like to use things like <strong>Fargate</strong> and <strong>Lambda</strong>, which removes the maintenance burden from me.</p>

<p>Is possible to still have the <strong>same level of hardware isolation</strong>?</p>

<p>Can I require Amazon to run my <strong>Lambda</strong> functions and <strong>Fargate</strong> containers in a <strong>physically</strong> <strong>isolated</strong> fashion?</p>
",290284.0,,,,,2020-01-09 10:06:59,Is it possible to have hardware level isolation if I choose to use the Serverless goodies of AWS?,<amazon-web-services><aws-lambda><serverless><isolation>,1,0,,,,CC BY-SA 4.0,I can choose to pay more to have dedicated AWS EC2 instances so that my VMs are physically isolated from other peoples instances  However  using EC2 also means I bear the responsibility of maintenance  either through automation or not  So I would like to use things like Fargate and Lambda  which removes the maintenance burden from me  Is possible to still have the same level of hardware isolation  Can I require Amazon to run my Lambda functions and Fargate containers in a physically isolated fashion  
43113198,1,,,2017-03-30 09:04:47,,1,1628,"<p>I am thinking to use AWS API Gateway and AWS Lambda(Python) to create a serverless API's , but while designing this i was thinking of some aspects like pagination,security,caching,versioning ..etc </p>

<p><strong>so my question is:</strong> 
What is the best approach performance &amp; cost wise to implement API pagination with very big data (1 million records)? </p>

<ol>
<li>should i implement the pagination in postgresql db? (i think this
would be slow)</li>
<li>should i not use postgresql db pagination and just cache all the results i get from db into aws elastic cache and then do server side pagination in lambda.</li>
</ol>

<p>I appreciate your help guys.  </p>
",4663398.0,,,,,2017-03-30 19:32:33,AWS API Gateway & Lambda - API Pagination,<python><postgresql><amazon-web-services><aws-lambda><aws-api-gateway>,1,0,1.0,,,CC BY-SA 3.0,I am thinking to use AWS API Gateway and AWS Lambda(Python) to create a serverless APIs   but while designing this i was thinking of some aspects like pagination security caching versioning   etc  so my question is:  What is the best approach performance &amp; cost wise to implement API pagination with very big data (1 million records)    should i implement the pagination in postgresql db  (i think this would be slow) should i not use postgresql db pagination and just cache all the results i get from db into aws elastic cache and then do server side pagination in lambda   I appreciate your help guys    
42320204,1,42320922.0,,2017-02-18 20:15:52,,1,609,"<p>I'm interested in hosting a website for a small business  (&lt; 100 users / month) and I wanted to try going 'serverless'. I've read that using Amazon S3, Lambda and DynamoDB is a way to set this up, by hosting the front-end on S3, using Lambda functions to access the back-end, and storing data in DynamoDB. I'll need to run a script on page load to get data to display, save user profiles/allow logins, and acccept payments using Stripe or Braintree. </p>

<p>Is this a good situation to use this setup, or am I better off just using EC2 with a LAMP stack? Which is better in terms of cost?</p>
",5022615.0,,,,,2017-02-18 21:32:33,Amazon S3 + Lambda + DynamoDB Website Hosting,<amazon-web-services><amazon-s3><amazon-ec2><stripe-payments><aws-lambda>,1,0,,,,CC BY-SA 3.0,Im interested in hosting a website for a small business  (&lt; 100 users / month) and I wanted to try going serverless  Ive read that using Amazon S3  Lambda and DynamoDB is a way to set this up  by hosting the front-end on S3  using Lambda functions to access the back-end  and storing data in DynamoDB  Ill need to run a script on page load to get data to display  save user profiles/allow logins  and acccept payments using Stripe or Braintree   Is this a good situation to use this setup  or am I better off just using EC2 with a LAMP stack  Which is better in terms of cost  
42333929,1,,,2017-02-19 22:56:37,,3,350,"<p>I have AWS DynamoDB table called ""Users"", whose hash key/primary key is ""UserID"" which consist of emails. It has two attributes, first called ""Daily Points"" and second ""TimeSpendInTheApp"". Now I need to run a query or scan on the table, that will give me top 50 users which have the highest points and top 50 users which have spend the most time in the app. Now this query will be executed only once a day by cron aws lambda. I am trying to find the best solutions for this query or scan. For me, the cost is most important than speed/or efficiency. As maintaining secondary global index or a local index on points can be costly operations, as I have to assign Read and Write units for those indexes, which I want to avoid.  ""Users"" table will have a maximum of 100,000 to 150,000 records and on average it will have 50,000 records. What are my best options? Please suggest.</p>

<p>I am thinking, my first option is, I can scan the whole table on Filter Expression for records above certain points (5000 for example), after this scan, if 50 or more than 50 records are found, then simply sort the values and take the top 50 records. If this scan returns no or very less results then reduce the Filter Expression value (3000 for example), then again do the same scan operation. If Filter Expression value (2500 for example) returns too many records, like 5000 or more, then reduce the Filter Expression value. Is this even possible, I guess it would also need to handle pagination. Is it advisable to scan on a table which has 50,000 record? </p>

<p>Any advice or suggestion will be helpful. Thanks in advance. </p>
",1194577.0,,-1.0,,2017-09-22 18:01:22,2017-08-04 14:37:14,Scan on DynamDB table or Query on secondary global index or a local index (What's the best solution),<performance><amazon-dynamodb><aws-lambda><query-performance><nosql>,1,0,1.0,,,CC BY-SA 3.0,I have AWS DynamoDB table called Users  whose hash key/primary key is UserID which consist of emails  It has two attributes  first called Daily Points and second TimeSpendInTheApp  Now I need to run a query or scan on the table  that will give me top 50 users which have the highest points and top 50 users which have spend the most time in the app  Now this query will be executed only once a day by cron aws lambda  I am trying to find the best solutions for this query or scan  For me  the cost is most important than speed/or efficiency  As maintaining secondary global index or a local index on points can be costly operations  as I have to assign Read and Write units for those indexes  which I want to avoid   Users table will have a maximum of 100 000 to 150 000 records and on average it will have 50 000 records  What are my best options  Please suggest  I am thinking  my first option is  I can scan the whole table on Filter Expression for records above certain points (5000 for example)  after this scan  if 50 or more than 50 records are found  then simply sort the values and take the top 50 records  If this scan returns no or very less results then reduce the Filter Expression value (3000 for example)  then again do the same scan operation  If Filter Expression value (2500 for example) returns too many records  like 5000 or more  then reduce the Filter Expression value  Is this even possible  I guess it would also need to handle pagination  Is it advisable to scan on a table which has 50 000 record   Any advice or suggestion will be helpful  Thanks in advance   
43200113,1,,,2017-04-04 06:46:22,,0,26,"<p>I'm building a site and API for my own personal purposes, and I figured I'd use API Gateway and Lambda functions to keep costs down.</p>

<p>I need a way to authenticate, and using my Google or GitHub account with OAuth seems to be a good idea. </p>

<p>However, since:</p>

<ol>
<li>There's only going to be one user for the foreseeable: me.</li>
<li>I'd like to be able to build command-line tools around these APIs I'm building.</li>
</ol>

<p>I'm not sure that OAuth is the right approach. Is there a way to limit who can sign up/sign in with OAuth as an authentication backend? Additionally, should I add more allowed users, is it easy to build authorization around an OAuth based system?</p>
",128967.0,,,,,2017-04-04 15:55:48,OAuth for a Single Allowed User via API Gateway and Lambda,<oauth><aws-lambda><aws-api-gateway>,1,0,,,,CC BY-SA 3.0,Im building a site and API for my own personal purposes  and I figured Id use API Gateway and Lambda functions to keep costs down  I need a way to authenticate  and using my Google or GitHub account with OAuth seems to be a good idea   However  since:  Theres only going to be one user for the foreseeable: me  Id like to be able to build command-line tools around these APIs Im building   Im not sure that OAuth is the right approach  Is there a way to limit who can sign up/sign in with OAuth as an authentication backend  Additionally  should I add more allowed users  is it easy to build authorization around an OAuth based system  
62356635,1,,,2020-06-13 07:05:37,,0,34,"<p>I have a use case where an API backed by a lambda has to be latency critical for a few clients but there are clients how call the API with high volume in bursts and the latency restrictions are liberal . </p>

<p>We are using provisioned concurrency for the latency critical calls and do not want to use it for non latency critical calls as the cost is high. </p>

<p>Since provisioned concurrency can only be used with alias/version, is it possible to choose the lambda version at runtime based on the API Key?</p>

<p>Determine the client based on the API Key and point to the appropriate version. I am trying to avoid creating 2 API endpoints one for latency critical clients and the other for non-latency critical clients. </p>
",5099532.0,,,,,2020-06-13 08:07:28,How to use selectively choose the lambda version for an APIG API at runtime?,<aws-lambda><aws-api-gateway>,1,0,,,,CC BY-SA 4.0,I have a use case where an API backed by a lambda has to be latency critical for a few clients but there are clients how call the API with high volume in bursts and the latency restrictions are liberal    We are using provisioned concurrency for the latency critical calls and do not want to use it for non latency critical calls as the cost is high   Since provisioned concurrency can only be used with alias/version  is it possible to choose the lambda version at runtime based on the API Key  Determine the client based on the API Key and point to the appropriate version  I am trying to avoid creating 2 API endpoints one for latency critical clients and the other for non-latency critical clients   
62750324,1,,,2020-07-06 06:22:35,,4,2455,"<p>Serving APIs from <code>Lambda@Edge</code> offers more latency benefit than serving APIs from <code>API-Gateway + Lambda</code> stack, if my understanding is correct.</p>
<p>Plus, cost of API-Gateway ($3.5/million call) + Lambda ($0.2/million call) == $3.7 / million call seems to be more expensive than <code>Lambda@Edge</code> ($0.6 / million call).</p>
<p>If both of the above observations are true, shouldn't we all migrate our API-Gateway + Lambda (for those who use this stack) to lambda@edge stack?</p>
",152859.0,,13460933.0,,2020-07-06 06:27:23,2020-07-06 07:46:31,Shouldn't we all migrate to Lambda@Edge from API-Gateway + Lambda stack for API serving?,<amazon-web-services><aws-lambda><aws-api-gateway><amazon-cloudfront><aws-lambda-edge>,2,1,1.0,,,CC BY-SA 4.0,Serving APIs from  offers more latency benefit than serving APIs from  stack  if my understanding is correct  Plus  cost of API-Gateway ($3 5/million call) + Lambda ($0 2/million call) == $3 7 / million call seems to be more expensive than  ($0 6 / million call)  If both of the above observations are true  shouldnt we all migrate our API-Gateway + Lambda (for those who use this stack) to lambda@edge stack  
62207524,1,,,2020-06-05 03:14:45,,0,28,"<p>I am working on a AWS amplify application and I have an existing AWS API gateway(payments api, payments path) which points to a .Net lambda function(payments namespace, payments class, createPaymentIntent method name).</p>

<p>a. If I edit the .Net lambda function's payments class to add another public method, how do I expose it as an api? 
b. Similarly if I add another class(charges?) and a public method(ListAllCharges) to the .Net lambda function project, how do I expose it as an api? </p>

<p><a href=""https://i.stack.imgur.com/QprMs.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QprMs.jpg"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/CNDtC.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CNDtC.jpg"" alt=""enter image description here""></a></p>
",391918.0,,5424128.0,,2020-06-05 17:01:04,2020-06-05 17:01:04,Expose added public method in AWS lambda project as api,<.net><amazon-web-services><aws-lambda><aws-api-gateway><aws-amplify>,1,0,,,,CC BY-SA 4.0,I am working on a AWS amplify application and I have an existing AWS API gateway(payments api  payments path) which points to a  Net lambda function(payments namespace  payments class  createPaymentIntent method name)  a  If I edit the  Net lambda functions payments class to add another public method  how do I expose it as an api   b  Similarly if I add another class(charges ) and a public method(ListAllCharges) to the  Net lambda function project  how do I expose it as an api     
62759147,1,,,2020-07-06 15:17:11,,2,375,"<p>The following are the two key points.</p>
<ul>
<li>I have a mongo instance deployed in AWS Lightsail.</li>
<li>I have set of lambda functions(written in python) which need to communicate with this particular DB</li>
</ul>
<p>But every time I run the function (triggered from API Gateway) it times out.</p>
<p>The following are the things I have tried with no luck:</p>
<ul>
<li>Added VPCExecution IAM role to Lambda.</li>
<li>Tried opening the Lightsail instance to the public(0.0.0.0), in which communication happens(obviously), but is definitely not a recommended solution</li>
<li>Tried setting up a NAT Gateway with a static IP and whitelisting it in Lightsail's firewall. This also works but I cannot afford the cost.</li>
<li>Tried enabling the VPC Peering from Lighsail's account panel. Still no luck. (This is where I was hoping it to work)</li>
</ul>
<p>Is there anything I'm missing. I really don't need to go towards EC2. But if you can offer any advice that results in the same cost, that'd be great.</p>
<p>PS: I am able to connect to the instance after whitelisting my Public IP.</p>
<p>I really need help with this. Any response will be highly appreciated.</p>
",7369806.0,,7369806.0,,2020-07-06 15:47:17,2020-07-06 15:47:17,AWS - Unable to connect to mongo in Lightsail from Lambda,<mongodb><amazon-web-services><aws-lambda><amazon-lightsail>,0,8,,,,CC BY-SA 4.0,The following are the two key points   I have a mongo instance deployed in AWS Lightsail  I have set of lambda functions(written in python) which need to communicate with this particular DB  But every time I run the function (triggered from API Gateway) it times out  The following are the things I have tried with no luck:  Added VPCExecution IAM role to Lambda  Tried opening the Lightsail instance to the public(0 0 0 0)  in which communication happens(obviously)  but is definitely not a recommended solution Tried setting up a NAT Gateway with a static IP and whitelisting it in Lightsails firewall  This also works but I cannot afford the cost  Tried enabling the VPC Peering from Lighsails account panel  Still no luck  (This is where I was hoping it to work)  Is there anything Im missing  I really dont need to go towards EC2  But if you can offer any advice that results in the same cost  thatd be great  PS: I am able to connect to the instance after whitelisting my Public IP  I really need help with this  Any response will be highly appreciated  
62765780,1,,,2020-07-06 23:08:58,,1,558,"<p>I have created a model endpoint which is InService and deployed on an ml.m4.xlarge instance. I am also using API Gateway to create a RESTful API.</p>
<p>Questions:</p>
<ol>
<li><p>Is it possible to have my model endpoint only Inservice (or on standby) when I receive inference requests? Maybe by writing a lambda function or something that turns off the endpoint (so that it does not keep accumulating the per hour charges)</p>
</li>
<li><p>If q1 is possible, would this have some weird latency issues on the end users? Because it usually takes a couple of minutes for model endpoints to be created when I configure them for the first time.</p>
</li>
<li><p>If q1 is not possible, how would choosing a cheaper instance type affect the time it takes to perform inference (Say I'm only using the endpoints for an application that has a low number of users).</p>
</li>
</ol>
<p>I am aware of this site that compares different instance types (<a href=""https://aws.amazon.com/sagemaker/pricing/instance-types/"" rel=""nofollow noreferrer"">https://aws.amazon.com/sagemaker/pricing/instance-types/</a>)</p>
<p>But, does having a moderate network performance mean that the time to perform realtime inference may be longer?</p>
<p>Any recommendations are much appreciated. The goal is not to burn money when users are not requesting for predictions.</p>
",12655955.0,,,,,2020-12-09 20:47:38,Is there a way to turn on SageMaker model endpoints only when I am receiving inference requests,<amazon-web-services><aws-lambda><aws-api-gateway><amazon-sagemaker>,2,0,,,,CC BY-SA 4.0,I have created a model endpoint which is InService and deployed on an ml m4 xlarge instance  I am also using API Gateway to create a RESTful API  Questions:  Is it possible to have my model endpoint only Inservice (or on standby) when I receive inference requests  Maybe by writing a lambda function or something that turns off the endpoint (so that it does not keep accumulating the per hour charges)  If q1 is possible  would this have some weird latency issues on the end users  Because it usually takes a couple of minutes for model endpoints to be created when I configure them for the first time   If q1 is not possible  how would choosing a cheaper instance type affect the time it takes to perform inference (Say Im only using the endpoints for an application that has a low number of users)    I am aware of this site that compares different instance types () But  does having a moderate network performance mean that the time to perform realtime inference may be longer  Any recommendations are much appreciated  The goal is not to burn money when users are not requesting for predictions  
44613416,1,,,2017-06-18 09:04:49,,0,61,"<p>I'm considering building a serverless web API which uses API Gateway to receive a stream of JSON blobs. I'd like to archive every incoming blob (after some basic authentication and validation of course). What are your recommendations on how to do this?</p>

<p>Additional info:</p>

<ul>
<li>I'm using AWS Lambda reduce cost.</li>
<li>The archives will be accessed very infrequently, so I've been eyeballing S3 Glacier to reduce pricing. My issue is I need to figure out how to do batching of blobs per S3 file to avoid the overhead of many files.</li>
<li>Alternative storage services that I've been looking at are Cloudwatch logs and DynamoDB.</li>
</ul>
",260805.0,,,,,2017-06-18 09:56:42,Serverless approach to archive incoming JSON blobs?,<aws-lambda>,1,0,2.0,,,CC BY-SA 3.0,Im considering building a serverless web API which uses API Gateway to receive a stream of JSON blobs  Id like to archive every incoming blob (after some basic authentication and validation of course)  What are your recommendations on how to do this  Additional info:  Im using AWS Lambda reduce cost  The archives will be accessed very infrequently  so Ive been eyeballing S3 Glacier to reduce pricing  My issue is I need to figure out how to do batching of blobs per S3 file to avoid the overhead of many files  Alternative storage services that Ive been looking at are Cloudwatch logs and DynamoDB   
44614048,1,44615173.0,,2017-06-18 10:29:26,,0,37,"<p>I'm using AWS Lambda to consume from Kinesis. My Lambda function doesn't have any requirements on max concurrency. Is there any reason for me to not have the maximum possible number of shards for my stream? I can't see that number of shards would affect cost.</p>
",260805.0,,,,,2017-06-18 12:52:14,Why would I want less Kinesis when consuming using Lambda?,<aws-lambda><amazon-kinesis>,1,0,,,,CC BY-SA 3.0,Im using AWS Lambda to consume from Kinesis  My Lambda function doesnt have any requirements on max concurrency  Is there any reason for me to not have the maximum possible number of shards for my stream  I cant see that number of shards would affect cost  
62798957,1,,,2020-07-08 16:09:12,,0,69,"<p>I have a AWS Lambda function using an AWS SQS trigger to pull messages, process them with an AWS Comprehend endpoint, and put the output in AWS S3. The AWS Comprehend endpoint has a rate limit which goes up and down throughout the day based off something I can control. The fastest way to process my data, which also optimizes the costs I am paying for the AWS Comprehend endpoint to be up, is to set concurrency high enough that I get throttling errors returned from the api. This however comes with the caveat, that I am paying for more AWS Lambda invocations, the flip side being, that to optimize the costs I am paying for AWS Lambda, I want 0 throttling errors.</p>
<p>Is it possible to set up autoscaling for the concurrency limit of the lambda such that it will increase if it isn't getting any throttling errors, but decrease if it is getting too many?</p>
",9579869.0,,,,,2020-07-08 16:28:21,Autoscale AWS Lambda concurrency based off throttling errors,<amazon-web-services><aws-lambda><autoscaling>,1,1,,,,CC BY-SA 4.0,I have a AWS Lambda function using an AWS SQS trigger to pull messages  process them with an AWS Comprehend endpoint  and put the output in AWS S3  The AWS Comprehend endpoint has a rate limit which goes up and down throughout the day based off something I can control  The fastest way to process my data  which also optimizes the costs I am paying for the AWS Comprehend endpoint to be up  is to set concurrency high enough that I get throttling errors returned from the api  This however comes with the caveat  that I am paying for more AWS Lambda invocations  the flip side being  that to optimize the costs I am paying for AWS Lambda  I want 0 throttling errors  Is it possible to set up autoscaling for the concurrency limit of the lambda such that it will increase if it isnt getting any throttling errors  but decrease if it is getting too many  
62801694,1,,,2020-07-08 18:49:53,,0,402,"<p>I want some solution where a CloudWatch rule triggers the lambda function that takes a snapshot and shutdown the cluster at the given time, and resume the cluster from the created snapshot at another time.</p>
<p>This way a lot money can be saved.</p>
<p>As of now, AWS does not provides such solutions. Cluster Pause and Resume can be done by scheduling but still we need to pay for the storage resource of the cluster.</p>
",4967872.0,,,,,2020-07-09 12:32:22,How to automate Redshift snapshot creation and resume cluster from snapshot at a particular time?,<amazon-web-services><aws-lambda><amazon-redshift><aws-cli><amazon-redshift-spectrum>,2,1,,,,CC BY-SA 4.0,I want some solution where a CloudWatch rule triggers the lambda function that takes a snapshot and shutdown the cluster at the given time  and resume the cluster from the created snapshot at another time  This way a lot money can be saved  As of now  AWS does not provides such solutions  Cluster Pause and Resume can be done by scheduling but still we need to pay for the storage resource of the cluster  
45209250,1,,,2017-07-20 08:21:38,,7,1514,"<p>I'm building an API that will act as a proxy to <em>n</em> underlying API's that all do the same thing.  It will use <a href=""https://martinfowler.com/bliki/CircuitBreaker.html"" rel=""noreferrer"">circuit breaker</a> pattern to determine when one of the underlying API's is unavailable, therefore the proxy API will have state. One solution is to run the API on AWS lambda and store the circuit breaker state in AWS ElastiCache. </p>

<p>Is there another more cost effective solution that would not require me to run an 'always on' service like ElasticCache?</p>
",1022177.0,,,,,2020-08-07 20:32:02,Proxy API with Circuit Breaker on AWS lambda,<amazon-web-services><aws-lambda><amazon-elasticache><circuit-breaker>,1,5,3.0,,,CC BY-SA 3.0,Im building an API that will act as a proxy to n underlying APIs that all do the same thing   It will use  pattern to determine when one of the underlying APIs is unavailable  therefore the proxy API will have state  One solution is to run the API on AWS lambda and store the circuit breaker state in AWS ElastiCache   Is there another more cost effective solution that would not require me to run an always on service like ElasticCache  
62227576,1,62228000.0,,2020-06-06 05:07:43,,0,151,"<p>Working on product which is served as PaaS, backend of product is completely developed for serverless using NodeJS Serverless Framework and deployed on AWS Lambda.</p>

<p>When I started working on it, I found there are lots of duplicate code, LambdaFunctionOne and LambdaFunctionTwo both have same function performing same operation, and this is the problem if we have to change the logic of function then we will have to change the function in all lambda function.</p>

<p>Wanted to remove the duplicated, so if LambdaFunctionTwo required the some function which is LambdaFunctionOne then instead of replicating, should invoke LambdaFunctionOne and call its function. Suppose created a lambda function for utils and used that utils in every function of Lambda by invoking lambda instead of replicating.</p>

<ol>
<li>How does it will affect the pricing?</li>
<li>Is it good practice to call the lambda from another lambda in terms of cost?</li>
<li>Is it good practice to develop such product in serverless?</li>
</ol>
",11141189.0,,,,,2020-06-06 09:44:43,How does internal invocation of AWS lambda affect costing?,<amazon-web-services><aws-lambda><serverless-framework><aws-serverless><serverless-architecture>,3,3,,,,CC BY-SA 4.0,Working on product which is served as PaaS  backend of product is completely developed for serverless using NodeJS Serverless Framework and deployed on AWS Lambda  When I started working on it  I found there are lots of duplicate code  LambdaFunctionOne and LambdaFunctionTwo both have same function performing same operation  and this is the problem if we have to change the logic of function then we will have to change the function in all lambda function  Wanted to remove the duplicated  so if LambdaFunctionTwo required the some function which is LambdaFunctionOne then instead of replicating  should invoke LambdaFunctionOne and call its function  Suppose created a lambda function for utils and used that utils in every function of Lambda by invoking lambda instead of replicating   How does it will affect the pricing  Is it good practice to call the lambda from another lambda in terms of cost  Is it good practice to develop such product in serverless   
45222993,1,45225160.0,,2017-07-20 18:49:53,,1,213,"<h3>Background</h3>

<p>I am building an API using AWS Lambda and API Gateway. Rather than splitting each API endpoint into individual lambda functions I am wrapping them into a single library and using the <code>aws-serverless-express</code> library.</p>

<h3>Question</h3>

<p>Given that only a portion of the entire API might be used in a single Lambda execution from a memory utilization standpoint (to cut down on cost) is there a difference between: </p>



<pre class=""lang-js prettyprint-override""><code>var myModule = require(""mymodule"");

...

function handleSomething1()
{
    myModule.doSomething();
}

function handleSomething2()
{
    ...
}
</code></pre>



<p>or</p>



<pre class=""lang-js prettyprint-override""><code>function handleSomething()
{
    require(""mymodule"").doSomething();
}

function handleSomething2()
{
    ...
}
</code></pre>



<p>So for example, a single API request might result in only <code>handleSomething2</code> being called before the Lambda function is powered down. In that case are we effectively wasting memory by calling <code>var myModule = require(""mymodule"");</code> up top?</p>

<p>I suppose the more direct question is, when I <code>var myModule = require(""mymodule"")</code> does the node.js runtime actually allocate memory for <code>myModule</code> at that moment? Or is it effectively a no-op until I actually <em>do</em> something with <code>myModule</code>?</p>
",297140.0,,297140.0,,2017-07-20 19:07:23,2017-07-21 01:17:28,Lambda/Serverless inline require vs. header require,<javascript><node.js><memory><aws-lambda><serverless-framework>,2,0,,,,CC BY-SA 3.0,Background I am building an API using AWS Lambda and API Gateway  Rather than splitting each API endpoint into individual lambda functions I am wrapping them into a single library and using the  library  Question Given that only a portion of the entire API might be used in a single Lambda execution from a memory utilization standpoint (to cut down on cost) is there a difference between:   or  So for example  a single API request might result in only  being called before the Lambda function is powered down  In that case are we effectively wasting memory by calling  up top  I suppose the more direct question is  when I  does the node js runtime actually allocate memory for  at that moment  Or is it effectively a no-op until I actually do something with   
62826010,1,,,2020-07-10 01:25:57,,5,944,"<p>Been doing a ton of research. I am a mere padawan, however, I have a project where I must run a user's untrusted Python3 code from a website.</p>
<p>I also apologize in advance if this question has some moving parts.</p>
<ul>
<li>I am looking for an <strong>as safe as possible</strong> approach. This doesn't need to be 100% perfect unless there is a <strong>big risk of leaking extremely sensitive data</strong>.</li>
</ul>
<p><strong>Main questions</strong>:</p>
<ul>
<li>Does my <strong>AWS-lambda</strong> plan run an extreme risk for <strong>leaking sensitive data</strong>?</li>
<li>Are there any other simple precautions that I should take which could make this work safer in <strong>AWS-lambda</strong>?</li>
<li>Are there ways for <strong>exec()</strong> to break out of the <strong>AWS-lambda</strong> container and make any other network connections if all I have connected to it is the single <strong>AWS-api-gateway</strong> for the REST call?</li>
<li>Do I even need to limit <code>__builtins__</code> and locals, or are AWS-lambda containers safe enough?</li>
</ul>
<p><strong>BackGround</strong></p>
<p>It seems most companies use <strong>Kubernetes</strong> and <strong>Docker containers</strong> to execute untrusted python code (such a <strong>Leetcode</strong>, <strong>Programiz</strong>, or <strong>hackerRank</strong>).</p>
<p>See these helpful links:</p>
<ul>
<li><a href=""https://www.programiz.com/blog/online-python-compiler-engineering/"" rel=""nofollow noreferrer"">https://www.programiz.com/blog/online-python-compiler-engineering/</a></li>
<li><a href=""https://aws.amazon.com/blogs/compute/running-executables-in-aws-lambda/"" rel=""nofollow noreferrer"">https://aws.amazon.com/blogs/compute/running-executables-in-aws-lambda/</a></li>
</ul>
<p><strong>My Plan</strong></p>
<p>I am thinking that I can POST my arbitrary Python code to an AWS Lambda Function as a <strong>microservice</strong>, using <em>their</em> <strong>containerization/scaling</strong> rather than build my own. In the Lambda container, I can just run the code through a simple <strong>exec</strong> or <strong>eval</strong> function, perhaps with some limitation like this:</p>
<p>&quot;</p>
<pre><code>safe_list = ['math','acos', 'asin', 'atan', 'print','atan2', 'ceil', 'cos', 'cosh', 'de grees', 'e', 'exp', 'fabs', 'floor', 'fmod', 'frexp', 'hypot', 'ldexp', 'log', 'log10', 'modf', 'pi', 'pow', 'radians', 'sin', 'sinh', 'sqrt', 'tan', 'tanh'] 
    safe_dict = dict([ (k, locals().get(k, None)) for k in safe_list ]) 
    safe_dict['abs'] = abs
    exec(userCode,{&quot;**__builtins__&quot;**:None},safe_dict )
</code></pre>
<p><strong>Special Note:</strong></p>
<ul>
<li>I am not too concerned about infinite loops or crashing things, because I will just timeout and tell the user to try again.</li>
<li>All I need to do is run <em>pretty simple</em> python code <em>(generally less than a few lines)</em> and return exceptions, stdout, prints, and run a check on the result. Need to run:
<ul>
<li>Math operators, lists, loops, lambda functions, maps, filters, declare methods, declare classes with properties, print.</li>
</ul>
</li>
<li>This doesn't need to be a perfect project for hundreds of thousands of users. I just want to have a live site for a resume booster and maybe make a little money on ads to help with costs.</li>
<li>If there are severe limitations, I can eventually implement it in Kubernetes (as in the above link), but hopefully, this solution will work well enough.</li>
<li>I just want this to work relatively well and not take too long to build or cost too much money.</li>
<li>I do not want to leak any sensitive information.</li>
</ul>
<p><strong>Security things I am already planning on doing:</strong></p>
<ul>
<li>AWS lambda: Limit the <strong>time out</strong> to around <strong>1-2 seconds</strong></li>
<li>AWS lambda: Limit the <strong>memory usage</strong> to <strong>128mb</strong></li>
<li>My Own Code: Use <strong>regex</strong> to make sure no one is passing in <strong>double underscores</strong> <strong>badstuff</strong></li>
<li>Keeping this microservice as minimal as possible (only connecting a single AWS-API-gateway).</li>
</ul>
<p>Other notes:</p>
<ul>
<li>I don't think I can use <strong>restrictedPython</strong> or <strong>PyPy's sandbox</strong> feature in <strong>AWS Lambda</strong> because I don't have access to those dependencies OOB. I'm hoping that those are not necessary for this use case.</li>
<li>If it's impossible to do this with <strong>exec()</strong>, are there <strong>safe python interpreters</strong> on GitHub or someplace that I can literally copy-paste into files in AWS-lambda and just call them?</li>
<li>I am planning on allowing the user to print from exec with something like this:</li>
</ul>
<p>&quot;</p>
<pre><code>@contextlib.contextmanager
def stdoutIO(stdout=None):
    old = sys.stdout
    if stdout is None:
        stdout = StringIO()
    sys.stdout = stdout
    yield stdout
    sys.stdout = old

    
with stdoutIO() as s:
    try:
        exec(userCode)
    except:
        print(&quot;Something wrong with the code&quot;)
print( s.getvalue())
print(i)
</code></pre>
<p>Please let me know if you have any questions or suggestions.</p>
<p>___ Edit ** adding architecture diagram ___</p>
<p><a href=""https://i.stack.imgur.com/GUO7h.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GUO7h.png"" alt=""I think this is pretty much it as for the architecture fr now "" /></a></p>
",13477898.0,,13477898.0,,2020-07-10 17:43:14,2021-01-04 17:47:15,Running Untrusted Python Code in AWS-LAMBDA using Exec or Eval,<python><amazon-web-services><aws-lambda><architecture><aws-api-gateway>,2,2,2.0,,,CC BY-SA 4.0,Been doing a ton of research  I am a mere padawan  however  I have a project where I must run a users untrusted Python3 code from a website  I also apologize in advance if this question has some moving parts   I am looking for an as safe as possible approach  This doesnt need to be 100% perfect unless there is a big risk of leaking extremely sensitive data   Main questions:  Does my AWS-lambda plan run an extreme risk for leaking sensitive data  Are there any other simple precautions that I should take which could make this work safer in AWS-lambda  Are there ways for exec() to break out of the AWS-lambda container and make any other network connections if all I have connected to it is the single AWS-api-gateway for the REST call  Do I even need to limit  and locals  or are AWS-lambda containers safe enough   BackGround It seems most companies use Kubernetes and Docker containers to execute untrusted python code (such a Leetcode  Programiz  or hackerRank)  See these helpful links:     My Plan I am thinking that I can POST my arbitrary Python code to an AWS Lambda Function as a microservice  using their containerization/scaling rather than build my own  In the Lambda container  I can just run the code through a simple exec or eval function  perhaps with some limitation like this:   Special Note:  I am not too concerned about infinite loops or crashing things  because I will just timeout and tell the user to try again  All I need to do is run pretty simple python code (generally less than a few lines) and return exceptions  stdout  prints  and run a check on the result  Need to run:  Math operators  lists  loops  lambda functions  maps  filters  declare methods  declare classes with properties  print    This doesnt need to be a perfect project for hundreds of thousands of users  I just want to have a live site for a resume booster and maybe make a little money on ads to help with costs  If there are severe limitations  I can eventually implement it in Kubernetes (as in the above link)  but hopefully  this solution will work well enough  I just want this to work relatively well and not take too long to build or cost too much money  I do not want to leak any sensitive information   Security things I am already planning on doing:  AWS lambda: Limit the time out to around 1-2 seconds AWS lambda: Limit the memory usage to 128mb My Own Code: Use regex to make sure no one is passing in double underscores badstuff Keeping this microservice as minimal as possible (only connecting a single AWS-API-gateway)   Other notes:  I dont think I can use restrictedPython or PyPys sandbox feature in AWS Lambda because I dont have access to those dependencies OOB  Im hoping that those are not necessary for this use case  If its impossible to do this with exec()  are there safe python interpreters on GitHub or someplace that I can literally copy-paste into files in AWS-lambda and just call them  I am planning on allowing the user to print from exec with something like this:    Please let me know if you have any questions or suggestions  ___ Edit ** adding architecture diagram ___  
57333870,1,57380143.0,,2019-08-02 21:58:10,,3,1294,"<p>I currently have a lambda that is triggered by requests made to an API Gateway route, and I made some research about how to set a payload limit (e.g. 2kb) for this route. My goal is to guarantee that my lambda will never receive a large input to deal with, so it will have a slight execution time and low costs.</p>

<p>I found that the default payload limit for AWS API Gateway is 10 MB, while the default limit to AWS Lambda is 6 MB. Both of them cannot be increased.</p>

<p>However, I did not found any docs or discussion about how to <strong>decrease</strong> it. Is it possible? Are there any other AWS services that I should use as a middleware between API Gateway and my lambda to limit the received input? Or should I solve it by another approach?</p>
",6837519.0,,,,,2019-08-06 16:12:46,How to limit payload size for a specific API Gateway route,<aws-lambda><aws-api-gateway>,1,0,0.0,,,CC BY-SA 4.0,I currently have a lambda that is triggered by requests made to an API Gateway route  and I made some research about how to set a payload limit (e g  2kb) for this route  My goal is to guarantee that my lambda will never receive a large input to deal with  so it will have a slight execution time and low costs  I found that the default payload limit for AWS API Gateway is 10 MB  while the default limit to AWS Lambda is 6 MB  Both of them cannot be increased  However  I did not found any docs or discussion about how to decrease it  Is it possible  Are there any other AWS services that I should use as a middleware between API Gateway and my lambda to limit the received input  Or should I solve it by another approach  
62238188,1,62254109.0,,2020-06-06 21:22:52,,0,131,"<p>I'm trying to build a serverless NuxtJS app, utilizing firebase for authentication, netlify for deployment (and functions) and stripe for payment. </p>

<p>This whole payment-process and serverless functions on netlify is all new to me, so this might be a nooby question. </p>

<p>I've followed serveral docs and guides, and accomplished an app with firebase authentication, netlify deployment and serverless functions making me able to process a stripe payment - now I just cant figure out the next step. My stripe <code>success_url</code> leads to a <code>/pages/succes/index.js</code> route containing a success message -> though here I'd need some data response from Stripe, making me able to present the purchased item and also attaching the product id as a ""bought-product"" entry on the user object in firebase (the products will essentially be an upgrade to the user profile).</p>

<p><strong>Click ""buy product"" function</strong></p>

<pre><code>async buyProduct(sku, qty) {
  const data = {
    sku: sku,
    quantity: qty,
  };

  const response = await fetch('/.netlify/functions/create-checkout', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
    },
    body: JSON.stringify(data),
  }).then((res) =&gt; res.json());

  console.log(response);

  const stripe = await loadStripe(response.publishableKey);
  const { error } = await stripe.redirectToCheckout({
    sessionId: response.sessionId,
  });

  if (error) {
    console.error(error);
  }
}
</code></pre>

<p><strong>create-checkout Netlify function</strong></p>

<pre><code>const stripe = require('stripe')(process.env.STRIPE_SECRET_KEY);
const inventory = require('./data/products.json');

exports.handler = async (event) =&gt; {
  const { sku, quantity } = JSON.parse(event.body);
  const product = inventory.find((p) =&gt; p.sku === sku);
  const validatedQuantity = quantity &gt; 0 &amp;&amp; quantity &lt; 11 ? quantity : 1;

  const session = await stripe.checkout.sessions.create({
    payment_method_types: ['card'],
    billing_address_collection: 'auto',
    shipping_address_collection: {
      allowed_countries: ['US', 'CA'],
    },
    success_url: `${process.env.URL}/success`,
    cancel_url: process.env.URL,
    line_items: [
      {
        name: product.name,
        description: product.description,
        images: [product.image],
        amount: product.amount,
        currency: product.currency,
        quantity: validatedQuantity,
      },
    ],
  });

  return {
    statusCode: 200,
    body: JSON.stringify({
      sessionId: session.id,
      publishableKey: process.env.STRIPE_PUBLISHABLE_KEY,
    }),
  };
};
</code></pre>

<p><strong>Please let me know if you need more information, or something doesnt make sense!</strong></p>

<p><em>tldr;</em> I've processed a Stripe payment in a serverless app using Netlify function, and would like on the success-page to be able to access the bought item and user information.</p>
",6893066.0,,,,,2020-06-08 02:28:49,"Stripe processed, now what?",<stripe-payments><nuxt.js><serverless><netlify><netlify-function>,1,0,,,,CC BY-SA 4.0,Im trying to build a serverless NuxtJS app  utilizing firebase for authentication  netlify for deployment (and functions) and stripe for payment   This whole payment-process and serverless functions on netlify is all new to me  so this might be a nooby question   Ive followed serveral docs and guides  and accomplished an app with firebase authentication  netlify deployment and serverless functions making me able to process a stripe payment - now I just cant figure out the next step  My stripe  leads to a  route containing a success message -&gt; though here Id need some data response from Stripe  making me able to present the purchased item and also attaching the product id as a bought-product entry on the user object in firebase (the products will essentially be an upgrade to the user profile)  Click buy product function  create-checkout Netlify function  Please let me know if you need more information  or something doesnt make sense  tldr; Ive processed a Stripe payment in a serverless app using Netlify function  and would like on the success-page to be able to access the bought item and user information  
45261473,1,,,2017-07-23 04:24:11,,0,1380,"<p>I am trying to implement a solution on AWS which is as follows:</p>

<p>I have a crawler that will run once a day to index certain sites. I want to cache this data and expose it the the form of an API since after crawling, this data will not change for an entire day. After the crawler refetches, I want to invalidate and rebuild this cache to serve the updated data. I'm trying to use serverless architecture to build this.</p>

<p><strong>Possible Solutions</strong></p>

<p>It is clear that the crawler will run on AWS Lambda. What is unclear to me is how to manage the cache that will serve the data. Here are some solutions I thought of</p>

<ol>
<li><p><strong>S3 and Cloudfront for caching:</strong> After crawling, store the data in the form of .json files in S3 that will be cached using AWS Cloudfront. When the crawler refetches new data, it will rebuild these files and ask Cloudfront to invalidate the cache.</p></li>
<li><p><strong>API Gateway DynamoDB</strong>: After Crawling store the data in DynamoDB which will be then served by API Gateway which is cached. The only problem here is how can I ask for this cache to be invalidated at the end of the day when the crawler re-crawls? Since the data will be static for a day, how can I not pay for the extra time that DynamoDB will be running (because if I implement caching on API Gateway, there will only one call to DynamoDB for caching after that it will be sitting idle for a day)</p></li>
</ol>

<p>Is there any other way that I am missing?</p>

<p>Thanks!</p>
",1032179.0,,,,,2017-07-25 10:34:39,Caching and invalidating AWS Lambda response,<amazon-s3><amazon-dynamodb><aws-lambda><aws-api-gateway><serverless-architecture>,1,1,,,,CC BY-SA 3.0,I am trying to implement a solution on AWS which is as follows: I have a crawler that will run once a day to index certain sites  I want to cache this data and expose it the the form of an API since after crawling  this data will not change for an entire day  After the crawler refetches  I want to invalidate and rebuild this cache to serve the updated data  Im trying to use serverless architecture to build this  Possible Solutions It is clear that the crawler will run on AWS Lambda  What is unclear to me is how to manage the cache that will serve the data  Here are some solutions I thought of  S3 and Cloudfront for caching: After crawling  store the data in the form of  json files in S3 that will be cached using AWS Cloudfront  When the crawler refetches new data  it will rebuild these files and ask Cloudfront to invalidate the cache  API Gateway DynamoDB: After Crawling store the data in DynamoDB which will be then served by API Gateway which is cached  The only problem here is how can I ask for this cache to be invalidated at the end of the day when the crawler re-crawls  Since the data will be static for a day  how can I not pay for the extra time that DynamoDB will be running (because if I implement caching on API Gateway  there will only one call to DynamoDB for caching after that it will be sitting idle for a day)  Is there any other way that I am missing  Thanks  
45919464,1,45922958.0,,2017-08-28 12:48:09,,2,6739,"<p>I have a case where I want to remove cookie in the request and send the request to another server and display response to the end user.</p>

<p>Example:
<code>
client -&gt; x.website.com -&gt; remove cookie -&gt; y.website.com
</code></p>

<p>Current solution:
<code>
client -&gt; x.website.com -&gt; ec2 instance, nginx proxy, remove cookie -&gt; y.website.com</code></p>

<p>I want to remove ec2 instance in the middle as it's expensive.</p>

<p>Is any way I can achieve using AWS Resources?</p>
",944032.0,,13070.0,,2017-08-28 14:28:50,2019-08-12 21:05:25,Create proxy solution using AWS Lambda,<amazon-web-services><aws-lambda><reverse-proxy>,2,0,1.0,,,CC BY-SA 3.0,I have a case where I want to remove cookie in the request and send the request to another server and display response to the end user  Example:  Current solution:  I want to remove ec2 instance in the middle as its expensive  Is any way I can achieve using AWS Resources  
57532771,1,57915526.0,,2019-08-17 02:01:38,,0,174,"<p>I have a Lambda function that needs to connect to S3, RDS, and Rekognition.  Obviously this means putting it in a VPC, which of course kills internet access (thanks AWS....)</p>

<p>I was able to maintain access to S3 by simply creating a networking endpoint to <code>com.amazonaws.us-east-2.s3</code>.  This prevented me from having to create a Gateway, as I refuse to pay $33 a month simply for my Lambda function to access the internet....</p>

<p>But, are there really no endpoints that will work with Rekognition? I didn't see anything obvious, but I'm really out of my comfort zone as it is dealing with VPC networking, so I wanted to double check to see if any of the other 45 endpoints would work, or if anyone has come up with any work-arounds?</p>
",895810.0,,,,,2019-09-12 23:15:50,VPC Rekognition endpoint for use with Lambda,<amazon-web-services><aws-lambda><amazon-vpc>,1,0,,,,CC BY-SA 4.0,I have a Lambda function that needs to connect to S3  RDS  and Rekognition   Obviously this means putting it in a VPC  which of course kills internet access (thanks AWS    ) I was able to maintain access to S3 by simply creating a networking endpoint to    This prevented me from having to create a Gateway  as I refuse to pay $33 a month simply for my Lambda function to access the internet     But  are there really no endpoints that will work with Rekognition  I didnt see anything obvious  but Im really out of my comfort zone as it is dealing with VPC networking  so I wanted to double check to see if any of the other 45 endpoints would work  or if anyone has come up with any work-arounds  
57554595,1,,,2019-08-19 10:08:42,,1,33,"<p>I Have AWS Environment where we are giving a training session to students there we want to restrict the large type of Resources Launching like Ec2 and other AWS services.</p>

<p>Is there any IAM roles or any Policy Script in order to avoid unauthorized and cost-saving.</p>

<p>PLS advise.</p>
",9561792.0,,,,,2019-08-19 10:19:54,How to restrict the user to create High-end Ec2 machines and costly Resources in AWS,<amazon-web-services><amazon-ec2><aws-lambda>,1,1,,,,CC BY-SA 4.0,I Have AWS Environment where we are giving a training session to students there we want to restrict the large type of Resources Launching like Ec2 and other AWS services  Is there any IAM roles or any Policy Script in order to avoid unauthorized and cost-saving  PLS advise  
62360293,1,62360784.0,,2020-06-13 13:17:06,,5,3911,"<p>My current stack is like this:</p>

<ol>
<li>User creates an account via AWS Cognito</li>
<li>A post confirmation lambda is triggered which then adds further user details to a database </li>
</ol>

<p>My database uses the <code>sub</code> id generated by cognito as the userId so they are the same. I also copy the email address as the Username in my database. My intention is to use Cognito for Authentication and my own database for the functionality of my app. </p>

<p>However if the user wishes to update their email address I need to amend this in both cognito and my database. My first attempt made a call to cognito in my lambda using <code>admin_update_user_attributes</code> but soon realised it was blocked from making external calls to the internet, so i created a nat gateway which worked but it simply costs way too much!</p>

<p>My second idea was to go through cognito, having my front end make the call and then have cognito trigger a lambda to update my database but I don't think this is possible. </p>

<p>Is there a configuration or something I'm missing to be able to access AWS cognito via a lambda through the API gateway as they are both AWS services.</p>

<p>I dont want to make two seperate calls via my frontend as this creates a risk of one being completed but not the other. </p>

<p>Thanks  </p>
",4563137.0,,,,,2021-10-22 11:02:47,Can I make a call to AWS Cognito via a Lambda through the API gateway?,<aws-lambda><aws-api-gateway><amazon-cognito><api-gateway><amazon-cognito-triggers>,1,0,,,,CC BY-SA 4.0,My current stack is like this:  User creates an account via AWS Cognito A post confirmation lambda is triggered which then adds further user details to a database   My database uses the  id generated by cognito as the userId so they are the same  I also copy the email address as the Username in my database  My intention is to use Cognito for Authentication and my own database for the functionality of my app   However if the user wishes to update their email address I need to amend this in both cognito and my database  My first attempt made a call to cognito in my lambda using  but soon realised it was blocked from making external calls to the internet  so i created a nat gateway which worked but it simply costs way too much  My second idea was to go through cognito  having my front end make the call and then have cognito trigger a lambda to update my database but I dont think this is possible   Is there a configuration or something Im missing to be able to access AWS cognito via a lambda through the API gateway as they are both AWS services  I dont want to make two seperate calls via my frontend as this creates a risk of one being completed but not the other   Thanks   
62363403,1,62363535.0,,2020-06-13 17:30:15,,2,3183,"<p>I have a client website that serves the images through AWS CloudFront and S3. The images are in PNG format and the URL on the webpage is of the CloudFront domain rather than the custom domain. In order to reduce costs, we are planning to compress them to JPG format. However, we have found that the image URL's are hardcoded in the database. </p>

<p>The problem is when we compress the image to JPG, the file extension changes and thus the old URL will not work as it is. The client does not wants to make changes to the database right now. </p>

<p>Is there a way we can serve the compressed files (with a different extension) for the same requests coming from the webpage?</p>

<p>I was looking into a solution to route requests using Lambda Edge - <a href=""https://aws.amazon.com/blogs/networking-and-content-delivery/dynamically-route-viewer-requests-to-any-origin-using-lambdaedge/"" rel=""nofollow noreferrer"">https://aws.amazon.com/blogs/networking-and-content-delivery/dynamically-route-viewer-requests-to-any-origin-using-lambdaedge/</a> </p>

<p>Is there any other optimal solution for this which you could suggest? Is there a way CloudFront to check for both types of file (jpg and png) in S3 and serve one of them? Example: request came for images/car.png -- > CloudFront to check for both images/car.png and images/car.jpg. If jpg exists serve this one.</p>
",3097157.0,,,,,2020-06-13 17:41:56,How to redirect URL at AWS CloudFront?,<amazon-web-services><amazon-s3><aws-lambda><amazon-cloudfront>,1,0,1.0,,,CC BY-SA 4.0,I have a client website that serves the images through AWS CloudFront and S3  The images are in PNG format and the URL on the webpage is of the CloudFront domain rather than the custom domain  In order to reduce costs  we are planning to compress them to JPG format  However  we have found that the image URLs are hardcoded in the database   The problem is when we compress the image to JPG  the file extension changes and thus the old URL will not work as it is  The client does not wants to make changes to the database right now   Is there a way we can serve the compressed files (with a different extension) for the same requests coming from the webpage  I was looking into a solution to route requests using Lambda Edge -   Is there any other optimal solution for this which you could suggest  Is there a way CloudFront to check for both types of file (jpg and png) in S3 and serve one of them  Example: request came for images/car png -- &gt; CloudFront to check for both images/car png and images/car jpg  If jpg exists serve this one  
44760307,1,44761721.0,,2017-06-26 12:56:53,,0,159,"<p>I'm new to AWS, so apologies in advance if this question is missing some important considerations, or has incorrect assumptions.</p>

<p>But basically I want to implement a service on AWS to store and retrieve data from multiple clients, which may be Android apps, Windows applications, websites etc. The way I've considered doing this is via a RESTful service using API Gateway front end, with a Lambda back end and maybe an S3 bucket to hold the data.</p>

<p>The basic requirements are:
(1) Clients can publish data to the server, where it is stored, perhaps with some kind of key/value structure.
(2) Clients can retrieve said data by key.
(3) If it is possible, clients to be able to subscribe to events from the service, so that they are notified if the value of a piece of data changes. This would avoid the need to poll the service, which would presumably start racking up unnecessary charges if the data doesn't change often.</p>

<p>Any pointers on how to get started with this welcome!</p>
",630562.0,,,,,2017-06-26 23:18:05,Implementing a simple Restful service to store and retrieve data using AWS API Gateway/Lambda,<amazon-web-services><aws-lambda><aws-api-gateway>,1,3,,,,CC BY-SA 3.0,Im new to AWS  so apologies in advance if this question is missing some important considerations  or has incorrect assumptions  But basically I want to implement a service on AWS to store and retrieve data from multiple clients  which may be Android apps  Windows applications  websites etc  The way Ive considered doing this is via a RESTful service using API Gateway front end  with a Lambda back end and maybe an S3 bucket to hold the data  The basic requirements are: (1) Clients can publish data to the server  where it is stored  perhaps with some kind of key/value structure  (2) Clients can retrieve said data by key  (3) If it is possible  clients to be able to subscribe to events from the service  so that they are notified if the value of a piece of data changes  This would avoid the need to poll the service  which would presumably start racking up unnecessary charges if the data doesnt change often  Any pointers on how to get started with this welcome  
42375009,1,42413097.0,,2017-02-21 18:20:01,,5,1052,"<p>My app is offline-first, and therefore Realm has been wonderful for persisting and accessing data. I love it. However, I also want to store the user data in the cloud (for backup but also in case I add web support later). I know that's exactly what Realm Object Server is for, but I think I'd prefer to use DynamoDB for the following reasons:</p>

<p>1) I've already invested in DynamoDB and Amazon's authentication (Cognito).</p>

<p>2) I like that Realm is effectively a relational database because from the client I do need to run complex queries. However, on the backend I mostly just want to backup all the data in a way that I can easily access it and manipulate if needed through Lambda functions). I'm totally fine with a NoSQL solution for this, and my understanding is that DynamoDB is a cost effective database with horizontal scaling, which is appealing to me. If I wanted to access the data in this way with Realm Object Server, my understanding is it would cost at least 1,500 a month.</p>

<p>3) No offense to the Realm team, but I got screwed by Parse shutting down and so I'd like to use something that I can trust will be around for 5+ years as my backend.</p>

<p>Anyway, with that out of the way, here's how I'm currently making this work:</p>

<p>1) Whenever I create or edit a Realm object, I have logic that will map that change into my DynamoDB schema (which is made up of far fewer tables than Realm).</p>

<p>2) I call these updates <code>UpdateTasks</code> and I queue them up and merge them as needed (if you, for instance, changed the same property more than once).</p>

<p>3) I go through the queue and pass chunks of <code>UpdateTasks</code> to a Lambda function I wrote that will iterate through the updates and perform the necessary put or update commands to DynamoDB.</p>

<p>4) I have retry logic in place in case you're offline or a request fails</p>

<p>5) Assuming everything is synced properly if you got a new phone and signed in, I have a separate Lambda function that will fetch all of the user's data and populate the Realm file just as it was before.</p>

<p>Like I said, all of this is working right now, but it feels fragile, and I can't help but feel like I'm going about this the wrong way. Plus it doesn't support two-way syncing or real-time communication if I wanted to add some social features</p>

<p>So my question is if this is a reasonable approach to making Realm sync with DynamoDB or if there's a better/more robust way? Also if I should reconsider using Realm Object Server or something else instead of DynamoDB, I'd be interested to hear why.</p>

<p>It is a big decision for me, so I'd appreciate all the help I can get! Thanks</p>
",7513206.0,,7513206.0,,2017-02-21 18:28:44,2017-11-08 11:19:25,Using Realm with Amazon DynamoDB,<swift><realm><amazon-dynamodb><aws-lambda><realm-object-server>,1,2,3.0,2017-11-09 04:46:05,,CC BY-SA 3.0,My app is offline-first  and therefore Realm has been wonderful for persisting and accessing data  I love it  However  I also want to store the user data in the cloud (for backup but also in case I add web support later)  I know thats exactly what Realm Object Server is for  but I think Id prefer to use DynamoDB for the following reasons: 1) Ive already invested in DynamoDB and Amazons authentication (Cognito)  2) I like that Realm is effectively a relational database because from the client I do need to run complex queries  However  on the backend I mostly just want to backup all the data in a way that I can easily access it and manipulate if needed through Lambda functions)  Im totally fine with a NoSQL solution for this  and my understanding is that DynamoDB is a cost effective database with horizontal scaling  which is appealing to me  If I wanted to access the data in this way with Realm Object Server  my understanding is it would cost at least 1 500 a month  3) No offense to the Realm team  but I got screwed by Parse shutting down and so Id like to use something that I can trust will be around for 5+ years as my backend  Anyway  with that out of the way  heres how Im currently making this work: 1) Whenever I create or edit a Realm object  I have logic that will map that change into my DynamoDB schema (which is made up of far fewer tables than Realm)  2) I call these updates  and I queue them up and merge them as needed (if you  for instance  changed the same property more than once)  3) I go through the queue and pass chunks of  to a Lambda function I wrote that will iterate through the updates and perform the necessary put or update commands to DynamoDB  4) I have retry logic in place in case youre offline or a request fails 5) Assuming everything is synced properly if you got a new phone and signed in  I have a separate Lambda function that will fetch all of the users data and populate the Realm file just as it was before  Like I said  all of this is working right now  but it feels fragile  and I cant help but feel like Im going about this the wrong way  Plus it doesnt support two-way syncing or real-time communication if I wanted to add some social features So my question is if this is a reasonable approach to making Realm sync with DynamoDB or if theres a better/more robust way  Also if I should reconsider using Realm Object Server or something else instead of DynamoDB  Id be interested to hear why  It is a big decision for me  so Id appreciate all the help I can get  Thanks 
57940850,1,,,2019-09-15 03:07:56,,-1,481,"<p>I have a small amount of data (~2kb string) I need to save and retrieve from a lambda function. What's the best way to do this?</p>

<p>I only need to write these values a couple of times a day but I need to retrieve them each time my Lambda function is called so it will be a lot of reads with just a couple writes.</p>

<p>I know my solutions are DynamoDB, S3 or ElasticCache. I also have seen SSM Parameter Store but I'm not clear if it's possible to write values or if this is a server just to read values.</p>

<p>I was hoping someone could advise on the best and most cost-effective approach for this in a Lambda Function.</p>
",284714.0,,174777.0,,2019-09-16 01:23:28,2019-09-16 01:23:28,What's the best way to save and retrieve data for a Lambda function?,<amazon-web-services><aws-lambda>,2,2,,,,CC BY-SA 4.0,I have a small amount of data (~2kb string) I need to save and retrieve from a lambda function  Whats the best way to do this  I only need to write these values a couple of times a day but I need to retrieve them each time my Lambda function is called so it will be a lot of reads with just a couple writes  I know my solutions are DynamoDB  S3 or ElasticCache  I also have seen SSM Parameter Store but Im not clear if its possible to write values or if this is a server just to read values  I was hoping someone could advise on the best and most cost-effective approach for this in a Lambda Function  
57943748,1,,,2019-09-15 11:40:54,,-1,30,"<p>The main question is: how is time accounted in a wsgi application deployed on aws Lambda?</p>

<p>Suppose I deploy the following simple Flask app:</p>

<pre class=""lang-py prettyprint-override""><code>from flask import Flask

app = Flask(__name__)

@app.route(""/"")
def hello():
  return ""Hello World!"", 200
</code></pre>

<p>using Zappa on AWS Lambda, with the following configuration:</p>

<pre><code>{
    ""dev"": {
        ""app_function"": ""simple_application.app"",
        ""profile_name"": ""default"",
        ""project_name"": ""simple_application"",
        ""runtime"": ""python3.7"",
        ""s3_bucket"": ""zappa-deployments-RANDOM"",
        ""memory_size"": 128,
        ""keep_warm"": false,
        ""aws_region"": ""us-east-1""
    }
}
</code></pre>

<p>Now if AWS has a request for my website, it will spin up a container with my code inside and let it deal with the request. Suppose the request is served in 200ms.</p>

<p>Obviously the Lambda with the wsgi server inside continues running (Zappa by default makes the lambda run for at least 30s).</p>

<p>So now to the various subquestions:</p>

<ol>
<li><p>How much time am I charged for the execution?</p>

<ul>
<li>200ms because of the request duration</li>
<li>30s because of the below limit for my lambda execution time</li>
<li>until the lambda is killed by AWS to reclaim space (which could occur even 30-45 minutes after)</li>
</ul></li>
<li><p>If another request come along (and the first one is still being served) will the second request spin up another Lambda container or it will be queued until a threshold time has passed?</p></li>
</ol>

<p>I expected to be charged just for the 200ms by reading AWS Lambda pricing page, but I would bet on it charging for 30s because, after all, I'm the one who imposed such limit.</p>

<p>In case I'm just charged for 200ms (and subsequent requests time) but the container keeps running uninterruptly for 30-45 minutes, I have a third subquestion:</p>

<ol start=""3"">
<li><p>Suppose now that I want to use a global variable as a simple local cache and synchronize it with a database (let's say DynamoDB) before the container is killed.
To do this I would like to raise the execution time limit of my lambdas to 15m, then at lambda creation set a Timer to fire a function that synchronizes the state and aborts the function after 14m30s.</p>

<p>How would accounted running time change in this settings (i.e. with a Timer that fires after a certain amount of time)?</p></li>
</ol>

<p>The proposed lambda code for this subquestion is:</p>

<pre class=""lang-py prettyprint-override""><code>from flask import Flask
from threading import Timer
from db import Visits
import sys

lambda_uuid = ""SOME-UUID-OF-LAMBDA-INSTANCE""

# Collects number of visits
visits = 0

def report_visits():
    Visits(uuid=lambda_uuid, visits=visits).save()
    sys.exit(0)

t = Timer(14 * 60 + 30, report_visits)
t.start()

# Start of flask routes
app = Flask(__name__)

@app.route(""/"")
def hello():
    visits = visits + 1
    return ""Hello World!"", 200

</code></pre>

<p>Thanks in advance for any information.</p>
",6125576.0,,,,,2019-09-15 13:39:59,How is inactivity time accounted in a wsgi application on aws lambda?,<aws-lambda><sleep><wsgi>,1,0,,,,CC BY-SA 4.0,The main question is: how is time accounted in a wsgi application deployed on aws Lambda  Suppose I deploy the following simple Flask app:  using Zappa on AWS Lambda  with the following configuration:  Now if AWS has a request for my website  it will spin up a container with my code inside and let it deal with the request  Suppose the request is served in 200ms  Obviously the Lambda with the wsgi server inside continues running (Zappa by default makes the lambda run for at least 30s)  So now to the various subquestions:  How much time am I charged for the execution   200ms because of the request duration 30s because of the below limit for my lambda execution time until the lambda is killed by AWS to reclaim space (which could occur even 30-45 minutes after)  If another request come along (and the first one is still being served) will the second request spin up another Lambda container or it will be queued until a threshold time has passed   I expected to be charged just for the 200ms by reading AWS Lambda pricing page  but I would bet on it charging for 30s because  after all  Im the one who imposed such limit  In case Im just charged for 200ms (and subsequent requests time) but the container keeps running uninterruptly for 30-45 minutes  I have a third subquestion:  Suppose now that I want to use a global variable as a simple local cache and synchronize it with a database (lets say DynamoDB) before the container is killed  To do this I would like to raise the execution time limit of my lambdas to 15m  then at lambda creation set a Timer to fire a function that synchronizes the state and aborts the function after 14m30s  How would accounted running time change in this settings (i e  with a Timer that fires after a certain amount of time)   The proposed lambda code for this subquestion is:  Thanks in advance for any information  
42390164,1,42393446.0,,2017-02-22 11:33:01,,3,726,"<p>I'm working on a website that mostly displays items created by registered users. So I'd say 95% of API calls are to read a single item and 5% are to store a single item. System is designed with AWS API Gateway that calls AWS Lambda function which manipulates data in DynamoDB.</p>

<p>My next step is to implement voting system (upvote/downvote) with basic fetaures:</p>

<ul>
<li>Each registered user can vote only once per item, and later is only allowed to change that vote.</li>
<li>number of votes needs to be displayed to all users next to every item.</li>
<li>items have only single-item views, and are (almost) never displayed in a list view.</li>
<li>only list view I need is ""top 100 items by votes"" but it is ok to calculate this once per day and serve cached version</li>
</ul>

<p>My goal is to design a database/lambda to minimize costs of AWS. It's easy to make the logic work but I'm not sure if my solution is the optimal one:</p>

<ul>
<li>My <code>items</code> table currently has hashkey <code>slug</code> and sortkey <code>version</code></li>
<li>I created <code>items-votes</code> table with hashkey <code>slug</code> and sortkey <code>user</code> and also <code>voted</code> field (containing -1 or 1)</li>
<li>I added field <code>votes</code> to <code>items</code> table</li>
<li>API call to upvote/downvote inserts to <code>item-votes</code> table but before checks constraints that user has not already voted that way. Then in second query updates <code>items</code> table with updated votes count. (so 1 API call and 2 db queries)</li>
<li>old API call to show an item stays the same but grabs new <code>votes</code> count too (1 API call and 1 db query)</li>
</ul>

<p>I was wondering if this can be done even better with avoiding new <code>items-votes</code> table and storing user votes inside <code>items</code> table? It looks like it is possible to save one query that way, and half the lambda execution time but I'm worried it might make that table too big/complex. Each <code>user</code> field is a 10 chars user ID so if item gets thousands of votes I'm not sure how Lambda/DynamoDB will behave compared to original solution.</p>

<p>I don't expect thousands of votes any time soon, but it is not impossible to happen to a few items and I'd like to avoid situation where I need to migrate to different solution in the near future.</p>
",7353636.0,,,,,2017-02-22 13:54:31,Voting on items - how to design database/aws-lambda to minimize AWS costs,<amazon-dynamodb><aws-lambda>,1,0,1.0,,,CC BY-SA 3.0,Im working on a website that mostly displays items created by registered users  So Id say 95% of API calls are to read a single item and 5% are to store a single item  System is designed with AWS API Gateway that calls AWS Lambda function which manipulates data in DynamoDB  My next step is to implement voting system (upvote/downvote) with basic fetaures:  Each registered user can vote only once per item  and later is only allowed to change that vote  number of votes needs to be displayed to all users next to every item  items have only single-item views  and are (almost) never displayed in a list view  only list view I need is top 100 items by votes but it is ok to calculate this once per day and serve cached version  My goal is to design a database/lambda to minimize costs of AWS  Its easy to make the logic work but Im not sure if my solution is the optimal one:  My  table currently has hashkey  and sortkey  I created  table with hashkey  and sortkey  and also  field (containing -1 or 1) I added field  to  table API call to upvote/downvote inserts to  table but before checks constraints that user has not already voted that way  Then in second query updates  table with updated votes count  (so 1 API call and 2 db queries) old API call to show an item stays the same but grabs new  count too (1 API call and 1 db query)  I was wondering if this can be done even better with avoiding new  table and storing user votes inside  table  It looks like it is possible to save one query that way  and half the lambda execution time but Im worried it might make that table too big/complex  Each  field is a 10 chars user ID so if item gets thousands of votes Im not sure how Lambda/DynamoDB will behave compared to original solution  I dont expect thousands of votes any time soon  but it is not impossible to happen to a few items and Id like to avoid situation where I need to migrate to different solution in the near future  
57994393,1,57995123.0,,2019-09-18 13:52:58,,1,446,"<p>I have a client that performs a weekly data upload of 3 CSV files to an S3 bucket, always within (at most) 5 minutes of each other. I have python code that ingests the 3 files, aggregates them, and writes and output that I would like to use to create a lambda function to fully automate this job. The issue is that I can't configure an S3 trigger that is every 3 object creates. I could have the lambda trigger every upload and exit until the 3 files are there but I don't want to do that as it's not really cost effective.</p>

<p>So I came across this question <a href=""https://stackoverflow.com/questions/34376697/aws-want-to-upload-multiple-files-to-s3-and-only-when-all-are-uploaded-trigger"">here</a> that suggested having an SNS Topic that gets notified after a batch of uploads is completed, however I'm having trouble figuring out how to configure that. Basically what I'd like to do is create something similar to a CloudWatch Alarm that triggers when 3 object PUTS have occurred within 5 minutes of each other. Is this possible? Or, how can I configure my SNS event in a way as is suggested in the linked question?</p>
",9908952.0,,,,,2019-09-18 14:26:32,AWS SNS trigger when 3 objects are loaded into S3,<amazon-web-services><amazon-s3><aws-lambda><amazon-sns>,1,2,,,,CC BY-SA 4.0,I have a client that performs a weekly data upload of 3 CSV files to an S3 bucket  always within (at most) 5 minutes of each other  I have python code that ingests the 3 files  aggregates them  and writes and output that I would like to use to create a lambda function to fully automate this job  The issue is that I cant configure an S3 trigger that is every 3 object creates  I could have the lambda trigger every upload and exit until the 3 files are there but I dont want to do that as its not really cost effective  So I came across this question  that suggested having an SNS Topic that gets notified after a batch of uploads is completed  however Im having trouble figuring out how to configure that  Basically what Id like to do is create something similar to a CloudWatch Alarm that triggers when 3 object PUTS have occurred within 5 minutes of each other  Is this possible  Or  how can I configure my SNS event in a way as is suggested in the linked question  
57995007,1,57995206.0,,2019-09-18 14:21:04,,2,442,"<p>I have built an EC2 reverse proxy (Nginx) that communicates with an external API endpoint over the internet. I have a Route53 DNS with an A record linking to my EC2. There are a few endpoints (Nginx locations) and depending on which url you hit, you are redirected to a specific proxy location, and forwarded to the right endpoint on the external API. It all works great.</p>

<p>Now i want to create some type of job that will test this process periodically to ensure that it's running and notify me if it's not. AWS has so many tools and i think i need to use Lambda and API Gateway.</p>

<p>I'd like to hit my url(Route53 DNS) go thru the EC2 and receive a response from the endpoint server. My site does this, postman can, but i can't figure out how to accomplish this in an automated way and alert me based on the response values.</p>

<p>how can i test my full pathway (www.example.com/option -> nginxEC2 path('/option') -> www.endpoint.com/option) and be notified based on the results.</p>

<p>EDIT: i need to be able to send a body with this. if i send it without body the server returns 404, if i can send with a body/payload, i'll get a response.</p>

<p>EDIT: basically looking for a way to hit my DNS, which thru A record, routes to my reverse proxy, to an endpoint. i just need to do an HTTP request to the Domain, and get and answer back and know the status code. </p>

<p>Mark B's solution is the closest as the free site he sent me has an option to pay for this service. gonna leave it open a few more days. </p>
",8334663.0,,8334663.0,,2019-09-23 15:53:51,2019-09-23 15:53:51,how do i set up a HTTP test for a Route53 -> EC2 -> API endpoint reverse proxy pathway,<amazon-web-services><api><http><aws-lambda>,2,0,,,,CC BY-SA 4.0,I have built an EC2 reverse proxy (Nginx) that communicates with an external API endpoint over the internet  I have a Route53 DNS with an A record linking to my EC2  There are a few endpoints (Nginx locations) and depending on which url you hit  you are redirected to a specific proxy location  and forwarded to the right endpoint on the external API  It all works great  Now i want to create some type of job that will test this process periodically to ensure that its running and notify me if its not  AWS has so many tools and i think i need to use Lambda and API Gateway  Id like to hit my url(Route53 DNS) go thru the EC2 and receive a response from the endpoint server  My site does this  postman can  but i cant figure out how to accomplish this in an automated way and alert me based on the response values  how can i test my full pathway (www example com/option -&gt; nginxEC2 path(/option) -&gt; www endpoint com/option) and be notified based on the results  EDIT: i need to be able to send a body with this  if i send it without body the server returns 404  if i can send with a body/payload  ill get a response  EDIT: basically looking for a way to hit my DNS  which thru A record  routes to my reverse proxy  to an endpoint  i just need to do an HTTP request to the Domain  and get and answer back and know the status code   Mark Bs solution is the closest as the free site he sent me has an option to pay for this service  gonna leave it open a few more days   
58014734,1,58025294.0,,2019-09-19 15:47:31,,0,201,"<p>I created a new Lambda based on a 2MB zip file (it has a heavy dependency). After that, my S3 costs really increased (from $12.27 to $31).</p>

<p><strong>Question 1:</strong> As this is uploaded from a CI/CD pipeline, could it be that it's storing every version and then increasing costs?</p>

<p><strong>Question 2:</strong> Is this storage alternative more expensive than choosing directly an owned s3 bucket instead of the private one owned by Amazon where this zip goes? Looking at the S3 prices list, only 2MB can't result in 19 Dollars.</p>

<p>Thanks!</p>
",4593995.0,,4593995.0,,2019-09-19 15:54:12,2019-09-20 09:10:20,Lambda triggers high s3 costs,<amazon-s3><aws-lambda>,2,0,,,,CC BY-SA 4.0,I created a new Lambda based on a 2MB zip file (it has a heavy dependency)  After that  my S3 costs really increased (from $12 27 to $31)  Question 1: As this is uploaded from a CI/CD pipeline  could it be that its storing every version and then increasing costs  Question 2: Is this storage alternative more expensive than choosing directly an owned s3 bucket instead of the private one owned by Amazon where this zip goes  Looking at the S3 prices list  only 2MB cant result in 19 Dollars  Thanks  
58019678,1,58020332.0,,2019-09-19 22:39:23,,0,47,"<p>bcrypt's thing is that it takes a set amount of time to run, so if someone wants to run a brute-force or dictionary attack against your server, bcrypt limits the number of ""guesses"" that can be run in a set amount of time.
AWS Lambda functions auto-scale. If a Lambda is busy, AWS will helpfully run up another one.
If you want to use bcrypt in a lambda function, the auto-scaling will negate the benefits of a function that takes time to run.
In addition, AWS will charge you for the extra lambda instances,
So you are effectively paying for someone else to hack into your server.
Is there a way to prevent this? eg limit the number of concurrent lambda instances for a particular function (to 1 maybe?)</p>
",1688208.0,,174777.0,,2019-12-03 18:10:36,2019-12-03 18:10:36,bcrypt Lambda auto scaling - are you paying for someone else to hack your system?,<amazon-web-services><authentication><aws-lambda><bcrypt>,1,0,,,,CC BY-SA 4.0,bcrypts thing is that it takes a set amount of time to run  so if someone wants to run a brute-force or dictionary attack against your server  bcrypt limits the number of guesses that can be run in a set amount of time  AWS Lambda functions auto-scale  If a Lambda is busy  AWS will helpfully run up another one  If you want to use bcrypt in a lambda function  the auto-scaling will negate the benefits of a function that takes time to run  In addition  AWS will charge you for the extra lambda instances  So you are effectively paying for someone else to hack into your server  Is there a way to prevent this  eg limit the number of concurrent lambda instances for a particular function (to 1 maybe ) 
58079600,1,58080227.0,,2019-09-24 11:45:04,,0,139,"<p>I am using Visual Studio code for debugging a lamda function written in python. 
Is the local execution of the lamda function is chargeable ? since at the end of each execution we are getting an entry in the log showing execution time charged.</p>

<p>Please note we are calling some AWS api from within the lamda function, and that is understandably chargable, I have no issues with that.</p>
",529352.0,,174777.0,,2019-12-03 18:10:07,2019-12-03 18:10:07,AWS SAM local debugging is chargeable?,<python><aws-lambda><aws-sam-cli><aws-sam>,2,0,1.0,,,CC BY-SA 4.0,I am using Visual Studio code for debugging a lamda function written in python   Is the local execution of the lamda function is chargeable   since at the end of each execution we are getting an entry in the log showing execution time charged  Please note we are calling some AWS api from within the lamda function  and that is understandably chargable  I have no issues with that  
44792573,1,,,2017-06-28 02:10:43,,3,700,"<p>In my first foray into any computing in the cloud, I was able to follow Mark West's <a href=""https://github.com/markwest1972/smart-security-camera"" rel=""nofollow noreferrer"">instructions</a> on how to use AWS Rekognition to process images from a security camera that are dumped into an S3 bucket and provide a notification if a person was detected. His code was setup for the Raspberry Pi camera but I was able to adapt it to my IP camera by having it FTP the triggered images to my Synology NAS and use CloudSync to mirror it to the S3 bucket. A step function calls Lambda functions per the below figure and I get an email within 15 seconds with a list of labels detected and the image attached.</p>

<p><a href=""https://i.stack.imgur.com/tw8tB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tw8tB.png"" alt=""Step functions""></a></p>

<p>The problem is the camera will upload one image per second as long the condition is triggered and if there is a lot of activity in front of the camera, I can quickly rack up a few hundred emails.</p>

<p>I'd like to insert a function between make-alert-decision and nodemailer-send-notification that would check to see if an email notification was sent within the last minute and if not, proceed to nodemailer-send-notification right away and if so, store the list of labels, and path to the attachment in an array and then send a single email with all of the attachments once 60 seconds had passed.</p>

<p>I know I have to store the data externally and came across <a href=""https://sc5.io/posts/amazon-aws-lambda-data-caching-solutions-compared/"" rel=""nofollow noreferrer"">this article</a> explaining the benefits of different methods of caching data and I also thought that I could examine the timestamps of the files uploaded to S3 to compare the time elapsed between the two most recent uploaded files to decide whether to proceed or batch the file for later.</p>

<p>Being completely new to AWS, I am looking for advice on which method makes the most sense from a complexity and cost perspective.  I can live with the lag involved in any of methods discussed in the article, just don't know how to proceed as I've never used or even heard of any of the services.</p>

<p>Thanks!</p>
",7612553.0,,2593745.0,,2017-06-29 20:51:50,2017-06-30 07:24:07,AWS Step/Lambda - storing variable between runs,<node.js><amazon-web-services><amazon-s3><aws-lambda>,1,0,0.0,,,CC BY-SA 3.0,In my first foray into any computing in the cloud  I was able to follow Mark Wests  on how to use AWS Rekognition to process images from a security camera that are dumped into an S3 bucket and provide a notification if a person was detected  His code was setup for the Raspberry Pi camera but I was able to adapt it to my IP camera by having it FTP the triggered images to my Synology NAS and use CloudSync to mirror it to the S3 bucket  A step function calls Lambda functions per the below figure and I get an email within 15 seconds with a list of labels detected and the image attached   The problem is the camera will upload one image per second as long the condition is triggered and if there is a lot of activity in front of the camera  I can quickly rack up a few hundred emails  Id like to insert a function between make-alert-decision and nodemailer-send-notification that would check to see if an email notification was sent within the last minute and if not  proceed to nodemailer-send-notification right away and if so  store the list of labels  and path to the attachment in an array and then send a single email with all of the attachments once 60 seconds had passed  I know I have to store the data externally and came across  explaining the benefits of different methods of caching data and I also thought that I could examine the timestamps of the files uploaded to S3 to compare the time elapsed between the two most recent uploaded files to decide whether to proceed or batch the file for later  Being completely new to AWS  I am looking for advice on which method makes the most sense from a complexity and cost perspective   I can live with the lag involved in any of methods discussed in the article  just dont know how to proceed as Ive never used or even heard of any of the services  Thanks  
44794464,1,,,2017-06-28 05:44:42,,0,217,"<p>I want to prototype an app and I thought Apps Script could be a quick way to do this, but being new to Google toolset I need someone to put me in the right direction to figure what is available and what components I need to put together to get this working.</p>

<p>Here's what the app does:</p>

<ol>
<li>I put in an order in a form on a web page. (Google Forms? App Script web app?)</li>
<li>Some inputs in the form are dynamically shown depending on previous inputs.</li>
<li>The inputs are validated.</li>
<li>There is a list of receivers with mobile numbers (Google Spreadsheets?)</li>
<li>When submitted, the order is sent to a web service (Twilio REST API) that sends SMS text messages to each of the receivers.</li>
<li>Before submitting I want to be able to see a preview of the generated text message and the total cost for sending SMS.</li>
<li>After submitting, the order is also logged (Spreadsheets?)</li>
<li>In the text message there is a link to a web page that is specific to that order for more details</li>
</ol>

<p>I don't need specific code but I need to find out what to use for each step. Also any examples or samples that could help me on the way would be very useful.</p>

<p><strong>Update</strong> - My specific question for now to get started is:</p>

<p>Is it possible to use Google Forms to have dynamic fields (entering data in one field defines next fields with their choices), custom validations (validations are more than regex and multiple fields are validated together) and previews (when entering data, show some live calculations)? If not, how can I use Apps Script with Html to do that?</p>

<p>Thanks!</p>
",450913.0,,450913.0,,2017-06-29 02:09:33,2017-06-29 02:09:33,Prototyping a workflow app with Apps Script,<google-app-engine><google-apps-script><serverless-framework><google-workspace>,1,0,,2017-06-28 13:09:51,,CC BY-SA 3.0,I want to prototype an app and I thought Apps Script could be a quick way to do this  but being new to Google toolset I need someone to put me in the right direction to figure what is available and what components I need to put together to get this working  Heres what the app does:  I put in an order in a form on a web page  (Google Forms  App Script web app ) Some inputs in the form are dynamically shown depending on previous inputs  The inputs are validated  There is a list of receivers with mobile numbers (Google Spreadsheets ) When submitted  the order is sent to a web service (Twilio REST API) that sends SMS text messages to each of the receivers  Before submitting I want to be able to see a preview of the generated text message and the total cost for sending SMS  After submitting  the order is also logged (Spreadsheets ) In the text message there is a link to a web page that is specific to that order for more details  I dont need specific code but I need to find out what to use for each step  Also any examples or samples that could help me on the way would be very useful  Update - My specific question for now to get started is: Is it possible to use Google Forms to have dynamic fields (entering data in one field defines next fields with their choices)  custom validations (validations are more than regex and multiple fields are validated together) and previews (when entering data  show some live calculations)  If not  how can I use Apps Script with Html to do that  Thanks  
58295709,1,,,2019-10-09 00:56:54,,0,237,"<p>Disclaimer: newbie in aws arena</p>

<p>I have an endpoint that send message to an sqs queue to process order, once process order is done it forward that message to another queue to process payment.</p>

<p>I successfully send my message to first queue to process order, and that also prints success message saying message has been forward to process payment queue, however, I cannot see any logs printing in the cloudwatch logs.</p>

<p>I have created a serverless.yml file for process order repo.</p>

<pre><code>service: process-order

plugins:
  - serverless-offline
  - serverless-domain-manager

provider:
  name: aws
  runtime: nodejs10.x
  stage:  ${file(./env.yml):${opt:stage}.stage}
  region: ${file(./env.yml):${opt:stage}.region}
  deploymentBucket: ${file(./env.yml):${opt:stage}.bucket}
  environment:
    MESSAGE: ${file(./env.yml):${opt:stage}.me

iamRoleStatements:
  - Effect: ""Allow""
    Action:
      - ""sqs:SendMessage""
    Resource: ""arn:aws:sqs:eu-east-2:996333333061:ApiPaymentTest""

functions:
  compute:
    handler: index.handler
    events:
      - sqs: arn:aws:sqs:eu-east-2:996333333061:ApiPaymentTest
      - sqs:
          arn:
            Fn::GetAtt:
              - ApiPaymentTest
              - Arn
          bacthSize: 1
</code></pre>

<p>Looking forward for your expert opinion please.</p>

<p>regard</p>
",9787484.0,,,,,2019-10-09 00:56:54,AWS SQS message forwarding to next sqs service not showing cloudwatch logs,<amazon-web-services><aws-lambda><aws-sdk><amazon-sqs><amazon-cloudwatch>,0,2,,,,CC BY-SA 4.0,Disclaimer: newbie in aws arena I have an endpoint that send message to an sqs queue to process order  once process order is done it forward that message to another queue to process payment  I successfully send my message to first queue to process order  and that also prints success message saying message has been forward to process payment queue  however  I cannot see any logs printing in the cloudwatch logs  I have created a serverless yml file for process order repo   Looking forward for your expert opinion please  regard 
43256240,1,43258863.0,,2017-04-06 13:11:40,,3,609,"<p>For a project I'm working on, there is a need to pull down a largish text file that's updated daily and made available at a specific customer URL, and store it in AWS S3 which then triggers downstream processing of the file (details unimportant). </p>

<p>I was thinking of having the download + store in S3 done by an AWS Lambda triggered every 24 hours by CloudWatch, which would work, but there's a catch: the file is 36MB in size and is served by a host that throttles downloads to 100kB/s (outside of my control). This means it takes at least 360s (i.e. 6 mins) to completely download the file. AWS Lambda functions however have an upper limit of 300s run time, which effectively makes it impossible to use for this task as the Lambda times-out and exits before the file is completely downloaded.</p>

<p>I'm looking for suggestions of ways of working around the 300s run time limit of AWS Lambda to achieve this goal.</p>

<p>As long as I'm sticking to AWS, the only alternative I see is to set up a cron job on an EC2 instance, but that seems expensive / overkill, especially if I end up not needing an always-on EC2 for anything else.</p>

<p>Thanks!</p>
",6833222.0,,2593745.0,,2017-04-07 09:11:11,2017-04-07 09:11:11,Advice sought on ways of downloading a large file from a bandwidth-throttled server to a AWS S3 bucket,<amazon-web-services><amazon-s3><amazon-ec2><aws-lambda>,1,0,1.0,,,CC BY-SA 3.0,For a project Im working on  there is a need to pull down a largish text file thats updated daily and made available at a specific customer URL  and store it in AWS S3 which then triggers downstream processing of the file (details unimportant)   I was thinking of having the download + store in S3 done by an AWS Lambda triggered every 24 hours by CloudWatch  which would work  but theres a catch: the file is 36MB in size and is served by a host that throttles downloads to 100kB/s (outside of my control)  This means it takes at least 360s (i e  6 mins) to completely download the file  AWS Lambda functions however have an upper limit of 300s run time  which effectively makes it impossible to use for this task as the Lambda times-out and exits before the file is completely downloaded  Im looking for suggestions of ways of working around the 300s run time limit of AWS Lambda to achieve this goal  As long as Im sticking to AWS  the only alternative I see is to set up a cron job on an EC2 instance  but that seems expensive / overkill  especially if I end up not needing an always-on EC2 for anything else  Thanks  
43263923,1,43266633.0,,2017-04-06 19:13:57,,1,1299,"<p>I'm new to Lambda and trying to figure out how to process Stripe charge in Lambda function.</p>

<p>My app creates token from Stripe API and now I need to send that token to Lambda function to execute <code>Stripe.customers.create</code> and <code>Stripe.charges.create</code>.</p>

<p>Here are the issues I'm having</p>

<ol>
<li><p>I'm not sure how to load <code>&lt;script type=""text/javascript"" src=""https://js.stripe.com/v2/""&gt;&lt;/script&gt;</code> into Lambda function so I can use <code>Stripe</code> function</p></li>
<li><p>Would love to get a sample code to charge credit card by token</p></li>
</ol>

<p>Thank you.</p>
",7724980.0,,,,,2017-04-06 22:17:00,AWS Lambda process Stripe charge,<javascript><amazon-web-services><lambda><stripe-payments><aws-lambda>,1,0,2.0,,,CC BY-SA 3.0,Im new to Lambda and trying to figure out how to process Stripe charge in Lambda function  My app creates token from Stripe API and now I need to send that token to Lambda function to execute  and   Here are the issues Im having  Im not sure how to load  into Lambda function so I can use  function Would love to get a sample code to charge credit card by token  Thank you  
58317315,1,,,2019-10-10 07:11:00,,0,583,"<p>I have a requirement of <strong>creating an ECS Cluster without using autoscaling</strong>.</p>

<p>This is because of <strong>a Dedicated Host (DH), Tenancy=Host, is not supported with ASGs</strong>.  DH is mainly for cost savings and for some cases because of the savings is worth doing even if we can't use ASGs.</p>

<p>I understand this can be done using <strong>Macros</strong> and <strong>Custom Resources backed up with Lambda</strong> or using <strong>Troposphere</strong> to loop over the instance.</p>

<p>But to start with any example of the same or any other approach would be really appreciated .</p>

<p>Below is my appsec.yaml template file:</p>

<pre><code>AWSTemplateFormatVersion: 2010-09-09
Description: Provision Platform Container Service

Parameters:
  PlatformCluster:
    Type: String
  PlatformClusterNotifications:
    Type: String
  PlatformClusterLifecycleNotification:
    Type: String
  Product:
    Type: String
  Environment:
    Type: String
  CDRevisionLoc: 
    Type: String
  ClusterIdentifier:
    Type: Number
  ClusterMinSize:
    Type: Number
  ClusterMaxSize:
    Type: Number
  ClusterSubnets: 
    Type: List&lt;AWS::EC2::Subnet::Id&gt;    
  NodeImageId: 
    Type: AWS::EC2::Image::Id
  NodeOSVolumeSize: 
    Type: Number
    MinValue: 8
  NodeInstanceRole:
    Type: String
  NodeInstanceProfile: 
    Type: String
  NodeKeyName:
    Type: AWS::EC2::KeyPair::KeyName
  NodeInstanceType:
    Type: String
  NodeSecurityGroups:
    Type: List&lt;AWS::EC2::SecurityGroup::Id&gt;
  HanoverSchedule: 
    Type: String 

Mappings:
  InstanceStoreDevices:
    ""i3.2xlarge"":
      DEVS: ""/dev/nvme0n1""
    ""i3.4xlarge"":
      DEVS: ""/dev/nvme0n1 /dev/nvme1n1""
    ""i3.8xlarge"":
      DEVS: ""/dev/nvme0n1 /dev/nvme1n1 /dev/nvme2n1 /dev/nvme3n1""
    ""m5d.2xlarge"":
      DEVS: ""/dev/nvme1n1""
    ""m5d.4xlarge"":
      DEVS: ""/dev/nvme1n1 /dev/nvme2n1""
    ""c5d.2xlarge"":
      DEVS: ""/dev/nvme1n1""
    ""c5d.4xlarge"":
      DEVS: ""/dev/nvme1n1""
    ""c5d.9xlarge"":
      DEVS: ""/dev/nvme1n1""

Resources:  
  PlatformClusterLaunchConfiguration:
    Type: AWS::AutoScaling::LaunchConfiguration
    Metadata:
      AWS::CloudFormation::Init:
        configSets:
          all: [install_cfn, update_ecs_agent, faro_self_install]
        install_cfn:
          files:
            /etc/cfn/cfn-hup.conf:
              content: !Sub |
                [main]
                stack=${AWS::StackId}
                region=${AWS::Region}
                interval=5
              mode: '000400'
              owner: root
              group: root
            /etc/cfn/hooks.d/cfn-auto-reloader.conf:
              content: !Sub |
                [cfn-auto-reloader-hook]
                runas=root
                triggers=post.update
                path=Resources.PlatformClusterLaunchConfiguration.Metadata.AWS::CloudFormation::Init
                action=/opt/aws/bin/cfn-init -v --stack ${AWS::StackName} --resource PlatformClusterLaunchConfiguration --configsets all --region ${AWS::Region}
              mode: '000400'
              owner: root
              group: root
          services:
            sysvinit:
              cfn-hup:
                enabled: True
                ensureRunning: True
                files: [/etc/cfn/cfn-hup.conf, /etc/cfn/hooks.d/cfn-auto-reloader.conf]
        update_ecs_agent:
          commands:
            update_agent:
              command: yum update -y ecs-init
        faro_self_install:
          packages:
            yum:
              ruby: []
              aws-cli: []
              python27: []
              python27-boto3: []
              epel-release: [] 
              unzip: [] 
              ack: []
              wget: []
              jq: []
          commands:
            self_install:
              command: !Sub |
                #!/bin/bash
                yum -y --security update

                mkdir -p /etc/salt
                cd $(mktemp -d)
                REVNAME=$(basename ${CDRevisionLoc})
                aws --region ${AWS::Region} s3 cp ${CDRevisionLoc} $REVNAME.zip
                unzip -o $REVNAME.zip -d $REVNAME
                chmod +x $REVNAME/install.sh
                ./$REVNAME/install.sh
    Properties:
      BlockDeviceMappings:
        - DeviceName: /dev/xvda
          Ebs:
            VolumeSize: !Ref NodeOSVolumeSize
            VolumeType: gp2
            DeleteOnTermination: True
        - DeviceName: /dev/xvdcz
          VirtualName: ephemeral0
      EbsOptimized: True
      IamInstanceProfile: !Ref NodeInstanceProfile
      ImageId: !Ref NodeImageId
      InstanceMonitoring: True
      InstanceType: !Ref NodeInstanceType
      KeyName: !Ref NodeKeyName
      SecurityGroups: !Ref NodeSecurityGroups
      UserData: 
        Fn::Base64: 
          Fn::Sub: 
            - |
              Content-Type: multipart/mixed; boundary=""**""
              MIME-Version: 1.0

              --**
              MIME-Version: 1.0
              Content-Type: text/cloud-boothook; charset=""us-ascii""

              cloud-init-per once yum_update yum update -y
              cloud-init-per once install_aws_cfn_bootstrap yum -y install aws-cfn-bootstrap

              cloud-init-per instance custom_docker_options cat &lt;&lt;'EOF' &gt; /etc/sysconfig/docker
              DAEMON_MAXFILES=1048576
              DAEMON_PIDFILE_TIMEOUT=10
              OPTIONS=""--default-ulimit nofile=1024:4096""
              EOF

              cloud-init-per instance custom_docker_storage_options cat &lt;&lt;'EOF' &gt; /etc/sysconfig/docker-storage-setup
              DEVS=""${InstanceStoreDevices}""
              STORAGE_DRIVER=""devicemapper""
              VG=docker
              DATA_SIZE=99%FREE
              AUTO_EXTEND_POOL=yes
              LV_ERROR_WHEN_FULL=yes
              EXTRA_DOCKER_STORAGE_OPTIONS=""--storage-opt dm.fs=ext4 --storage-opt dm.use_deferred_deletion=true --storage-opt dm.basesize=20G""
              EOF

              cloud-init-per instance custom_ecs_options cat &lt;&lt;'EOF' &gt; /etc/ecs/ecs.config
              ECS_CLUSTER=${PlatformCluster}
              ECS_ENABLE_TASK_IAM_ROLE=true
              ECS_ENABLE_TASK_IAM_ROLE_NETWORK_HOST=true 
              ECS_DISABLE_PRIVILEGED=true 
              ECS_AVAILABLE_LOGGING_DRIVERS=[""json-file"", ""awslogs"", ""splunk""] 
              ECS_SELINUX_CAPABLE=false 
              ECS_APPARMOR_CAPABLE=false 
              ECS_ENGINE_TASK_CLEANUP_WAIT_DURATION=10m 
              ECS_CONTAINER_STOP_TIMEOUT=1m 
              ECS_DISABLE_IMAGE_CLEANUP=false 
              ECS_IMAGE_CLEANUP_INTERVAL=30m 
              ECS_IMAGE_MINIMUM_CLEANUP_AGE=30m 
              ECS_NUM_IMAGES_DELETE_PER_CYCLE=50 
              ECS_UPDATES_ENABLED=false 
              ECS_DISABLE_METRICS=false 
              ECS_ENABLE_CONTAINER_METADATA=true 
              ECS_AWSVPC_ADDITIONAL_LOCAL_ROUTES=[""169.254.120.120/32""] 
              EOF

              --**
              MIME-Version: 1.0
              Content-Type: text/x-shellscript; charset=""us-ascii""

              #!/bin/bash
              set -e

              # set sysctl before doing anything
              echo ""net.ipv4.conf.all.forwarding = 1"" &gt;&gt; /etc/sysctl.d/99-local.conf
              sysctl net.ipv4.conf.all.forwarding=1

              /opt/aws/bin/cfn-init -v --stack ${AWS::StackName} --resource PlatformClusterLaunchConfiguration --configsets all --region ${AWS::Region}
              /opt/aws/bin/cfn-signal -e $? --stack ${AWS::StackName} --resource PlatformClusterASG --region ${AWS::Region}
            - PlatformCluster: !Ref PlatformCluster
              InstanceStoreDevices: !FindInMap [InstanceStoreDevices, !Ref NodeInstanceType, ""DEVS""]


  PlatformClusterASG:
    Type: ""AWS::AutoScaling::AutoScalingGroup""
    DependsOn:
      - PlatformClusterLaunchConfiguration
    Properties:
      Cooldown: 900
      HealthCheckGracePeriod: 600
      HealthCheckType: EC2
      LaunchConfigurationName: !Ref PlatformClusterLaunchConfiguration
      VPCZoneIdentifier: !Ref ClusterSubnets
      MaxSize: !Ref ClusterMaxSize
      MinSize: !Ref ClusterMinSize
      DesiredCapacity: !Ref ClusterMinSize
      MetricsCollection:
        - 
          Granularity: 1Minute
      NotificationConfigurations:
        -
          NotificationTypes:
            - autoscaling:EC2_INSTANCE_LAUNCH_ERROR
            - autoscaling:EC2_INSTANCE_TERMINATE_ERROR
          TopicARN: !Ref PlatformClusterNotifications
      TerminationPolicies:
        - NewestInstance
      Tags:
        - Key: Name
          Value: !Sub ${Product}${Environment}-pcs${ClusterIdentifier}
          PropagateAtLaunch: true
        - Key: Product
          Value: !Ref Product
          PropagateAtLaunch: true
        - Key: Environment
          Value: !Ref Environment
          PropagateAtLaunch: true
        - Key: Service
          Value: !Sub gtn:${Environment}:pcs
          PropagateAtLaunch: true
        - Key: Schedule
          Value: !Ref HanoverSchedule
          PropagateAtLaunch: true
    CreationPolicy:
      ResourceSignal:
        Timeout: PT20M
    UpdatePolicy:
      AutoScalingRollingUpdate:
        WaitOnResourceSignals: True
        PauseTime: PT20M
      AutoScalingScheduledAction:
        IgnoreUnmodifiedGroupSizeProperties: True

  PlatformClusterTeardownLifecycleHook:
    Type: ""AWS::AutoScaling::LifecycleHook""
    DependsOn:
      - PlatformClusterASG
    Properties:
      AutoScalingGroupName: !Ref PlatformClusterASG
      DefaultResult: ABANDON
      HeartbeatTimeout: 900
      LifecycleTransition: autoscaling:EC2_INSTANCE_TERMINATING
      NotificationTargetARN: !Ref PlatformClusterLifecycleNotification
      RoleARN: !Ref NodeInstanceRole

Outputs:
  ECSAutoScalingGroup:
    Value: !Ref PlatformClusterASG

</code></pre>
",11723467.0,,11723467.0,,2019-10-10 07:18:26,2019-10-15 22:34:31,ECS cluster without Autoscaling,<amazon-ec2><aws-lambda><amazon-cloudformation><amazon-ecs><autoscaling>,1,3,,,,CC BY-SA 4.0,I have a requirement of creating an ECS Cluster without using autoscaling  This is because of a Dedicated Host (DH)  Tenancy=Host  is not supported with ASGs   DH is mainly for cost savings and for some cases because of the savings is worth doing even if we cant use ASGs  I understand this can be done using Macros and Custom Resources backed up with Lambda or using Troposphere to loop over the instance  But to start with any example of the same or any other approach would be really appreciated   Below is my appsec yaml template file:  
43294224,1,,,2017-04-08 12:42:02,,0,68,"<p>Would it be possible to use AWS Lambda to run unsafe user scripts? It should be:</p>

<ul>
<li>should have memory limit (seems like AWS Lambda has it).</li>
<li>should have execution timeout (seems like AWS Lambda also has it). Additionally it should be impossible to setup something like <code>setInterval(consumeLittleCpu, 500)</code>.</li>
<li>(would be nice to have) different scripts should be isolated.</li>
</ul>

<p>Would it be efficient? Like for example you have let's say 5000 different scripts and every one of it get executed once for every 1-30 sec?</p>

<p>How much more would it cost compared to do the same using usual AWS instance? Order of magnitude precision would be fine, like saying ""it will cost you no more than 10x same on usual AWS instance"" would be good enough.</p>
",231624.0,,1022141.0,,2017-04-09 22:12:43,2017-04-09 22:12:43,AWS Lambda for unsafe user scripts with node.js?,<node.js><lambda><aws-lambda>,1,0,,,,CC BY-SA 3.0,Would it be possible to use AWS Lambda to run unsafe user scripts  It should be:  should have memory limit (seems like AWS Lambda has it)  should have execution timeout (seems like AWS Lambda also has it)  Additionally it should be impossible to setup something like   (would be nice to have) different scripts should be isolated   Would it be efficient  Like for example you have lets say 5000 different scripts and every one of it get executed once for every 1-30 sec  How much more would it cost compared to do the same using usual AWS instance  Order of magnitude precision would be fine  like saying it will cost you no more than 10x same on usual AWS instance would be good enough  
44818898,1,,,2017-06-29 07:30:15,,2,300,"<p>My handler python-function call another python-function in a new thread and return result on the next line - like this:</p>

<pre><code>def aws_lambda_handler(event, context):
  threading.Thread(target=my_second_python_function).start()
  # thread for return without waiting

  return True  # this terminate Lambda with my second thread in process
</code></pre>

<p>I already have result for client and for speed I want to return it NOW without waiting a few seconds for finishing of the second python-function. But I found that AWS Lambda terminates after few milliseconds after <code>return</code> in the handler.</p>

<p>I can extract my second python-function to the new Lambda and call it before <code>return</code> but I think that this will little more expensive for money and time.</p>

<p>Maybe exists some way to not shutdown Lambda after <code>return</code> in handler? I tried long <code>timeout</code> but of course this is not for such case.</p>
",1879101.0,,,,,2017-06-29 07:30:15,AWS Lambda: manual lifecycle management - I do not want termination after `return` in handler,<python><python-3.x><amazon-web-services><aws-lambda><python-multithreading>,0,3,1.0,,,CC BY-SA 3.0,My handler python-function call another python-function in a new thread and return result on the next line - like this:  I already have result for client and for speed I want to return it NOW without waiting a few seconds for finishing of the second python-function  But I found that AWS Lambda terminates after few milliseconds after  in the handler  I can extract my second python-function to the new Lambda and call it before  but I think that this will little more expensive for money and time  Maybe exists some way to not shutdown Lambda after  in handler  I tried long  but of course this is not for such case  
44822837,1,44825819.0,,2017-06-29 10:37:05,,2,424,"<p>Django <a href=""https://docs.djangoproject.com/en/1.11/topics/cache/"" rel=""nofollow noreferrer"">advises</a> on using a Redis or Memcached cache for high traffic sites, to cut down on the work done by the server.</p>

<p>Apps running on <a href=""https://aws.amazon.com/lambda/?sc_channel=PS&amp;sc_campaign=acquisition_UK&amp;sc_publisher=google&amp;sc_medium=lambda_b&amp;sc_content=lambda_e&amp;sc_detail=amazon%20lambda&amp;sc_category=lambda&amp;sc_segment=186455828798&amp;sc_matchtype=e&amp;sc_country=UK&amp;s_kwcid=AL!4422!3!186455828798!e!!g!!amazon%20lambda&amp;ef_id=VrmsZQAAAJVNqU2K:20170629103204:s"" rel=""nofollow noreferrer"">Amazon Lambda</a> via <a href=""https://github.com/Miserlou/Zappa"" rel=""nofollow noreferrer"">Zappa</a> have fantastic horizontal scale-ability. There does not seem to be the need to minimise the processing efforts of a server when another server can be fired up easily and very cheaply.</p>

<p>Are Caches such as Memcached and Redis redundant when using server-less architectures?</p>

<p>UPDATE: having CDN (eg cloudfront) infront of your app is still adviseable (eg <a href=""https://github.com/Miserlou/Zappa#serving-static-files--binary-uploads"" rel=""nofollow noreferrer"">by Zappa</a>).</p>
",960471.0,,2612002.0,,2017-06-30 00:56:34,2017-06-30 00:56:34,are app caches redundant when using serverless architecture (e.g. Amazon Lambda via python Zappa),<django><caching><redis><memcached><aws-lambda>,1,0,,,,CC BY-SA 3.0,Django  on using a Redis or Memcached cache for high traffic sites  to cut down on the work done by the server  Apps running on  via  have fantastic horizontal scale-ability  There does not seem to be the need to minimise the processing efforts of a server when another server can be fired up easily and very cheaply  Are Caches such as Memcached and Redis redundant when using server-less architectures  UPDATE: having CDN (eg cloudfront) infront of your app is still adviseable (eg )  
43367012,1,,,2017-04-12 10:29:01,,0,519,"<p>I have written following code in Nodejs for Lambda and get an output which shows all the lambda function filtered by node and none-node run-time across all regions (as expected ).</p>

<p>But, somehow the code looks bulky with lot of function calls to different regions. Can the code be cut short or can the execution time be decreased in some way?</p>

<pre><code>var AWS = require('aws-sdk');
exports.handler = (event, context, callback) =&gt; {
    var boolValue = true;
    console.log(""Only regions with Lambda Functions are being shown"");
    var callBackCount;
    callBackCount = 0;
    response = {
        Lambda: []
    };
    usWest_1();
    usWest_2();
    usEast_1();
    euWest_1();
    euCentral_1();
    apSouthEast_1();
    apSouthEast_2();
    apNorthEast_1();
    apNorthEast_2();

    // functions for Clients of Each Region

    function usWest_1() {
        var lambda = new AWS.Lambda({
            apiVersion: '2015-03-31',
            region: 'us-west-1'
        });
        var region = ""us-west-1"";
        ///// function called getFunctions//////
        getFunctions(lambda, region);
    }

    function usWest_2() {
        var lambda = new AWS.Lambda({
            apiVersion: '2015-03-31',
            region: 'us-west-2'
        });
        var region = ""us-west-2"";
        ///// function called getFunctions//////
        getFunctions(lambda, region);
    }

    function usEast_1() {
        var lambda = new AWS.Lambda({
            apiVersion: '2015-03-31',
            region: 'us-east-1'
        });
        var region = ""us-east-1"";
        ///// function called getFunctions//////
        getFunctions(lambda, region);
    }

    function euWest_1() {
        var lambda = new AWS.Lambda({
            apiVersion: '2015-03-31',
            region: 'eu-west-1'
        });
        var region = ""eu-west-1"";
        ///// function called getFunctions//////
        getFunctions(lambda, region);
    }

    function euCentral_1() {
        var lambda = new AWS.Lambda({
            apiVersion: '2015-03-31',
            region: 'eu-central-1'
        });
        var region = ""eu-central-1"";
        ///// function called getFunctions//////
        getFunctions(lambda, region);
    }

    function apSouthEast_1() {
        var lambda = new AWS.Lambda({
            apiVersion: '2015-03-31',
            region: 'ap-southeast-1'
        });
        var region = ""ap-southeast-1"";
        ///// function called getFunctions//////
        getFunctions(lambda, region);
    }

    function apSouthEast_2() {
        var lambda = new AWS.Lambda({
            apiVersion: '2015-03-31',
            region: 'ap-southeast-2'
        });
        var region = ""ap-southeast-2"";
        ///// function called getFunctions//////
        getFunctions(lambda, region);
    }

    function apNorthEast_1() {
        var lambda = new AWS.Lambda({
            apiVersion: '2015-03-31',
            region: 'ap-northeast-1'
        });
        var region = ""ap-northeast-1"";
        ///// function called getFunctions//////
        getFunctions(lambda, region);
    }

    function apNorthEast_2() {
        var lambda = new AWS.Lambda({
            apiVersion: '2015-03-31',
            region: 'ap-northeast-2'
        });
        var region = ""ap-northeast-2"";
        ///// function called getFunctions//////
        getFunctions(lambda, region);
    }

    function getFunctions(lambda, region) {
        var regionName = region;
        var callbackData = {};
        var params = {};
        lambda.listFunctions(params, function(err, data) {
            if (err) {
                console.log(err, err.stack);
            } // an error occurred
            else {
                callbackData = JSON.stringify(data);
            } // successful response
            getNodeFunctions(callbackData, regionName);
        });
    }

    function getNodeFunctions(callbackData, regionName) {
        var allFunctions = {
            region: """",
            nodeFunctions: [],
            noneNodeFunctions: []
        };
        var paramData = (JSON.parse(callbackData));
        if (paramData.Functions[0] === undefined) {
            boolValue = false; // if no function in this region
            viewData(allFunctions, boolValue, regionName);
        } else {
            boolValue = true;
            paramData.Functions.forEach(function(functs) {
                if (functs.Runtime.startsWith(""node"")) {

                    var names = {
                        ""FunctionName"": '',
                        ""Runtime"": '',
                        ""FunctionArn"": '',
                        ""LastModified"": ''

                    };
                    names.FunctionName = (functs.FunctionName);
                    names.Runtime = (functs.Runtime);
                    names.FunctionArn = (functs.FunctionArn);
                    names.LastModified = (functs.LastModified);
                    allFunctions.nodeFunctions.push(names);
                } else {
                    var noneNode = {

                        ""FunctionName"": '',
                        ""Runtime"": '',
                        ""FunctionArn"": '',
                        ""LastModified"": ''

                    };
                    noneNode.FunctionName = (functs.FunctionName);
                    noneNode.Runtime = (functs.Runtime);
                    noneNode.FunctionArn = (functs.FunctionArn);
                    noneNode.LastModified = (functs.LastModified);
                    allFunctions.noneNodeFunctions.push(noneNode);
                }
            });
        }
        viewData(allFunctions, boolValue, regionName);
    }

    function viewData(allFunctions, boolValue, regionName) {
        callBackCount++;
        var testVal = boolValue;
        var thisRegion = regionName;
        var viewAllFunctions = allFunctions;
        viewAllFunctions.region = thisRegion;
        if (testVal === true) {
            response.Lambda.push(viewAllFunctions);
        } else {
            //  regions without lambda function
        }
        if (callBackCount == 9 &amp;&amp; response.Lambda[0] === undefined) {
            //console.log(""weird"");
        } else if (callBackCount == 9 &amp;&amp; response.Lambda[0] !== undefined) {
            console.log(JSON.stringify(response));
            callback(null, response);
            console.log(JSON.stringify(event));
        } else {
            //console.log(""something else"");
        }
    }


};
</code></pre>

<p><strong>UPDATE 1</strong></p>

<p>As suggested by <a href=""https://stackoverflow.com/users/174777/john-rotenstein"">John</a>, I added regions to an array. It looks more cleaner now. but still it contains few loops which can be eliminated. Like: to return callback, condition is checked whether all regions are executed or not, if callback is kept outside loop, it will return null response, as asynchronous Node will execute it first. Promises can be used to overcome that situation, but still not sure how to use promises when loops are nested and execution time is a concern. Also, can there be a way other than promises this can be done without adding more cost in execution?</p>

<pre><code> var AWS = require('aws-sdk');
 exports.handler = (event, context, callback) =&gt;
 {
    var boolValue = true;
    console.log(""Only regions with Lambda Functions are being shown"");
    var callBackCount;
    callBackCount = 0;
    response = {
            Lambda: []
    };
    var regionNames = ['us-west-1', 'us-west-2', 'us-east-1', 'eu-west-1', 'eu-central-1', 'ap-southeast-1',
            'ap-southeast-2', 'ap-northeast-1', 'ap-northeast-2'
    ];
    regionNames.forEach(function(region)
    {
            getFunctions(region);
    });

    function getFunctions(region)
    {
            var regionName = region;
            var info = {
                    apiVersion: '2015-03-31',
                    region: ''
            };
            info.region = regionName;
            var lambda = new AWS.Lambda(info);
            var callbackData = {};
            var params = {};
            lambda.listFunctions(params, function(err, data)
            {
                    if (err)
                    {
                            console.log(err, err.stack);
                    } // an error occurred
                    else
                    {
                            callbackData = JSON.stringify(data);
                    } // successful response
                    getNodeFunctions(callbackData, regionName);
            });
    }

    function getNodeFunctions(callbackData, regionName)
    {
            var allFunctions = {
                    region: """",
                    nodeFunctions: [],
                    noneNodeFunctions: []
            };
            var paramData = (JSON.parse(callbackData));
            if (paramData.Functions[0] === undefined)
            {
                    boolValue = false;
                    viewData(allFunctions, boolValue, regionName);
            }
            else
            {
                    boolValue = true;
                    paramData.Functions.forEach(function(functs)
                    {
                            if (functs.Runtime.startsWith(""node""))
                            {
                                    var names = {
                                            ""FunctionName"": '',
                                            ""Runtime"": '',
                                            ""FunctionArn"": '',
                                            ""LastModified"": ''
                                    };
                                    names.FunctionName = (functs.FunctionName);
                                    names.Runtime = (functs.Runtime);
                                    names.FunctionArn = (functs.FunctionArn);
                                    names.LastModified = (functs.LastModified);
                                    allFunctions.nodeFunctions.push(names);
                            }
                            else
                            {
                                    var noneNode = {
                                            ""FunctionName"": '',
                                            ""Runtime"": '',
                                            ""FunctionArn"": '',
                                            ""LastModified"": ''
                                    };
                                    noneNode.FunctionName = (functs.FunctionName);
                                    noneNode.Runtime = (functs.Runtime);
                                    noneNode.FunctionArn = (functs.FunctionArn);
                                    noneNode.LastModified = (functs.LastModified);
                                    allFunctions.noneNodeFunctions.push(noneNode);
                            }
                    });
            }
            viewData(allFunctions, boolValue, regionName);
    }

    function viewData(allFunctions, boolValue, regionName)
    {
            callBackCount++;
            var testVal = boolValue;
            var thisRegion = regionName;
            var viewAllFunctions = allFunctions;
            viewAllFunctions.region = thisRegion;
            if (testVal === true)
            {
                    response.Lambda.push(viewAllFunctions);
            }
            else
            {
                    //  regions without lambda function
            }
            if (callBackCount == 9 &amp;&amp; response.Lambda[0] === undefined)
            {
                    //console.log(""weird"");
            }
            else if (callBackCount == 9 &amp;&amp; response.Lambda[0] !== undefined)
            {
                    console.log(JSON.stringify(response));
                    callback(null, response);
                    console.log(JSON.stringify(event));
            }
            else
            {
                    //console.log(""something else"");
            }
    }
};
</code></pre>
",6265883.0,,-1.0,,2017-05-23 12:32:32,2017-04-13 10:46:14,Lambda code with least execution time,<node.js><amazon-web-services><aws-lambda>,0,4,,,,CC BY-SA 3.0,I have written following code in Nodejs for Lambda and get an output which shows all the lambda function filtered by node and none-node run-time across all regions (as expected )  But  somehow the code looks bulky with lot of function calls to different regions  Can the code be cut short or can the execution time be decreased in some way   UPDATE 1 As suggested by   I added regions to an array  It looks more cleaner now  but still it contains few loops which can be eliminated  Like: to return callback  condition is checked whether all regions are executed or not  if callback is kept outside loop  it will return null response  as asynchronous Node will execute it first  Promises can be used to overcome that situation  but still not sure how to use promises when loops are nested and execution time is a concern  Also  can there be a way other than promises this can be done without adding more cost in execution   
62442590,1,,,2020-06-18 04:50:39,,0,24,"<p>I am a student and new to AWS services.
I am working on a web-based project.
in the server part (AWS ). I need to monitor the outbound data used by each customer. and generate a bill based on the data used by that customer ( Not AWS billing ).
how I do that which AWS service fit for that please help me.</p>
",8373848.0,,,,,2020-08-03 23:18:44,How to measure outbound data used by each user ( customer ),<amazon-web-services><amazon-s3><amazon-ec2><aws-lambda>,1,2,,,,CC BY-SA 4.0,I am a student and new to AWS services  I am working on a web-based project  in the server part (AWS )  I need to monitor the outbound data used by each customer  and generate a bill based on the data used by that customer ( Not AWS billing )  how I do that which AWS service fit for that please help me  
62448306,1,,,2020-06-18 10:53:01,,0,85,"<p><strong>My case:</strong>
 1. Iam using lambda for connect to redshirt and SQL query execution
 2. using Sam and cloudformation template for this we application 
 3. Generally these SQL queries taking lot of execution time(i.e alredy
    we have optimised queries only). so by this cost will be more for
    single query execution only</p>

<p><strong>Question:</strong></p>

<ul>
<li>is there any alternate to lambda  to execute sql to reduce cost?</li>
</ul>

<p>any suggestions?</p>

<p>thanks</p>

<hr>
",13697321.0,,,,,2020-06-18 10:53:01,Alternative to aws lambda to run sql query execution to reduce cost,<amazon-web-services><aws-lambda><amazon-redshift>,0,3,,,,CC BY-SA 4.0,My case:  1  Iam using lambda for connect to redshirt and SQL query execution  2  using Sam and cloudformation template for this we application   3  Generally these SQL queries taking lot of execution time(i e alredy     we have optimised queries only)  so by this cost will be more for     single query execution only Question:  is there any alternate to lambda  to execute sql to reduce cost   any suggestions  thanks  
44863996,1,44877657.0,,2017-07-01 18:09:17,,0,119,"<p>I am making a simple app where the user can create a text post and optionally include media (pictures only for now, but videos in the future).</p>

<p>Currently, the user sends a <code>POST</code> request to API Gateway that invokes a Lambda function that inserts the post data into the database. This works great. API Gateway uses body mapping to format the event data. </p>

<p>In order to upload the media, it seems I have at least three options:</p>

<ol>
<li><p>Make the HTTP <code>POST</code> request as normal <em>THEN</em> upload the media to S3 (via Cloudfront?). </p>

<ul>
<li>S3 would trigger a Lambda function that updates the post record with the media url.</li>
<li>This would require at least 2 API invocations on the frontend. -.- </li>
<li>What if the media upload fails? I would have to invoke another Lambda function to delete the post. What if that fails? This is a rabbit hole.</li>
</ul></li>
<li><p>Upload the media to S3 (via Cloudfront?) <em>THEN</em> make the HTTP <code>POST</code> request. </p>

<ul>
<li>This would require 2 API invocations on the frontend. -.- </li>
<li>What if the <code>POST</code> request fails? I would have extra objects in my bucket. I suppose I could have a bucket cleaning scheduled task but ugg. </li>
<li>Would the S3 key not correspond to the <code>id</code> of the post? (<code>id</code> is generated on database insertion.)</li>
</ul></li>
<li><p>Upload the media with the HTTP <code>POST</code> request in <code>multipart/form</code>.</p>

<ul>
<li>This is how I have done it in the past, but I have also had a web server (not lambda). If the photo isn't huge, the transfer to S3 <em>should</em> be relatively quick and my lambda costs wont increase too drastically. But what if I decide to add video? Now my lambda invocations would be seconds long. </li>
</ul></li>
</ol>

<p>What is the best practice here? This seems like a common problem but all the guides I've found online are not concerned with the post data (only media data). </p>
",6946673.0,,,,,2017-07-03 04:07:28,Media upload on AWS with data - Lambda and S3,<amazon-web-services><amazon-s3><aws-lambda><amazon-cloudfront>,1,6,,,,CC BY-SA 3.0,I am making a simple app where the user can create a text post and optionally include media (pictures only for now  but videos in the future)  Currently  the user sends a  request to API Gateway that invokes a Lambda function that inserts the post data into the database  This works great  API Gateway uses body mapping to format the event data   In order to upload the media  it seems I have at least three options:  Make the HTTP  request as normal THEN upload the media to S3 (via Cloudfront )    S3 would trigger a Lambda function that updates the post record with the media url  This would require at least 2 API invocations on the frontend  - -  What if the media upload fails  I would have to invoke another Lambda function to delete the post  What if that fails  This is a rabbit hole   Upload the media to S3 (via Cloudfront ) THEN make the HTTP  request    This would require 2 API invocations on the frontend  - -  What if the  request fails  I would have extra objects in my bucket  I suppose I could have a bucket cleaning scheduled task but ugg   Would the S3 key not correspond to the  of the post  ( is generated on database insertion )  Upload the media with the HTTP  request in    This is how I have done it in the past  but I have also had a web server (not lambda)  If the photo isnt huge  the transfer to S3 should be relatively quick and my lambda costs wont increase too drastically  But what if I decide to add video  Now my lambda invocations would be seconds long     What is the best practice here  This seems like a common problem but all the guides Ive found online are not concerned with the post data (only media data)   
43459873,1,43460867.0,,2017-04-17 21:24:12,,2,392,"<p>I have an infrastructure consisting of the following services in a VPC (<em>except S3 and Transcoder obviously</em>):</p>

<ul>
<li>EC2 (webserver)</li>
<li>RDS (database)</li>
<li>Lambda function with Node.js</li>
<li>S3</li>
<li>Elastic Transcoder</li>
</ul>

<p>The scenario is the following:</p>

<ol>
<li>the user uploads a video to the S3 bucket directly</li>
<li>the upload triggers the lambda function which would create a new job in the Elastic Transcoder (using the AWS SDK) and update the resource's row in the database (RDS)</li>
</ol>

<p>The problem is that since RDS is not publicly accessible, the lambda needs to be in the same VPC as the RDS in order to allow connections to it. This also results in generic Internet connection loss in the lambda function which means that it cannot access Elastic Transcoder (since it's an <em>out-of-VPC</em> from the point of view of the VPC). Now, I had similar problem with S3 but it was fairly easy to solve that by adding an endpoint to the VPC which points to the S3, however, there's no such option for Elastic Transcoder (or any other service as a matter of fact).</p>

<p>I don't want to create a NAT gateway as it's pretty expensive for such a nonsense thing.</p>

<p>So the simple question is: how can I solve that Lambda can communicate with RDS and Elastic Transcoder at the same time?</p>

<p><em>P.S.: The lambda role contains the policy that has access to Elastic Transcoder's <code>*Job</code></em></p>
",2644098.0,,174777.0,,2017-07-07 11:47:45,2018-04-25 10:44:49,AWS - Lambda cannot access ElasticTranscoder,<node.js><amazon-web-services><aws-lambda><aws-vpc><amazon-elastic-transcoder>,1,0,,,,CC BY-SA 3.0,I have an infrastructure consisting of the following services in a VPC (except S3 and Transcoder obviously):  EC2 (webserver) RDS (database) Lambda function with Node js S3 Elastic Transcoder  The scenario is the following:  the user uploads a video to the S3 bucket directly the upload triggers the lambda function which would create a new job in the Elastic Transcoder (using the AWS SDK) and update the resources row in the database (RDS)  The problem is that since RDS is not publicly accessible  the lambda needs to be in the same VPC as the RDS in order to allow connections to it  This also results in generic Internet connection loss in the lambda function which means that it cannot access Elastic Transcoder (since its an out-of-VPC from the point of view of the VPC)  Now  I had similar problem with S3 but it was fairly easy to solve that by adding an endpoint to the VPC which points to the S3  however  theres no such option for Elastic Transcoder (or any other service as a matter of fact)  I dont want to create a NAT gateway as its pretty expensive for such a nonsense thing  So the simple question is: how can I solve that Lambda can communicate with RDS and Elastic Transcoder at the same time  P S : The lambda role contains the policy that has access to Elastic Transcoders  
58770277,1,,,2019-11-08 16:19:31,,0,55,"<p>I'm working on architecting a micro-service solution where most code will be C# and most likely Angular for any front end. My question is about message chaining. I am still figuring out what message broker to use; Azure Service Bus , RabbitMQ, etc.. There is a concept which I haven't found much about. </p>

<p>How do I handle cases when I want to fire a message when a specific set of messages have fired. An example but not part of my actual solution: I want to say Notify someone when pays a bill. We send a message <code>""PAIDBILL""</code>
  which will fire off microservices which will be processed independently:</p>

<ol>
<li><p>FinanceService to Debit the ledger and fire <code>""PaymentPosted""</code></p></li>
<li><p>EmailService: email Customer Saying thank you for paying the bill
<code>""CustomerPaymentEmailSent""</code> </p></li>
<li><p>DiscountService: Check if they get a discount for paying on time then send
<code>""CustomerCanGetPaymentDiscount""</code></p></li>
</ol>

<p>If all three messages have fired for the Same <code>PAIDBILL</code>: Message <code>""PaymentPosted""</code>,  <code>""CustomerPaymentEmailSent""</code>, <code>""CustomerCanGetPaymentDiscount""</code>
then I want to email the customer that they will get a discount on their next bill. It Must be done AFTER all three have tiggered and the order doesn't matter. How do I Schedule a new message to be sent <code>""EmailNextTimeDiscount""</code> message, without having to poll for what messages have fired every minute, hour, day?</p>

<p>All I can think of is to have a SQL table which marks that each one is complete (by locking the table) and when the last one is filled then send off the message. Would this be a good solution? I find it an anti-pattern for the micro-service &amp; message queue design.</p>
",90717.0,,7508700.0,,2019-11-11 10:46:54,2019-11-11 10:46:54,Fire Message Event Only when These other Messages have been sent,<microservices><message-queue><serverless-architecture>,1,0,,,,CC BY-SA 4.0,Im working on architecting a micro-service solution where most code will be C# and most likely Angular for any front end  My question is about message chaining  I am still figuring out what message broker to use; Azure Service Bus   RabbitMQ  etc   There is a concept which I havent found much about   How do I handle cases when I want to fire a message when a specific set of messages have fired  An example but not part of my actual solution: I want to say Notify someone when pays a bill  We send a message    which will fire off microservices which will be processed independently:  FinanceService to Debit the ledger and fire  EmailService: email Customer Saying thank you for paying the bill   DiscountService: Check if they get a discount for paying on time then send   If all three messages have fired for the Same : Message       then I want to email the customer that they will get a discount on their next bill  It Must be done AFTER all three have tiggered and the order doesnt matter  How do I Schedule a new message to be sent  message  without having to poll for what messages have fired every minute  hour  day  All I can think of is to have a SQL table which marks that each one is complete (by locking the table) and when the last one is filled then send off the message  Would this be a good solution  I find it an anti-pattern for the micro-service &amp; message queue design  
62491950,1,62492529.0,,2020-06-20 21:33:27,,4,315,"<p>Currently, my application resides in lambda which I serve using HTTP API (API Gateway V2). This setup exists in multiple regions. Meaning, API Gateway invokes lambda in the same region which accesses DynamoDB Global Table in the same region. I use Route 53 to serve nearest API Gateway to user.</p>
<p>The problem I faced: API Gateway doesn't support redirection from http to https. I can achieve this with CloudFront. But, it'll increase cost as well as latency.</p>
<p>Can I remove API Gateway from the equation and use Lambda@Edge to access DynamoDB Table near the user? Can CloudFront be used to replace API Gateway?</p>
",6910860.0,,2188922.0,,2020-06-21 00:39:40,2020-06-21 00:39:40,Is it possible to remove API Gateway from the equation to serve Lambda over public internet?,<amazon-web-services><aws-lambda><amazon-cloudfront>,1,0,,,,CC BY-SA 4.0,Currently  my application resides in lambda which I serve using HTTP API (API Gateway V2)  This setup exists in multiple regions  Meaning  API Gateway invokes lambda in the same region which accesses DynamoDB Global Table in the same region  I use Route 53 to serve nearest API Gateway to user  The problem I faced: API Gateway doesnt support redirection from http to https  I can achieve this with CloudFront  But  itll increase cost as well as latency  Can I remove API Gateway from the equation and use Lambda@Edge to access DynamoDB Table near the user  Can CloudFront be used to replace API Gateway  
58785818,1,,,2019-11-10 04:51:37,,0,335,"<p>I am considering sending my logs into StackDriver instead of CloudWatch. But from the docs, it seem to only describe how to do it with EC2. What about lambda? I prefer to send logs directly to StackDriver instead of StackDriver reading from CloudWatch to remove the CloudWatch costs entirely. </p>
",292291.0,,174777.0,,2019-11-10 11:34:08,2019-11-11 18:42:47,How to send Lambda logs to StackDriver instead of CloudWatch?,<aws-lambda><amazon-cloudwatch><stackdriver>,1,3,,,,CC BY-SA 4.0,I am considering sending my logs into StackDriver instead of CloudWatch  But from the docs  it seem to only describe how to do it with EC2  What about lambda  I prefer to send logs directly to StackDriver instead of StackDriver reading from CloudWatch to remove the CloudWatch costs entirely   
58980871,1,,,2019-11-21 17:47:01,,4,1095,"<p>I'm using Visual Studio to publish an ASP.NET Core 2.1 app to AWS Lambda (serverless). No matter what I've tried I cannot get CORS to work. </p>

<p>All I really want to do is add the header <code>access-control-allow-origin</code> globally to my web app. </p>

<p>Has anyone ever successfully added headers to an ASP.NET Core 2.1 Serverless app?</p>

<p><strong>Startup.cs</strong></p>

<pre><code>public void ConfigureServices(IServiceCollection services)
{
    // AddCors must be before AddMvc
    services.AddCors();

    services.AddMvc()
        .SetCompatibilityVersion(CompatibilityVersion.Version_2_1)
    );
}

public void Configure(IApplicationBuilder app, IHostingEnvironment env)
{
    // UseCors must be before UseMvc
    app.UseCors(builder =&gt; builder
        .AllowAnyOrigin()
        .AllowAnyMethod()
        .AllowAnyHeader()
        .AllowCredentials()
    );
    // Also tried this
    // app.UseCors(
    //    o =&gt; o.WithOrigins(""http://example.com"").AllowAnyMethod()
    //);

    app.UseMvc();
}
</code></pre>

<p>No CORS headers are added to my pages. I'm using Chrome dev tools to inspect my headers. I should see them on the homepage (for example) correct?</p>

<p>Any ideas? I'm dyin over here. Thanks!</p>

<p><strong>EDIT</strong></p>

<p>This application only uses API Gateway, Lambda and a few other services. It's great because I'm only charged when someone hits my app. There are no hourly charges. No EC2 or ELB which is amazing. </p>

<p>Also, I almost added this to my original post. The <a href=""https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-cors.html"" rel=""nofollow noreferrer"">article</a> @sturcotte06 references has a gotcha. </p>

<p>The API Gateway (automatically generated) uses the ANY method in a proxy integration. The above article says this...</p>

<blockquote>
  <p><strong>Important</strong></p>
  
  <p>When applying the above instructions to the ANY method in a proxy integration, any applicable CORS headers will not be set. Instead, your backend must return the applicable CORS headers, such as Access-Control-Allow-Origin.</p>
</blockquote>

<p>Ugh! So it's saying I must do this on the backend (Startup.cs right?) which is exactly what seems to get ignored when published. </p>
",848148.0,,848148.0,,2019-11-21 18:16:53,2019-11-21 18:41:32,ASP.NET Core AWS Serverless and CORS,<c#><asp.net-core><cors><aws-serverless>,1,13,0.0,,,CC BY-SA 4.0,Im using Visual Studio to publish an ASP NET Core 2 1 app to AWS Lambda (serverless)  No matter what Ive tried I cannot get CORS to work   All I really want to do is add the header  globally to my web app   Has anyone ever successfully added headers to an ASP NET Core 2 1 Serverless app  Startup cs  No CORS headers are added to my pages  Im using Chrome dev tools to inspect my headers  I should see them on the homepage (for example) correct  Any ideas  Im dyin over here  Thanks  EDIT This application only uses API Gateway  Lambda and a few other services  Its great because Im only charged when someone hits my app  There are no hourly charges  No EC2 or ELB which is amazing   Also  I almost added this to my original post  The  @sturcotte06 references has a gotcha   The API Gateway (automatically generated) uses the ANY method in a proxy integration  The above article says this     Important When applying the above instructions to the ANY method in a proxy integration  any applicable CORS headers will not be set  Instead  your backend must return the applicable CORS headers  such as Access-Control-Allow-Origin   Ugh  So its saying I must do this on the backend (Startup cs right ) which is exactly what seems to get ignored when published   
60253671,1,,,2020-02-16 22:10:15,,0,355,"<p>I have an AWS Lambda function written in python that is starting to get a bit too big. It often runs over the AWS Lambda limit of 15 minutes. </p>

<p>The function is responsible for making loads of API calls, and unfortunately I have not taken the time to make them all async yet.</p>

<p>The workflow for the function looks something roughly like this:</p>

<ol>
<li>Receive data for xxx clients (1 api call)</li>
<li>Place API call for each client and wait for response (xxx api calls)</li>
<li>Based on #2 API call response, place yet another API call for each client and wait for response (xxx api calls)</li>
<li>Store response from #3 in DynamoDB (xxx DB updates)</li>
</ol>

<p>Initially my plan was to just keep everything in 1 lambda function, and modify my code such that all API calls run async, and to do one big dynamoDB batch update at the end.</p>

<p>However, it occurs to me that theoretically this function could still grow too large in the future, if we start needing to run it for enormous numbers of clients. Additionally, in my opinion, managing all those python async calls can be a bit cumbersome in specific situations.</p>

<p>My second approach, is to chain together lambda functions. That is, create a second lambda function, let's name it ""order_for_single_client"", which perform steps #2-4 for just one client.</p>

<p>The top level lambda function will gather data on all clients, and for each one, make a separate lambda call to ""order_for_single_client"". So, if we need to order for 500 clients, the top level function will just make 500 separate lambda calls. Naturally, all the async behavior should take care of itself on the end of AWS since it will run the lambda functions in parallel. (with the exception of the batch dynamoDB update we wanted, but we can worry about that later).</p>

<ol>
<li><p>What is the preferred way of handling this situation from an architecture perspective? Splitting things up into a lambda chain, or trying to speed things up using async inside a single lambda function?</p></li>
<li><p>If we were to implement the lambda chain, will it be more or less expensive to run at scale compared to the single function async approach?</p></li>
<li><p>Is there a third design solution to this problem that I have not considered? (Apart from spinning up an EC2 instance, which I would prefer not to do).</p></li>
</ol>
",10328030.0,,10328030.0,,2020-02-16 22:15:38,2020-02-16 22:16:48,Chaining lambda functions vs. Making program async,<python><amazon-web-services><asynchronous><aws-lambda><python-asyncio>,1,0,,,,CC BY-SA 4.0,I have an AWS Lambda function written in python that is starting to get a bit too big  It often runs over the AWS Lambda limit of 15 minutes   The function is responsible for making loads of API calls  and unfortunately I have not taken the time to make them all async yet  The workflow for the function looks something roughly like this:  Receive data for xxx clients (1 api call) Place API call for each client and wait for response (xxx api calls) Based on #2 API call response  place yet another API call for each client and wait for response (xxx api calls) Store response from #3 in DynamoDB (xxx DB updates)  Initially my plan was to just keep everything in 1 lambda function  and modify my code such that all API calls run async  and to do one big dynamoDB batch update at the end  However  it occurs to me that theoretically this function could still grow too large in the future  if we start needing to run it for enormous numbers of clients  Additionally  in my opinion  managing all those python async calls can be a bit cumbersome in specific situations  My second approach  is to chain together lambda functions  That is  create a second lambda function  lets name it order_for_single_client  which perform steps #2-4 for just one client  The top level lambda function will gather data on all clients  and for each one  make a separate lambda call to order_for_single_client  So  if we need to order for 500 clients  the top level function will just make 500 separate lambda calls  Naturally  all the async behavior should take care of itself on the end of AWS since it will run the lambda functions in parallel  (with the exception of the batch dynamoDB update we wanted  but we can worry about that later)   What is the preferred way of handling this situation from an architecture perspective  Splitting things up into a lambda chain  or trying to speed things up using async inside a single lambda function  If we were to implement the lambda chain  will it be more or less expensive to run at scale compared to the single function async approach  Is there a third design solution to this problem that I have not considered  (Apart from spinning up an EC2 instance  which I would prefer not to do)   
43601226,1,43601677.0,,2017-04-25 03:45:16,,6,5585,"<p>I would like to call the <code>aws s3 sync</code> command from within an AWS Lambda function with a runtime version of Python 3.6. How can I do this?</p>

<blockquote>
  <p>Why don't you just use the included boto3 SDK?</p>
</blockquote>

<ul>
<li><a href=""https://github.com/boto/boto3/issues/358"" rel=""noreferrer"">boto3 does not have an equivalent to the <code>sync</code> command</a></li>
<li><a href=""https://github.com/boto/boto3/issues/548#issuecomment-200450364"" rel=""noreferrer"">boto3 does not automatically find MIME types</a> (""If you do not provide anything for ContentType to ExtraArgs, the end content type will always be binary/octet-stream."")</li>
<li><a href=""http://docs.aws.amazon.com/cli/latest/reference/s3/sync.html"" rel=""noreferrer"">aws cli <em>does</em> automatically find MIME types</a> (""By default the mime type of a file is guessed when it is uploaded"")</li>
</ul>

<blockquote>
  <p><a href=""https://stackoverflow.com/a/33525082/3394807"">Architecturally this doesn't make sense!</a></p>
</blockquote>

<p>For my use case I think it makes sense architecturally and financially, but I'm open to alternatives. <a href=""https://github.com/pjgranahan/pjgranahan.comLambda"" rel=""noreferrer"">My Lambda function</a>:</p>

<ul>
<li>downloads Git and Hugo</li>
<li>downloads <a href=""https://github.com/pjgranahan/pjgranahan.com"" rel=""noreferrer"">my repository</a></li>
<li>runs Hugo to generate my small (&lt;100 pages) website</li>
<li>uploads the generated files to s3</li>
</ul>

<p>Right now, I'm able to do all of the above on a 1536 MB (the most powerful) Lambda function in around 1-2 seconds. This function is only triggered when I commit changes to my website, so it's inexpensive to run.</p>

<blockquote>
  <p>Maybe it is already installed in the Lambda environment?</p>
</blockquote>

<p>As of the time of this writing, it is not.</p>
",3394807.0,,-1.0,,2017-05-23 12:17:57,2017-04-25 23:30:58,How can I run the aws-cli in an AWS Lambda Python 3.6 environment?,<python-3.x><amazon-web-services><aws-lambda><aws-cli>,1,0,4.0,,,CC BY-SA 3.0,I would like to call the  command from within an AWS Lambda function with a runtime version of Python 3 6  How can I do this   Why dont you just use the included boto3 SDK      (If you do not provide anything for ContentType to ExtraArgs  the end content type will always be binary/octet-stream )  (By default the mime type of a file is guessed when it is uploaded)     For my use case I think it makes sense architecturally and financially  but Im open to alternatives  :  downloads Git and Hugo downloads  runs Hugo to generate my small (&lt;100 pages) website uploads the generated files to s3  Right now  Im able to do all of the above on a 1536 MB (the most powerful) Lambda function in around 1-2 seconds  This function is only triggered when I commit changes to my website  so its inexpensive to run   Maybe it is already installed in the Lambda environment   As of the time of this writing  it is not  
60179473,1,60179605.0,,2020-02-12 00:41:19,,-1,51,"<p>I am trying the understand how the AWS Lambda charge. I know that the first 1 million requests and the first 400000GB-second compute time if free are free. After that, it will charge 0.20USD per million requests and 400,000 seconds if the function is 1GB RAM. What I am not clear here is how the computing time is charged.</p>

<p>Let's say one request is ""0.0000002USD"". One the client makes the request, it already charged 0.0000002USD no matter how long it was run. The computing time is charged once the function starts running based on how much memory it consumes and how long it was run for. Is that correct? Let's say the function was run for 2 hours, but it only consumed 1GB RAM (which is not realistic - just for the educational purpose), so it is gonna charge me 0.0000002USD + (cost of execution/ computing for 400,000 seconds). Am I right?</p>

<p>Can anyone confirm and explain?</p>
",4675736.0,,,,,2020-02-12 01:18:57,How does the AWS Lambda charge?,<amazon-web-services><aws-lambda>,2,1,,,,CC BY-SA 4.0,I am trying the understand how the AWS Lambda charge  I know that the first 1 million requests and the first 400000GB-second compute time if free are free  After that  it will charge 0 20USD per million requests and 400 000 seconds if the function is 1GB RAM  What I am not clear here is how the computing time is charged  Lets say one request is 0 0000002USD  One the client makes the request  it already charged 0 0000002USD no matter how long it was run  The computing time is charged once the function starts running based on how much memory it consumes and how long it was run for  Is that correct  Lets say the function was run for 2 hours  but it only consumed 1GB RAM (which is not realistic - just for the educational purpose)  so it is gonna charge me 0 0000002USD + (cost of execution/ computing for 400 000 seconds)  Am I right  Can anyone confirm and explain  
60201185,1,60391747.0,,2020-02-13 05:47:06,,0,161,"<p>I have an Athena table of data in S3 that acts as a source table, with columns <code>id</code>, <code>name</code>, <code>event</code>. For every unique <code>name</code> value in this table, I would like to output a new table with all of the rows corresponding to that <code>name</code> value, and save to a different bucket in S3. This will result in n new files stored in S3, where n is also the number of unique <code>name</code> values in the source table.</p>

<p>I have tried single Athena queries in Lambda using <code>PARTITION BY</code> and CTAS queries, but can't seem to get the result that I wanted. It seems that AWS Glue may be able to get my expected result, but I've read online that it's more expensive, and that perhaps I may be able to get my expected result using Lambda.</p>

<p><strong>How can I store a new file (JSON format, preferably) that contains all rows corresponding to each unique <code>name</code> in S3?</strong></p>

<p>Preferably I would run this once a day to update the data stored by <code>name</code>, but the question above is the main concern for now.</p>
",11955626.0,,11955626.0,,2020-02-19 00:46:22,2020-02-25 09:51:49,"For each distinct value in col_a, yield a new table",<amazon-web-services><amazon-s3><aws-lambda><aws-glue><amazon-athena>,1,0,,,,CC BY-SA 4.0,I have an Athena table of data in S3 that acts as a source table  with columns       For every unique  value in this table  I would like to output a new table with all of the rows corresponding to that  value  and save to a different bucket in S3  This will result in n new files stored in S3  where n is also the number of unique  values in the source table  I have tried single Athena queries in Lambda using  and CTAS queries  but cant seem to get the result that I wanted  It seems that AWS Glue may be able to get my expected result  but Ive read online that its more expensive  and that perhaps I may be able to get my expected result using Lambda  How can I store a new file (JSON format  preferably) that contains all rows corresponding to each unique  in S3  Preferably I would run this once a day to update the data stored by   but the question above is the main concern for now  
61437558,1,,,2020-04-26 08:03:19,,1,220,"<p>I am looking to work on real-time chat application in a serverless architecture. </p>

<p>I am new to AWS so did some research on it but I found may option to do so.</p>

<p>Like: </p>

<ol>
<li>app sync</li>
<li>WebSocket</li>
<li>elasticache redis</li>
</ol>

<p>I am a bit confused about which I should go with.</p>

<p>I am looking for a cost-effective(<strong>low charges</strong>) setup for my app.</p>
",2170891.0,,2170891.0,,2020-04-26 11:25:36,2020-04-26 11:25:36,Chat application in AWS,<amazon-web-services><websocket><aws-lambda><chat><aws-appsync>,0,1,,,,CC BY-SA 4.0,I am looking to work on real-time chat application in a serverless architecture   I am new to AWS so did some research on it but I found may option to do so  Like:   app sync WebSocket elasticache redis  I am a bit confused about which I should go with  I am looking for a cost-effective(low charges) setup for my app  
43828054,1,,,2017-05-07 05:12:01,,4,1171,"<p>I have an AWS Lambda function which calls a deep learning function on <a href=""https://algorithmia.com/"" rel=""nofollow noreferrer"">Algorithmia</a>, does some post processing on the results and then returns some data. Algorithmia provides a <a href=""https://github.com/algorithmiaio/algorithmia-python"" rel=""nofollow noreferrer"">python client</a> which I am using that just makes things a little easier to send a request to an algorithm on the Algorithmia platform.</p>

<p>The problem is as follows: When an Algorithmia function hasn't been called for a while it is unloaded and the first call to warm it up (cold start) takes a while, possibly 30 seconds. If my Lambda function is going to be waiting for 30 seconds for a response whenever it happens to be triggering the Algorithmia function from a cold start that's going to be very expensive and wasteful.</p>

<p>Is there some way to send off a HTTP request in Lambda and when the request is finished the results are piped into a new Lambda function so as to not require a Lambda function to be waiting the entire time and wasting resources? I'd expect not as I'm not sure how that would practically work - does anyone have other ideas as to how to avoid waiting a while for a response and wasting Lambda resources?</p>

<p><strong>Edit:</strong> In most cases (except obviously the ones where the Algorithmia algorithm takes a while to load from cold start) latency is an issue and I can't afford to increase latency by doing some workaround method with the Algorithmia function writing it's response to S3 (for example) and then triggering a Lambda function.</p>
",3474089.0,,3474089.0,,2017-05-07 05:18:19,2017-05-09 11:54:07,AWS Lambda long running http requests,<python><amazon-web-services><lambda><aws-lambda><algorithmia>,2,5,,,,CC BY-SA 3.0,I have an AWS Lambda function which calls a deep learning function on   does some post processing on the results and then returns some data  Algorithmia provides a  which I am using that just makes things a little easier to send a request to an algorithm on the Algorithmia platform  The problem is as follows: When an Algorithmia function hasnt been called for a while it is unloaded and the first call to warm it up (cold start) takes a while  possibly 30 seconds  If my Lambda function is going to be waiting for 30 seconds for a response whenever it happens to be triggering the Algorithmia function from a cold start thats going to be very expensive and wasteful  Is there some way to send off a HTTP request in Lambda and when the request is finished the results are piped into a new Lambda function so as to not require a Lambda function to be waiting the entire time and wasting resources  Id expect not as Im not sure how that would practically work - does anyone have other ideas as to how to avoid waiting a while for a response and wasting Lambda resources  Edit: In most cases (except obviously the ones where the Algorithmia algorithm takes a while to load from cold start) latency is an issue and I cant afford to increase latency by doing some workaround method with the Algorithmia function writing its response to S3 (for example) and then triggering a Lambda function  
44119835,1,,,2017-05-22 18:33:19,,0,401,"<p>I'd like to achieve near real time search for a document service, and here is my idea:</p>

<ol>
<li>I plan to use DynamoDB as my primary document store; </li>
<li>and then whenever a new document update happens, an event in DynamoDB stream is created;</li>
<li>I'd like to ask CloudSearch to pick up the events in the stream and update the index in CloudSearch</li>
</ol>

<p>My question is how to integrate DynamoDB stream with CloudSearch. I feel I could use Lambda function in between (i.e., trigger a lambda function, which execute a write/update the index operation, to process an event in the stream). I would work, but I just feel it may be an expensive way to achieve my goal (because lambda cost $$). </p>

<p>Does Amazon provide any hook that directly integrate DynamoDB stream with CloudSearch? I am wondering this approach because of the following illustration figure (it clearly implied that CloudSearch and Lambda are different integration point).<a href=""https://i.stack.imgur.com/2yXhV.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2yXhV.jpg"" alt=""enter image description here""></a></p>
",152859.0,,,,,2017-05-22 18:33:19,What is the best way to integrate DynamoDB stream with CloudSearch?,<amazon-dynamodb><aws-lambda><amazon-cloudsearch><amazon-dynamodb-streams>,0,2,,2017-05-23 17:21:24,,CC BY-SA 3.0,Id like to achieve near real time search for a document service  and here is my idea:  I plan to use DynamoDB as my primary document store;  and then whenever a new document update happens  an event in DynamoDB stream is created; Id like to ask CloudSearch to pick up the events in the stream and update the index in CloudSearch  My question is how to integrate DynamoDB stream with CloudSearch  I feel I could use Lambda function in between (i e   trigger a lambda function  which execute a write/update the index operation  to process an event in the stream)  I would work  but I just feel it may be an expensive way to achieve my goal (because lambda cost $$)   Does Amazon provide any hook that directly integrate DynamoDB stream with CloudSearch  I am wondering this approach because of the following illustration figure (it clearly implied that CloudSearch and Lambda are different integration point)  
44126820,1,,,2017-05-23 05:55:58,,1,174,"<p>I'm trying to create an AWS Lambda function that is to be invoked by an Amazon Echo Skill. The Lambda function should connect to an MQTT broker, which is not in the AWS, and I noticed that the Lambda function alone was not able to access to the external resource. I have tried several configurations and it could connect to the broker after creating an NAT gateway. However, the NAT gateway is a charged service and I wonder if it is necessary.</p>

<p>Here is my question. Is it necessary to have the charged NAT gateway in my situation for the Lambda function to access to the external resource? If not, what else should I do? I would welcome any idea that would let an Echo Skill publish an MQTT message to my MQTT broker, even without the AWS Lambda.</p>

<p>Thanks.</p>
",8051581.0,,,,,2019-02-06 13:54:33,Is it necessary for AWS Lambda to have charged NAT gateway to access external resources?,<amazon-web-services><aws-lambda><mqtt><nat>,2,0,0.0,,,CC BY-SA 3.0,Im trying to create an AWS Lambda function that is to be invoked by an Amazon Echo Skill  The Lambda function should connect to an MQTT broker  which is not in the AWS  and I noticed that the Lambda function alone was not able to access to the external resource  I have tried several configurations and it could connect to the broker after creating an NAT gateway  However  the NAT gateway is a charged service and I wonder if it is necessary  Here is my question  Is it necessary to have the charged NAT gateway in my situation for the Lambda function to access to the external resource  If not  what else should I do  I would welcome any idea that would let an Echo Skill publish an MQTT message to my MQTT broker  even without the AWS Lambda  Thanks  
44158457,1,,,2017-05-24 12:29:19,,1,1195,"<p>I have the following use case scenario for which I am considering aws services to see how I can come up with a scalable solution. Thanks in advance for your help!</p>

<p>Scenario: 
Users can sign up to an application(which is named say 'Let's Remind' or something else) using their email and phone. 
The app does one thing that is to send email and sms alerts to user.
User can create n number of tasks for which he wants to be reminded. For instance he can set up a 
monthly reminder for paying card dues. Currently the value of n is from 5 to 10 per user.
The notifications are flexible meaning it can be daily, weekly, monthly, bi-weekly. User can also 
specify the start date of a notification. The end date is the date when the event is due (for instance 
the day the card payment is due). Once this date is expired the notification is rendered inactive for 
the current month. 
For weekly,daily,bi-weekly notifications, the notifications are deleted once the event date is passed. 
These are not recurring in nature.
For monthly recurring events such as payment of apartment rent etc, notification itself is not 
deleted but rendered inactive after the event due date. Once the next event cycle (typically next 
month billing cycle for payments use case) starts, the notification comes back to life and starts all 
over again.
Use can delete any event anytime he wants. If an event is deleted, the notifications for that event 
will be deleted as well.</p>

<p>First of all, I hope the use case is clear. Now here's my thoughts so far about solving this use case -</p>

<p>1) Use SNS since I need to send email and sms both. SES only supports emails.
2) When a user registers for the app, create 2 subscriptions(one for his email and one for his sms endpoint) and also create a topic for the user(maybe a dynamically generated random userid)
3) Once user creates an event (e.g. reminder for monthly apartment rental), store the event data such as userid, startdate, duedate, frequency, isactive in a dynamodb table.
4) Create a lambda function that will wake up when an entry is written to the dynamodb table (step 3); it will do the following -
i) it will read the event data from the dynamodb table
ii) determine the next date of the notification to be sent based on the current date and event 
data
iii) For active events (check isActive column of the dynamodb record) create a scheduled cron 
expression rule based on ii above in cloudwatch events and add the 
target as the user's topic (created in step 2 above). For now, the notification message is 
static.</p>

<p>I have some doubts/queries about step iii -
Is it possible to create cloudwatch event cron rule dynamically and add the user's topic as target dynamically as I described? Or is it better to trigger a second lambda function dedicated for sending messages to the user's topic using SNS notification? Which approach will be better for this use case?
If user base grows large, is it recommended to create one topic per user?
Am I on the right track with my approach above in general from a scalability point of view?
If not, can anyone suggest any better idea for implementing this use case?</p>

<p>Thanks in advance!</p>
",2943994.0,,2943994.0,,2017-05-31 18:19:58,2017-05-31 18:19:58,send periodic notification using aws sns and lambda,<amazon-web-services><aws-lambda><amazon-sns>,1,3,,,,CC BY-SA 3.0,I have the following use case scenario for which I am considering aws services to see how I can come up with a scalable solution  Thanks in advance for your help  Scenario:  Users can sign up to an application(which is named say Lets Remind or something else) using their email and phone   The app does one thing that is to send email and sms alerts to user  User can create n number of tasks for which he wants to be reminded  For instance he can set up a  monthly reminder for paying card dues  Currently the value of n is from 5 to 10 per user  The notifications are flexible meaning it can be daily  weekly  monthly  bi-weekly  User can also  specify the start date of a notification  The end date is the date when the event is due (for instance  the day the card payment is due)  Once this date is expired the notification is rendered inactive for  the current month   For weekly daily bi-weekly notifications  the notifications are deleted once the event date is passed   These are not recurring in nature  For monthly recurring events such as payment of apartment rent etc  notification itself is not  deleted but rendered inactive after the event due date  Once the next event cycle (typically next  month billing cycle for payments use case) starts  the notification comes back to life and starts all  over again  Use can delete any event anytime he wants  If an event is deleted  the notifications for that event  will be deleted as well  First of all  I hope the use case is clear  Now heres my thoughts so far about solving this use case - 1) Use SNS since I need to send email and sms both  SES only supports emails  2) When a user registers for the app  create 2 subscriptions(one for his email and one for his sms endpoint) and also create a topic for the user(maybe a dynamically generated random userid) 3) Once user creates an event (e g  reminder for monthly apartment rental)  store the event data such as userid  startdate  duedate  frequency  isactive in a dynamodb table  4) Create a lambda function that will wake up when an entry is written to the dynamodb table (step 3); it will do the following - i) it will read the event data from the dynamodb table ii) determine the next date of the notification to be sent based on the current date and event  data iii) For active events (check isActive column of the dynamodb record) create a scheduled cron  expression rule based on ii above in cloudwatch events and add the  target as the users topic (created in step 2 above)  For now  the notification message is  static  I have some doubts/queries about step iii - Is it possible to create cloudwatch event cron rule dynamically and add the users topic as target dynamically as I described  Or is it better to trigger a second lambda function dedicated for sending messages to the users topic using SNS notification  Which approach will be better for this use case  If user base grows large  is it recommended to create one topic per user  Am I on the right track with my approach above in general from a scalability point of view  If not  can anyone suggest any better idea for implementing this use case  Thanks in advance  
44208268,1,44210481.0,,2017-05-26 18:42:55,,8,6160,"<p>I have a Python 3 project which I am trying to deploy to AWS Lambda via AWS Codestar -> Codepipeline -> Codebuild -> Cloudformation. </p>

<p>My project (which really just consists of a simple API Gateway handler method) imports a Python 3 (requires 3) project (newspaper).  I am using Virtualenv 15.1.0 on my home computer and if I install Newspaper with Python 3.5 and then upload to Lambda (Python 3.6 runtime), it throws errors related to PIL / Pillow.  </p>

<p>First it says it can't find _image, which appears to be resolved by deleting the PIL directory in site-packages, however that just results in it throwing the error that it can't find PIL.</p>

<p>If, however, I build with Python 3.6 and then upload to Lambda, it works just fine (whether I delete PIL or not).</p>

<p>So, that appears to me that I can't install Newspaper with 3.5 and try to execute in a 3.6 runtime.</p>

<p>So, now I am trying to deploy via Codestar, however Codestar seems to default to aws/codebuild/eb-nodejs-4.4.6-amazonlinux-64:2.1.3, even for Python projects, and all it seems to have available in the Yum repository is Python 3.5 and of course Lambda only has the 3.6 runtime.</p>

<p>Even if I switch the image within Codebuild itself, there don't seem to be any images built with the Python3.6 runtime (according to the documentation). Even the Docker images seem to lack Python 3.6.</p>

<p>So, I am trying to install Python 3.6 in Codebuild during the INSTALL phase in my buildspec.yml file, however I can't find the python3* executable after the install.</p>

<p>The only other thing I can think of is to create the Codestar project, edit codebuild to use Ubuntu and then install everything (just like I did locally), but there is no way to do that from within Codestar and I feel like that may bring me down a rabbit hole and that's hardly automated.  Is there a way to make that configuration as code from within my project?</p>

<p><strong>EDIT</strong>
Attempting to build and install Python 3.6 from source works, but then when trying to install Pip, I get errors saying SSL was not installed.  And when looking back at the build logs, it seems other ""bits"" were not installed as well.</p>

<p>So, my questions here are:</p>

<ul>
<li>How do I get Python 3.6 into a Codebuild environment provisioned from a Codestar project?</li>
<li>Should I continue trying to build it from source or make the switch to the Ubuntu environment?</li>
<li>How can I automatically configure the image / environment within my code/project?</li>
</ul>

<p><strong>EDIT 1</strong> For anyone else, my complete buildspec.yml for installing and using Python3.6 is below. Note, it keeps everything as quiet as possible in order to reduce the log messages, reduce Cloudwatch cost and speed up the process.  I ended up shaving about 90 seconds off the whole process by doing that (installing Python and building my app).  Since CodeBuild charges based on time spent, this is crucial.</p>

<pre><code>version: 0.2

phases:
  install:
    commands:
      - yum -qye 0 update
      - yum -qye 0 groupinstall development
      - yum -y install python-devel
      - yum -qye 0 install libxml2-devel libxslt-devel libjpeg-devel zlib-devel libpng-devel openssl-devel sqlite-devel
      - export HOME_DIR=`pwd`
      # I would recommend hosting the tarball in an uncompressed format on S3 in order to speed up the download and decompression
      - wget --no-verbose https://www.python.org/ftp/python/3.6.1/Python-3.6.1.tgz
      - tar -xzf Python-3.6.1.tgz
      - cd Python-3.6.1
      - ./configure -q --enable-loadable-sqlite-extensions
      - make --silent -j2
      - make altinstall --silent
      - cd $HOME_DIR
      - rm Python-3.6.1.tgz
      - rm -rf Python-3.6.1/
      - ln -s /usr/local/bin/python3.6 /usr/bin/python3
      - python3 -m pip install virtualenv
      - pip3 install -U nltk
  pre_build:
    commands:
      - cd $HOME_DIR
      # Start a virtualenv and activate
      - virtualenv -p /usr/bin/python3 $VIRTUAL_ENV_DIR_NAME
      - source $VIRTUAL_ENV_DIR_NAME/bin/activate
      - $VIRTUAL_ENV_DIR_NAME/bin/pip3.6 install nltk
      # If you plan to use any separate resources on Codecommit, you need to configure git
      - git config --global credential.helper '!aws codecommit credential-helper $@'
      - git config --global credential.UseHttpPath true
      # git clone whatever you need
  build:
    commands:
      - cd $HOME_DIR
      - mv $VIRTUAL_ENV/lib/python3.6/site-packages/* .
      - aws cloudformation package --template template.yml --s3-bucket $S3_BUCKET --output-template template-export.json
artifacts:
  type: zip
  files:
    - template-export.json
</code></pre>
",4426091.0,,4426091.0,,2017-06-01 02:16:48,2018-05-11 14:43:38,"Python 3.6 unavailable in AWS CodeBuild, Python 3.5 unavailable in AWS Lambda",<python><amazon-web-services><aws-lambda><aws-codebuild><aws-codestar>,3,0,1.0,,,CC BY-SA 3.0,I have a Python 3 project which I am trying to deploy to AWS Lambda via AWS Codestar -&gt; Codepipeline -&gt; Codebuild -&gt; Cloudformation   My project (which really just consists of a simple API Gateway handler method) imports a Python 3 (requires 3) project (newspaper)   I am using Virtualenv 15 1 0 on my home computer and if I install Newspaper with Python 3 5 and then upload to Lambda (Python 3 6 runtime)  it throws errors related to PIL / Pillow    First it says it cant find _image  which appears to be resolved by deleting the PIL directory in site-packages  however that just results in it throwing the error that it cant find PIL  If  however  I build with Python 3 6 and then upload to Lambda  it works just fine (whether I delete PIL or not)  So  that appears to me that I cant install Newspaper with 3 5 and try to execute in a 3 6 runtime  So  now I am trying to deploy via Codestar  however Codestar seems to default to aws/codebuild/eb-nodejs-4 4 6-amazonlinux-64:2 1 3  even for Python projects  and all it seems to have available in the Yum repository is Python 3 5 and of course Lambda only has the 3 6 runtime  Even if I switch the image within Codebuild itself  there dont seem to be any images built with the Python3 6 runtime (according to the documentation)  Even the Docker images seem to lack Python 3 6  So  I am trying to install Python 3 6 in Codebuild during the INSTALL phase in my buildspec yml file  however I cant find the python3* executable after the install  The only other thing I can think of is to create the Codestar project  edit codebuild to use Ubuntu and then install everything (just like I did locally)  but there is no way to do that from within Codestar and I feel like that may bring me down a rabbit hole and thats hardly automated   Is there a way to make that configuration as code from within my project  EDIT Attempting to build and install Python 3 6 from source works  but then when trying to install Pip  I get errors saying SSL was not installed   And when looking back at the build logs  it seems other bits were not installed as well  So  my questions here are:  How do I get Python 3 6 into a Codebuild environment provisioned from a Codestar project  Should I continue trying to build it from source or make the switch to the Ubuntu environment  How can I automatically configure the image / environment within my code/project   EDIT 1 For anyone else  my complete buildspec yml for installing and using Python3 6 is below  Note  it keeps everything as quiet as possible in order to reduce the log messages  reduce Cloudwatch cost and speed up the process   I ended up shaving about 90 seconds off the whole process by doing that (installing Python and building my app)   Since CodeBuild charges based on time spent  this is crucial   
62132808,1,62486054.0,,2020-06-01 13:00:01,,2,480,"<p>I am developing a smart home skill for Alexa. So all requests from Alexa are sent to my AWS Lambda function which then forwards the requests to our servers which contacts the individual smart home devices. 
So according to the Alexa documentation (<a href=""https://developer.amazon.com/en-US/docs/alexa/device-apis/alexa-response.html#response"" rel=""nofollow noreferrer"">https://developer.amazon.com/en-US/docs/alexa/device-apis/alexa-response.html#response</a>) I can answer these requests Synchronously, meaning I wait all the way till the operation on the device has completed (while the http connection between the lambda and our servers remain open -> causing charges as lambda is running longer) and send the response via the lambda back to Alexa. </p>

<p>The other option is to answer Asynchronously by sending the answer as a new http request to the Alexa event gateway. </p>

<p>As some operations take some time (considering the way from our servers to the smart home devices, performing the operation, answering etc.) I'd prefer the async method, as it also saves the time on the lambda. I already implemented all the necessary components to answer async but I don't know what I should answer the lambda in case I'll answer async.</p>

<p>My Lambda currently looks somewhat like this:</p>

<pre><code>const https = require('https');

exports.handler = function (request, context) {

    function handleServerRequest(request, context) {

        const doPostRequest = () =&gt; {

            const data = request;

            return new Promise((resolve, reject) =&gt; {
                const options = {
                    host: 'xxx.ngrok.io',
                    path: '/dyn/alexa/request',
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json'
                    }
                };

                /* ... perform https request and resolve promise*/
            });
        };


        doPostRequest().then((response) =&gt; {

            log(""DEBUG"", ""Server Response: "", JSON.stringify(response));

            // in case the server decides to answer async (via event gateway) it 
            // immediately answers the https request with a flag ""async: true"". 

            if(response.async) {
                // -&gt; WHAT TO TELL THE LAMBDA HERE?
                //context.succeed();
                return;
            }
            context.succeed(response);
        });

    }

    handleServerRequest(request, context, """");


</code></pre>

<p>When I just do a context.succeed() without a proper response I'll get an error in the Alexa app, telling me ""the device is not reacting"", followed by indicating the correct status as quickly after that Alexa receives a valid StateReport directive via the event gateway.</p>

<p>How do I properly end the lambda function in case I'll answer asynchronously?</p>
",5035804.0,,5424128.0,,2020-06-01 15:41:01,2020-07-02 07:14:22,What to answer an AWS Lambda when answering Alexa Smart Home Skill asynchronous via event gateway?,<node.js><amazon-web-services><aws-lambda><alexa><alexa-skill>,3,0,,,,CC BY-SA 4.0,I am developing a smart home skill for Alexa  So all requests from Alexa are sent to my AWS Lambda function which then forwards the requests to our servers which contacts the individual smart home devices   So according to the Alexa documentation () I can answer these requests Synchronously  meaning I wait all the way till the operation on the device has completed (while the http connection between the lambda and our servers remain open -&gt; causing charges as lambda is running longer) and send the response via the lambda back to Alexa   The other option is to answer Asynchronously by sending the answer as a new http request to the Alexa event gateway   As some operations take some time (considering the way from our servers to the smart home devices  performing the operation  answering etc ) Id prefer the async method  as it also saves the time on the lambda  I already implemented all the necessary components to answer async but I dont know what I should answer the lambda in case Ill answer async  My Lambda currently looks somewhat like this:  When I just do a context succeed() without a proper response Ill get an error in the Alexa app  telling me the device is not reacting  followed by indicating the correct status as quickly after that Alexa receives a valid StateReport directive via the event gateway  How do I properly end the lambda function in case Ill answer asynchronously  
62137256,1,,,2020-06-01 17:05:09,,1,226,"<p>I am using stripe-node library for integrating the Stripe payment gateway.</p>

<p>Here is my lambda function to cancel the subscription. </p>

<pre><code>let stripe = require('stripe')('sk_test_....');


function handler(event, context) {
    const subId = event.params.subscriptionId;
    stripe.subscriptions.del(
        subId, 
        (err, confirmation) =&gt; {
            if (err) {
                return context.fail(err);
            }
        }
    );
}

exports.handler = handler;
</code></pre>

<p>I am getting this stack trace error in cloud watch.</p>

<pre><code>Error   
{
    ""errorType"": ""Error"",
    ""errorMessage"": ""An error occurred with our connection to Stripe."",
    ""type"": ""StripeConnectionError"",
    ""raw"": {
        ""message"": ""An error occurred with our connection to Stripe."",
        ""detail"": {
            ""errorType"": ""Error"",
            ""errorMessage"": ""write ETIMEDOUT"",
            ""code"": ""ETIMEDOUT"",
            ""errno"": ""ETIMEDOUT"",
            ""syscall"": ""write"",
            ""stack"": [
                ""Error: write ETIMEDOUT"",
                ""    at WriteWrap.onWriteComplete [as oncomplete] (internal/stream_base_commons.js:92:16)"",
                ""    at writevGeneric (internal/stream_base_commons.js:132:26)"",
                ""    at TLSSocket.Socket._writeGeneric (net.js:782:11)"",
                ""    at TLSSocket.Socket._writev (net.js:791:8)"",
                ""    at doWrite (_stream_writable.js:401:12)"",
                ""    at clearBuffer (_stream_writable.js:519:5)"",
                ""    at TLSSocket.Writable.uncork (_stream_writable.js:338:7)"",
                ""    at ClientRequest.end (_http_outgoing.js:782:17)"",
                ""    at ClientRequest.&lt;anonymous&gt; (/var/task/node_modules/stripe/lib/StripeResource.js:491:15)"",
                ""    at Object.onceWrapper (events.js:417:26)""
            ]
        }
    }
}
</code></pre>

<p>When I tried to cancel the subscription via CURL as suggested in docs, It worked fine. </p>

<pre><code>curl https://api.stripe.com/v1/subscriptions/sub_49ty4767H11z6a \
  -u sk_test_4.......: \
  -X DELETE
</code></pre>
",8498296.0,,,,,2020-06-01 17:05:09,cancelling subscription via stripe-node in nodejs,<node.js><aws-lambda><stripe-payments>,0,3,,,,CC BY-SA 4.0,I am using stripe-node library for integrating the Stripe payment gateway  Here is my lambda function to cancel the subscription    I am getting this stack trace error in cloud watch   When I tried to cancel the subscription via CURL as suggested in docs  It worked fine    
62172546,1,,,2020-06-03 12:11:58,,0,351,"<p>When creating a NodeJS API that accesses my serverless mysql cluster, the notes out there all point to NPM-loading the mysql package. I really try to avoid contaminating my lambdas with NPM and all the other dependencies that are going to spawn.
Is there no AWS native mysql client like the one for dynamoDb?</p>

<p>To clarify further, I'm looking for the mysql equivalent to this:
<a href=""https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GettingStarted.NodeJs.03.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GettingStarted.NodeJs.03.html</a></p>

<p>The advantage here is no package installation required and still being able to write classes without all the overhead code for managing types etc etc.</p>

<p>This project seems to acknowledge some work is needed in this regard:
<a href=""https://github.com/jeremydaly/data-api-client"" rel=""nofollow noreferrer"">https://github.com/jeremydaly/data-api-client</a></p>

<hr>

<p><strong>Update</strong>: So I bit the bullet and npm-installed a mysql package.Total cost is 11 modules and just 1.42Mb (below the 3Mb limit I like to set). I'm able to run all my CRUD ops as well as db and table management on Aurora. </p>

<p>If you're not doing sophisticated cluster management and are happy to leave that to Aurora, I think this is good enough. </p>

<p>I see a lot of code out there for lambdas crowded with AWS libraries, data-clients; this pushes the lambda to more than 20Mb. Totally not needed; for the rare case of cluster management involvement, you could just use a different lambda with those packages.</p>
",2473720.0,,2473720.0,,2020-06-05 06:10:48,2020-06-05 06:10:48,Is there an AWS-native mysql client library for nodejs?,<mysql><node.js><amazon-web-services><aws-lambda><amazon-dynamodb>,2,0,,,,CC BY-SA 4.0,When creating a NodeJS API that accesses my serverless mysql cluster  the notes out there all point to NPM-loading the mysql package  I really try to avoid contaminating my lambdas with NPM and all the other dependencies that are going to spawn  Is there no AWS native mysql client like the one for dynamoDb  To clarify further  Im looking for the mysql equivalent to this:  The advantage here is no package installation required and still being able to write classes without all the overhead code for managing types etc etc  This project seems to acknowledge some work is needed in this regard:   Update: So I bit the bullet and npm-installed a mysql package Total cost is 11 modules and just 1 42Mb (below the 3Mb limit I like to set)  Im able to run all my CRUD ops as well as db and table management on Aurora   If youre not doing sophisticated cluster management and are happy to leave that to Aurora  I think this is good enough   I see a lot of code out there for lambdas crowded with AWS libraries  data-clients; this pushes the lambda to more than 20Mb  Totally not needed; for the rare case of cluster management involvement  you could just use a different lambda with those packages  
62167741,1,,,2020-06-03 07:47:48,,1,942,"<p>I have a Django webapp (a forum) which have few screens like login, profile, posts, replies, etc..</p>

<p>A regular deployment on dedicated instances (with scalability, performance in mind) seems to be expensive. I have come across serverless deployment of Django apps on AWS Lambda. Here is one such <a href=""https://blog.lawrencemcdaniel.com/serve-a-django-app-from-an-aws-lambda-function/"" rel=""nofollow noreferrer"">example</a> on AWS. But I couldn't find anything similar on GCP.</p>

<p>Is a similar thing possible using Google cloud functions (GCF)?</p>

<p>In other words, can GCF be used to deploy any of the following:</p>

<ul>
<li>a web app which can serve dynamic pages</li>
<li>a microservice with multiple rest endpoints</li>
</ul>
",6179988.0,,6179988.0,,2020-06-03 12:42:16,2020-06-05 10:43:27,Deploy Django App serverless on GCP cloud functions,<django><aws-lambda><google-cloud-functions>,1,0,,,,CC BY-SA 4.0,I have a Django webapp (a forum) which have few screens like login  profile  posts  replies  etc   A regular deployment on dedicated instances (with scalability  performance in mind) seems to be expensive  I have come across serverless deployment of Django apps on AWS Lambda  Here is one such  on AWS  But I couldnt find anything similar on GCP  Is a similar thing possible using Google cloud functions (GCF)  In other words  can GCF be used to deploy any of the following:  a web app which can serve dynamic pages a microservice with multiple rest endpoints  
60259148,1,,,2020-02-17 09:16:08,,0,1376,"<p>Want to keep this question generic and expected answer in terms of best practice/approach/guidelines,</p>

<p>We need to know the best way to performance test and load test AWS cloud based applications.</p>

<p><strong>What we have tried:</strong></p>

<p>We used Gatling and Jmeter to execute our performance tests. These frameworks are pretty useful to test our functionality and to benchmark our applications latency and request rate.</p>

<p><strong>Problem:</strong></p>

<p>Performance benchmarks and limits of AWS managed services like Lambda and DDB are already specified by AWS e.g. Lambda concurrency behavior and DDB autoscaling under load etc. AWS also provides high availability and guaranteed performance of managed services.  </p>

<ol>
<li>Is it really worth executing expensive performance test and load test jobs for AWS managed services?</li>
<li>How to ensure that we are testing our application and not actually testing AWS limits which are already known.</li>
<li>What is the best practice and approach to performance test cloud based applications.</li>
</ol>

<p>Any suggestions will help tremendously.
Thanks,</p>
",4898337.0,,,,,2020-02-17 10:01:21,What is best way to performance and load test AWS cloud applications,<amazon-web-services><performance><testing><aws-lambda><performance-testing>,1,0,,,,CC BY-SA 4.0,Want to keep this question generic and expected answer in terms of best practice/approach/guidelines  We need to know the best way to performance test and load test AWS cloud based applications  What we have tried: We used Gatling and Jmeter to execute our performance tests  These frameworks are pretty useful to test our functionality and to benchmark our applications latency and request rate  Problem: Performance benchmarks and limits of AWS managed services like Lambda and DDB are already specified by AWS e g  Lambda concurrency behavior and DDB autoscaling under load etc  AWS also provides high availability and guaranteed performance of managed services     Is it really worth executing expensive performance test and load test jobs for AWS managed services  How to ensure that we are testing our application and not actually testing AWS limits which are already known  What is the best practice and approach to performance test cloud based applications   Any suggestions will help tremendously  Thanks  
59010349,1,,,2019-11-23 17:32:42,,1,694,"<p>I am planning to copy  the AWS CloudWatch Logs to ELK and want to use Kibana Dashboard to visualise the logs.</p>

<p>One option is to stream the logs from CloudWatch to ELK.</p>

<p><a href=""https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CWL_ES_Stream.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CWL_ES_Stream.html</a></p>

<p>But I feel this will involve execution of Lambda functions extensively and it might not be a cost effective option.</p>

<p>Is there any other cost effective way to copy logs from CloudWatch to maybe S3 and then to ELK?</p>

<p>I am Ok, if the logs are not realtime, maybe a delay of 15 mins or maybe one hour is OK.</p>

<p>But I am looking for a cost effective solution.</p>

<p>Btw, what is the best way to purge the CloudWatch logs periodically? ( maybe after one week)</p>
",1520421.0,,,,,2019-11-24 09:24:35,AWS CloudWatch Logs to ELK,<amazon-web-services><amazon-s3><aws-lambda><amazon-cloudwatch><elk>,1,0,,,,CC BY-SA 4.0,I am planning to copy  the AWS CloudWatch Logs to ELK and want to use Kibana Dashboard to visualise the logs  One option is to stream the logs from CloudWatch to ELK   But I feel this will involve execution of Lambda functions extensively and it might not be a cost effective option  Is there any other cost effective way to copy logs from CloudWatch to maybe S3 and then to ELK  I am Ok  if the logs are not realtime  maybe a delay of 15 mins or maybe one hour is OK  But I am looking for a cost effective solution  Btw  what is the best way to purge the CloudWatch logs periodically  ( maybe after one week) 
59022523,1,,,2019-11-24 21:21:04,,3,869,"<p>I would like to schedule triggering my Lambda function.<br>
AWS EventBridge allows me to do this (e.g. by creating cron based rule) but I don't understand <a href=""https://aws.amazon.com/eventbridge/pricing/"" rel=""nofollow noreferrer"">its pricing model</a>.<br>
It states ""All state change events published by AWS services are free"" but <strong>I'm not sure if an event fired by EventBridge (generated by EventBridge rule) relates to this free one</strong>. Probably not.<br>
If it's not free, then <strong>how 30 events per month will be billed if pricing is based on million of events</strong>.</p>

<p>Googling didn't help. Do you have ideas how 30 events will be billed?</p>

<hr>

<p><strong>Update</strong>:<br>
While experimenting I found that we should not pay for 30 events per months but it's not clear why. Probably less than million of events are free.</p>
",1262265.0,,1262265.0,,2019-12-02 20:29:14,2020-07-30 02:52:40,AWS EventBridge pricing for cron events,<amazon-web-services><aws-lambda>,1,0,,,,CC BY-SA 4.0,I would like to schedule triggering my Lambda function  AWS EventBridge allows me to do this (e g  by creating cron based rule) but I dont understand   It states All state change events published by AWS services are free but Im not sure if an event fired by EventBridge (generated by EventBridge rule) relates to this free one  Probably not  If its not free  then how 30 events per month will be billed if pricing is based on million of events  Googling didnt help  Do you have ideas how 30 events will be billed   Update: While experimenting I found that we should not pay for 30 events per months but its not clear why  Probably less than million of events are free  
59026283,1,,,2019-11-25 06:26:16,,0,75,"<p><strong>Generalities</strong></p>

<p>I am working in a Lambda authenticator. I have several modules and require a unique authenticator. This means that a client sends a user and password and a custom authenticator returns a JWT. Then another endpoint is called, the JWT is validated and the request forwarded to the final endpoint. </p>

<p><strong>Whats working</strong>
The JWT generation is ok and I have a custom authenticator working on lambda which validates the JWT. </p>

<p><strong>The problem</strong>
I am using serverless framework, and I cant find how to implement a http integration. If I use the console I just select my custom authenticator and pick http from the integration <a href=""https://i.stack.imgur.com/J5MGz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/J5MGz.png"" alt=""selecting http integration from console""></a> and I am done. The request is received, validated using the custom authenticator and forwarded to an external endpoint. </p>

<p>How can I do this in serverless? I have this config.</p>

<pre><code>functions:
hello:
   handler: handler.hello
   events:
     - http:
         path: hello
         method: get
         authorizer: authorize
</code></pre>

<p>It obviously do not do what I expect. A lambda handler.hello is called after the custom authenticator is done. I dont want to call the external endpoint inside the lambda since it will cost every time. I just want to use an http integration as the one in the console to call another endpoint. Is this possible?</p>
",1858664.0,,,,,2019-11-27 11:00:08,How to configure a http integration instead lf a lambda integration using serverless?,<amazon-web-services><http><aws-lambda><serverless>,1,2,,,,CC BY-SA 4.0,Generalities I am working in a Lambda authenticator  I have several modules and require a unique authenticator  This means that a client sends a user and password and a custom authenticator returns a JWT  Then another endpoint is called  the JWT is validated and the request forwarded to the final endpoint   Whats working The JWT generation is ok and I have a custom authenticator working on lambda which validates the JWT   The problem I am using serverless framework  and I cant find how to implement a http integration  If I use the console I just select my custom authenticator and pick http from the integration  and I am done  The request is received  validated using the custom authenticator and forwarded to an external endpoint   How can I do this in serverless  I have this config   It obviously do not do what I expect  A lambda handler hello is called after the custom authenticator is done  I dont want to call the external endpoint inside the lambda since it will cost every time  I just want to use an http integration as the one in the console to call another endpoint  Is this possible  
59031075,1,,,2019-11-25 11:44:34,,2,3013,"<p>I have set up a FIFO queue and I'd like to partition messages by a message group ID.</p>

<p>As an event trigger, I'm using a lambda function.</p>

<p>Now my question: Is it possible to allow only one concurrent lambda function invocation for every message group?</p>

<p>So, if I have <code>Group A</code> and <code>Group B</code>, and each message group has 20 messages, every message group passes one message at a time (I have set up the batch size of 1) to a lambda function and only passes the next one as soon as the previous message processing has finished.</p>

<p>Is that possible using FIFO queues and lambda or do I need to look into other services to allow that?</p>

<p><strong>Note</strong>: I've looking into Kinesis with separate shards already, but because there'll be many message groups, the total cost of the shards would be way too much.</p>

<p>Thank you!</p>
",7190575.0,,,,,2021-02-24 02:19:55,AWS FIFO Queues with lambda  one concurrent lambda per message group,<amazon-web-services><aws-lambda><amazon-sqs>,2,6,,,,CC BY-SA 4.0,I have set up a FIFO queue and Id like to partition messages by a message group ID  As an event trigger  Im using a lambda function  Now my question: Is it possible to allow only one concurrent lambda function invocation for every message group  So  if I have  and   and each message group has 20 messages  every message group passes one message at a time (I have set up the batch size of 1) to a lambda function and only passes the next one as soon as the previous message processing has finished  Is that possible using FIFO queues and lambda or do I need to look into other services to allow that  Note: Ive looking into Kinesis with separate shards already  but because therell be many message groups  the total cost of the shards would be way too much  Thank you  
60327663,1,,,2020-02-20 20:00:33,,2,400,"<p>Apologies for the long read, but I have only included the relevant parts necessary. I have inherited some code and CloudFormation scripts for a project and the previous developer left a cryptic note:</p>

<blockquote>
  <p>You must update the trailing date on the following items in app-deploy.cfn.yaml to get changes to take effect.</p>
</blockquote>

<p>He's long gone and not somewhere I can ask him about his instructions. Here are the items that must be updated:</p>

<pre><code>ApiDeployment20200214:
    Type: AWS::ApiGateway::Deployment
    Properties:
      RestApiId: !Ref NaasRestApi

RestApiStage:
    Type: AWS::ApiGateway::Stage
    Properties:
      DeploymentId: !Ref ApiDeployment20200214
      RestApiId: !Ref NaasRestApi
      StageName: !Sub ${Environment}

ApiBasePathMapping:
    Type: AWS::ApiGateway::BasePathMapping
    Condition: IsMainPipeline
    DependsOn: ApiDeployment20200214
    Properties:
      BasePath: ''
      DomainName: !Ref ApiGatewayDomainName
      RestApiId: !Ref NaasRestApi
      Stage: !Sub ${Environment}
</code></pre>

<p>Honestly, this seems way out of the ordinary. It looks like he is applying cache-busting techniques to the API Gateway. This, to my knowledge, should not ever have to happen.</p>

<p>Boy was I wrong. </p>

<p>I recently needed to make one simple change to an SNS Subscription, adding only <code>- redirect</code> as a source for a particular Lambda subscription. Here is that resource with the new source added:</p>

<pre><code>BHIEventLambdaSubscription:
    Type: AWS::SNS::Subscription
    Properties:
      Endpoint: !GetAtt BHIEventLambdaFunction.Arn
      Protocol: lambda
      TopicArn: !Ref EventTopicArn
      FilterPolicy:
        domain:
          - payments
        source:
          - recycling
          - processor
          - paas
          - redirect
        type:
          - payment.approved
          - payment.declined
        brand_id:
          - '40'
</code></pre>

<p>I deployed the scripts and witnessed the updates take place. I checked the console for the proper JSON, revealing this:</p>

<pre><code>{
  ""source"": [
    ""recycling"",
    ""processor"",
    ""paas"",
    ""redirect""
  ],
  ""type"": [
    ""payment.approved"",
    ""payment.declined""
  ],
  ""domain"": [
    ""payments""
  ],
  ""brand_id"": [
    ""40""
</code></pre>

<p>All good! I should be good to go, but when I sent the proper message to test the source I got an error message:</p>

<blockquote>
  <p>@message
  2020-02-14 19:11:02 &lt;071acec7-096b-4c45-996a-5ed996df7c23> ERROR AbstractEventHandler:66 - java.lang.IllegalArgumentException: Unable to process message, Invalid format.</p>
</blockquote>

<p>And in my UI</p>

<pre><code>{""message"": ""Invalid request body""}
</code></pre>

<p>Checking the CloudWatch Logs I can see the request being properly made with that source:</p>

<blockquote>
  <p>@message
  2020-02-14 19:13:41  INFO BHIEventHandler:23 - BHI Handler Received: {[{sns: {messageAttributes: {domain={type: String,value: payments}, <strong>source={type: String,value: redirect}</strong>, type={type: String,value: payment.approved}, brand_id={type: String,value: 40}}, ....</p>
</blockquote>

<hr>

<p>Suddenly I recalled the cryptic message from the previous developer, but I could not see how that would apply to make a change to SNS, especially since that update appeared to be deployed to AWS properly. Another developer who I was discussing this with said, ""You could just prove old Bruce wrong by trying it, you've got nothing to lose at this point and we can eliminate that as a distraction.""</p>

<p>Off we go, we change the dates assigned/appended/tagged to the API resources and redeploy. I see the updates in the console, CodePipeline reports the deployment has been completed successfully. We send the test...</p>

<p>...and the dang thing worked!</p>

<p><strong>My Question</strong></p>

<p>Why does making a change to a completely (apparently) unrelated portion of CF make this work? Have we done something wrong? If so, how do I correct this?</p>

<p><strong>By request</strong></p>

<p>Here is the complete template:</p>

<pre><code>Transform: AWS::Serverless-2016-10-31
Parameters:
  DeveloperPrefix:
    Type: String
    Default: ''
  AppName:
    Type: String
    Description: The application name
  Environment:
    Type: String
    Description: The environment name
  AppVersion:
    Type: String
    Description: The application version
  SharedBucketName:
    Type: String
    Description: Shared bucket name containing the deployable artifact
  WorkspacePrefix:
    Type: String
    Description: App workspace in shared bucket
  DeployPhase:
    Type: String
  CustomResourceLambdaS3Version:
    Type: String
    Description: The Custom Resource Lambda S3 version of the artifact
  S3Version:
    Type: String
    Description: The S3 Version
  HGEventLambdaS3Version:
    Type: String
    Description: The HG Event Lambda S3 version of the artifact
  HGEventRetryLambdaS3Version:
    Type: String
    Description: HG Event Retry Lambda S3 Version artifact
  SBEventLambdaS3Version:
    Type: String
    Description: The SB Event Lambda S3 version of the artifact
  SBEventRetryLambdaS3Version:
    Type: String
    Description: SB Event Retry Lambda S3 Version artifact
  BHEventLambdaS3Version:
    Type: String
    Description: The BH Event Lambda S3 version of the artifact
  BHEventRetryLambdaS3Version:
    Type: String
    Description: BH Event Retry Lambda S3 Version artifact
  BHIEventLambdaS3Version:
    Type: String
    Description: The BHI Event Lambda S3 version of the artifact
  BHIEventRetryLambdaS3Version:
    Type: String
    Description: BHI Event Retry Lambda S3 Version artifact
  AsoEventLambdaS3Version:
    Type: String
    Description: The ASO Event Lambda S3 version of the artifact
  AsoEventRetryLambdaS3Version:
    Type: String
    Description: ASO Event Retry Lambda S3 Version artifact
  CTEventLambdaS3Version:
    Type: String
    Description: The CT Event Lambda S3 version of the artifact
  CTEventRetryLambdaS3Version:
    Type: String
    Description: The CT Event Retry Lambda S3 Version artifact
  HZ:
    Type: String
    Description: The hosted zone in Route53
    Default: ""notifications.svcs.example.com""
  CloudFrontHZ:
    Type: String
    Default: Z2FDTNDATAQYW2
  VPCStackName:
    Type: String
    Description: ""The Stack containing the VPC you wish to attach the VPN to""

Mappings:
  EnvironmentMap:
    dev:
      EnvironmentName: 'dev.'
      CertificateId: ''
      HostedZoneId: ''
      VpcStackName: VPC-QA
    qa:
      EnvironmentName: 'qa.'
      CertificateId: 'xxxxxxxxxxxx'
      HostedZoneId: 'xxxxxxxxxxxx'
      VpcStackName: VPC-QA
    stage:
      EnvironmentName: 'stage.'
      CertificateId: 'xxxxxxxxxxxx'
      HostedZoneId: 'xxxxxxxxxxxx'
      VpcStackName: VPC-Stage
    prod:
      EnvironmentName: ''
      CertificateId: 'xxxxxxxxxxxx'
      HostedZoneId: 'xxxxxxxxxxxx'
      VpcStackName: VPC-Prod

Conditions:
  IsMainPipeline: !And
    - !Equals [ !Ref DeveloperPrefix, '' ]
    - !Equals [ !Ref DeployPhase, rel ]

Resources:
  #####################
  # HostedZone Config #
  #####################
  HostedZoneResource:
    Type: ""AWS::CloudFormation::Stack""
    Condition: IsMainPipeline
    Properties:
      Parameters:
        DeveloperPrefix: !Ref DeveloperPrefix
        Environment: !Ref Environment
        DeployPhase: !Ref DeployPhase
      TemplateURL: !Sub ""https://s3.amazonaws.com/${SharedBucketName}/${DeveloperPrefix}${AppName}/${Environment}/cf/nested/${S3Version}/hosted-zone.cfn.yaml""

  ####################
  # Lambda Resources #
  ####################
  CustomResource:
    Type: ""AWS::CloudFormation::Stack""
    Properties:
      Parameters:
        AppName: !Ref AppName
        Environment: !Ref Environment
        DeveloperPrefix: !Ref DeveloperPrefix
        DeployPhase: !Ref DeployPhase
        AppVersion: !Ref AppVersion
        SharedBucketName: !Ref SharedBucketName
        WorkspacePrefix: !Ref WorkspacePrefix
        CustomResourceLambdaS3Version: !Ref CustomResourceLambdaS3Version
        #VPCStackName: !Ref VPCStackName
        VPCStackName: !FindInMap
          - EnvironmentMap
          - !Ref Environment
          - VpcStackName
      TemplateURL: !Sub ""https://s3.amazonaws.com/${SharedBucketName}/${DeveloperPrefix}${AppName}/${Environment}/cf/nested/${S3Version}/custom-resource.cfn.yaml""

  HGResources:
    Type: ""AWS::CloudFormation::Stack""
    Properties:
      Parameters:
        AppName: !Ref AppName
        Environment: !Ref Environment
        DeveloperPrefix: !Ref DeveloperPrefix
        DeployPhase: !Ref DeployPhase
        AppVersion: !Ref AppVersion
        SharedBucketName: !Ref SharedBucketName
        WorkspacePrefix: !Ref WorkspacePrefix
        HGEventLambdaS3Version: !Ref HGEventLambdaS3Version
        HGEventRetryLambdaS3Version: !Ref HGEventRetryLambdaS3Version
        EventTopicArn: !Ref EventTopic
        #VPCStackName: !Ref VPCStackName
        VPCStackName: !FindInMap
          - EnvironmentMap
          - !Ref Environment
          - VpcStackName
      TemplateURL: !Sub ""https://s3.amazonaws.com/${SharedBucketName}/${DeveloperPrefix}${AppName}/${Environment}/cf/nested/${S3Version}/HG-subscriber.cfn.yaml""

  SBResources:
    Type: ""AWS::CloudFormation::Stack""
    Properties:
      Parameters:
        AppName: !Ref AppName
        Environment: !Ref Environment
        DeveloperPrefix: !Ref DeveloperPrefix
        DeployPhase: !Ref DeployPhase
        AppVersion: !Ref AppVersion
        SharedBucketName: !Ref SharedBucketName
        WorkspacePrefix: !Ref WorkspacePrefix
        SBEventLambdaS3Version: !Ref SBEventLambdaS3Version
        SBEventRetryLambdaS3Version: !Ref SBEventRetryLambdaS3Version
        EventTopicArn: !Ref EventTopic
        #VPCStackName: !Ref VPCStackName
        VPCStackName: !FindInMap
          - EnvironmentMap
          - !Ref Environment
          - VpcStackName
      TemplateURL: !Sub ""https://s3.amazonaws.com/${SharedBucketName}/${DeveloperPrefix}${AppName}/${Environment}/cf/nested/${S3Version}/SB-subscriber.cfn.yaml""

  BHResources:
    Type: ""AWS::CloudFormation::Stack""
    Properties:
      Parameters:
        AppName: !Ref AppName
        Environment: !Ref Environment
        DeveloperPrefix: !Ref DeveloperPrefix
        DeployPhase: !Ref DeployPhase
        AppVersion: !Ref AppVersion
        SharedBucketName: !Ref SharedBucketName
        WorkspacePrefix: !Ref WorkspacePrefix
        BHEventLambdaS3Version: !Ref BHEventLambdaS3Version
        BHEventRetryLambdaS3Version: !Ref BHEventRetryLambdaS3Version
        EventTopicArn: !Ref EventTopic
        #VPCStackName: !Ref VPCStackName
        VPCStackName: !FindInMap
        - EnvironmentMap
        - !Ref Environment
        - VpcStackName
      TemplateURL: !Sub ""https://s3.amazonaws.com/${SharedBucketName}/${DeveloperPrefix}${AppName}/${Environment}/cf/nested/${S3Version}/BH-subscriber.cfn.yaml""

  BHIResources:
    Type: ""AWS::CloudFormation::Stack""
    Properties:
      Parameters:
        AppName: !Ref AppName
        Environment: !Ref Environment
        DeveloperPrefix: !Ref DeveloperPrefix
        DeployPhase: !Ref DeployPhase
        AppVersion: !Ref AppVersion
        SharedBucketName: !Ref SharedBucketName
        WorkspacePrefix: !Ref WorkspacePrefix
        BHIEventLambdaS3Version: !Ref BHIEventLambdaS3Version
        BHIEventRetryLambdaS3Version: !Ref BHIEventRetryLambdaS3Version
        EventTopicArn: !Ref EventTopic
        #VPCStackName: !Ref VPCStackName
        VPCStackName: !FindInMap
          - EnvironmentMap
          - !Ref Environment
          - VpcStackName
      TemplateURL: !Sub ""https://s3.amazonaws.com/${SharedBucketName}/${DeveloperPrefix}${AppName}/${Environment}/cf/nested/${S3Version}/BHI-subscriber.cfn.yaml""

  AsoResources:
    Type: ""AWS::CloudFormation::Stack""
    Properties:
      Parameters:
        AppName: !Ref AppName
        Environment: !Ref Environment
        DeveloperPrefix: !Ref DeveloperPrefix
        DeployPhase: !Ref DeployPhase
        AppVersion: !Ref AppVersion
        SharedBucketName: !Ref SharedBucketName
        WorkspacePrefix: !Ref WorkspacePrefix
        AsoEventLambdaS3Version: !Ref AsoEventLambdaS3Version
        AsoEventRetryLambdaS3Version: !Ref AsoEventRetryLambdaS3Version
        EventTopicArn: !Ref EventTopic
        #VPCStackName: !Ref VPCStackName
        VPCStackName: !FindInMap
        - EnvironmentMap
        - !Ref Environment
        - VpcStackName
      TemplateURL: !Sub ""https://s3.amazonaws.com/${SharedBucketName}/${DeveloperPrefix}${AppName}/${Environment}/cf/nested/${S3Version}/aso-subscriber.cfn.yaml""

  CTResources:
    Type: ""AWS::CloudFormation::Stack""
    Properties:
      Parameters:
        AppName: !Ref AppName
        Environment: !Ref Environment
        DeveloperPrefix: !Ref DeveloperPrefix
        DeployPhase: !Ref DeployPhase
        AppVersion: !Ref AppVersion
        SharedBucketName: !Ref SharedBucketName
        WorkspacePrefix: !Ref WorkspacePrefix
        CTEventLambdaS3Version: !Ref CTEventLambdaS3Version
        CTEventRetryLambdaS3Version: !Ref CTEventRetryLambdaS3Version
        EventTopicArn: !Ref EventTopic
        #VPCStackName: !Ref VPCStackName
        VPCStackName: !FindInMap
          - EnvironmentMap
          - !Ref Environment
          - VpcStackName
      TemplateURL: !Sub ""https://s3.amazonaws.com/${SharedBucketName}/${DeveloperPrefix}${AppName}/${Environment}/cf/nested/${S3Version}/CT-subscriber.cfn.yaml""

  #########################
  # API Gateway Resources #
  #########################
  ApiGatewayDomainName:
    Type: AWS::ApiGateway::DomainName
    Condition: IsMainPipeline
    Properties:
      CertificateArn: !Join ["""", ['arn:aws:acm:', !Ref 'AWS::Region', ':', !Ref 'AWS::AccountId', ':certificate/', !FindInMap [ EnvironmentMap, !Ref Environment, CertificateId ] ] ]
      DomainName: !Join [ '', [ !FindInMap [ EnvironmentMap, !Ref Environment, EnvironmentName ], !Ref HZ ] ]

  ApiBasePathMapping:
    Type: AWS::ApiGateway::BasePathMapping
    Condition: IsMainPipeline
    DependsOn: ApiDeployment20200214
    Properties:
      BasePath: ''
      DomainName: !Ref ApiGatewayDomainName
      RestApiId: !Ref NaasRestApi
      Stage: !Sub ${Environment}

  ApiGatewayARecord:
    Type: AWS::Route53::RecordSetGroup
    Condition: IsMainPipeline
    Properties:
      HostedZoneId: !FindInMap [ EnvironmentMap, !Ref Environment, HostedZoneId ]
      RecordSets:
        - Type: A
          Name: !Join  [ """", [ !FindInMap [ EnvironmentMap, !Ref Environment, EnvironmentName ], !Ref HZ, ""."" ] ]
          AliasTarget:
            HostedZoneId: !Ref CloudFrontHZ
            DNSName: !GetAtt ApiGatewayDomainName.DistributionDomainName

  NaasRestApi:
    Type: AWS::ApiGateway::RestApi
    DependsOn: EventTopic
    Properties:
      Body:
        Fn::Transform:
          Name: 'AWS::Include'
          Parameters:
            Location: !Join ['/', [ 's3:/', !Ref SharedBucketName, !Ref WorkspacePrefix, 'naas.yaml' ]]

  RestApiStage:
    Type: AWS::ApiGateway::Stage
    Properties:
      DeploymentId: !Ref ApiDeployment20200214
      RestApiId: !Ref NaasRestApi
      StageName: !Sub ${Environment}

  ApiDeployment20200214:
    Type: AWS::ApiGateway::Deployment
    Properties:
      RestApiId: !Ref NaasRestApi

  TESTApiGatewayUsagePlan:
    Type: AWS::ApiGateway::UsagePlan
    DependsOn: RestApiStage
    Properties:
      UsagePlanName: !Sub TEST-${DeveloperPrefix}${AppName}-${Environment}-${DeployPhase}-UsagePlan
      Description: !Sub 'TEST-${DeveloperPrefix}${AppName}-${Environment}-${DeployPhase} Usage plan for Testing ONLY.'
      ApiStages:
        - ApiId: !Ref NaasRestApi
          Stage: !Sub ${Environment}
      Quota:
        Limit: 5000
        Offset: 0
        Period: DAY
      Throttle:
        BurstLimit: 150
        RateLimit: 100

  DefaultUsagePlan:
    Type: AWS::ApiGateway::UsagePlan
    Condition: IsMainPipeline
    DependsOn: RestApiStage
    Properties:
      UsagePlanName: !Sub ${DeveloperPrefix}${AppName}-${Environment}-Default-UsagePlan
      Description: !Sub '${DeveloperPrefix}${AppName}-${Environment} Default Usage plan for Brands'
      ApiStages:
        - ApiId: !Ref NaasRestApi
          Stage: !Sub ${Environment}
      Quota:
        Limit: 50000
        Offset: 0
        Period: DAY
      Throttle:
        BurstLimit: 1500
        RateLimit: 1000

  ApiGatewayNaasSNSRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub ${DeveloperPrefix}${AppName}-${Environment}-${DeployPhase}-ApiGatewayNaasSNSRole
      AssumeRolePolicyDocument:
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - apigateway.amazonaws.com
            Action: ['sts:AssumeRole']
      Path: /
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/AmazonSNSFullAccess

  TESTApiKey:
    Type: AWS::ApiGateway::ApiKey
    DependsOn: RestApiStage
    Properties:
      Name: !Sub ""TEST-${DeveloperPrefix}${Environment}-${DeployPhase}""
      Description: ""TEST API Key""
      Enabled: true
      StageKeys:
        - RestApiId: !Ref NaasRestApi
          StageName: !Sub ${Environment}

  CTApiKey:
    Type: AWS::ApiGateway::ApiKey
    Condition: IsMainPipeline
    DependsOn: RestApiStage
    Properties:
      Name: !Sub ""${DeveloperPrefix}CT-${Environment}""
      Description: !Sub ""CT API Key""
      Enabled: true
      StageKeys:
        - RestApiId: !Ref NaasRestApi
          StageName: !Sub ${Environment}

  TESTUsagePlanKey:
    Type: AWS::ApiGateway::UsagePlanKey
    Properties:
      KeyId: !Ref TESTApiKey
      KeyType: API_KEY
      UsagePlanId: !Ref TESTApiGatewayUsagePlan

  CTUsagePlanKey:
    Type: AWS::ApiGateway::UsagePlanKey
    Condition: IsMainPipeline
    Properties:
      KeyId: !Ref CTApiKey
      KeyType: API_KEY
      UsagePlanId: !Ref DefaultUsagePlan

  ################################
  # Cognito User Pools Resources #
  ################################
  NaasUserPool:
    Type: AWS::Cognito::UserPool
    Properties:
      UserPoolName: !Sub ${DeveloperPrefix}${AppName}-${Environment}-${DeployPhase}

  TESTUserPoolClient:
    Type: AWS::Cognito::UserPoolClient
    Properties:
      ClientName: 'TEST Brand'
      GenerateSecret: true
      UserPoolId: !Ref NaasUserPool

  CTUserPoolClient:
    Type: AWS::Cognito::UserPoolClient
    Condition: IsMainPipeline
    Properties:
      ClientName: 'CT'
      GenerateSecret: true
      UserPoolId: !Ref NaasUserPool

  UserPoolResourceServer:
    Type: Custom::CognitoUserPoolResourceServer
    DependsOn:
      - NaasRestApi
      - CustomResource
    Properties:
      ServiceToken: !Sub ${CustomResource.Outputs.CustomResourceLambdaFunctionArn}
      RequestTypes:
        Create:
          Service: 'com.example.naas.lambda.cloudformation.service.CognitoService'
          Method: createResourceServer
          Model: 'com.amazonaws.services.cognitoidp.model.CreateResourceServerRequest'
          Parameters:
            userPoolId: !Ref NaasUserPool
            name: NaaS
            identifier: naas
            scopes:
              - scopeName: ""fraud.publish""
                scopeDescription: ""Fraud Decision Change Event""
        Delete:
          Service: 'com.example.naas.lambda.cloudformation.service.CognitoService'
          Method: deleteResourceServer
          Model: 'com.amazonaws.services.cognitoidp.model.DeleteResourceServerRequest'
          Parameters:
            userPoolId: !Ref NaasUserPool
            identifier: naas
        Update:
          Service: 'com.example.naas.lambda.cloudformation.service.CognitoService'
          Method: updateResourceServer
          Model: 'com.amazonaws.services.cognitoidp.model.UpdateResourceServerRequest'

  TESTBrandUserPoolClientSettings:
    Type: Custom::CognitoUserPoolClientSettings
    DependsOn: UserPoolResourceServer
    Properties:
      ServiceToken: !Sub ${CustomResource.Outputs.CustomResourceLambdaFunctionArn}
      RequestTypes:
        Create:
          Service: 'com.example.naas.lambda.cloudformation.service.CognitoService'
          Method: updateUserPoolClientSettings
          Model: 'com.amazonaws.services.cognitoidp.model.UpdateUserPoolClientRequest'
          Parameters:
            userPoolId: !Ref NaasUserPool
            clientId: !Ref TESTUserPoolClient
            allowedOAuthFlowsUserPoolClient: true
            explicitAuthFlows:
              - 'ADMIN_NO_SRP_AUTH'
            supportedIdentityProviders:
              - COGNITO
            allowedOAuthFlows:
              - client_credentials
            allowedOAuthScopes:
              - 'naas/fraud.publish'
        Delete:
          Service: 'com.example.naas.lambda.cloudformation.service.CognitoService'
          Method: deleteUserPoolClientSettings
          Model: 'com.amazonaws.services.cognitoidp.model.DeleteUserPoolClientRequest'
          Parameters:
            userPoolId: !Ref NaasUserPool
            clientId: !Ref TESTUserPoolClient
        Update:
          Service: 'com.example.naas.lambda.cloudformation.service.CognitoService'
          Method: updateUserPoolClientSettings
          Model: 'com.amazonaws.services.cognitoidp.model.UpdateUserPoolClientRequest'

  CTUserPoolClientSettings:
    Type: Custom::CognitoUserPoolClientSettings
    Condition: IsMainPipeline
    DependsOn: UserPoolResourceServer
    Properties:
      ServiceToken: !Sub ${CustomResource.Outputs.CustomResourceLambdaFunctionArn}
      RequestTypes:
        Create:
          Service: 'com.example.naas.lambda.cloudformation.service.CognitoService'
          Method: updateUserPoolClientSettings
          Model: 'com.amazonaws.services.cognitoidp.model.UpdateUserPoolClientRequest'
          Parameters:
            userPoolId: !Ref NaasUserPool
            clientId: !Ref CTUserPoolClient
            allowedOAuthFlowsUserPoolClient: true
            explicitAuthFlows:
              - 'ADMIN_NO_SRP_AUTH'
            supportedIdentityProviders:
              - COGNITO
            allowedOAuthFlows:
              - client_credentials
            allowedOAuthScopes:
              - 'naas/fraud.publish'
        Delete:
          Service: 'com.example.naas.lambda.cloudformation.service.CognitoService'
          Method: deleteUserPoolClientSettings
          Model: 'com.amazonaws.services.cognitoidp.model.DeleteUserPoolClientRequest'
          Parameters:
            userPoolId: !Ref NaasUserPool
            clientId: !Ref CTUserPoolClient
        Update:
          Service: 'com.example.naas.lambda.cloudformation.service.CognitoService'
          Method: updateUserPoolClientSettings
          Model: 'com.amazonaws.services.cognitoidp.model.UpdateUserPoolClientRequest'

  CognitoUserPoolDomain:
    Type: Custom::CreateCognitoUserPoolDomain
    DependsOn: CustomResource
    Properties:
      ServiceToken: !Sub ${CustomResource.Outputs.CustomResourceLambdaFunctionArn}
      RequestTypes:
        Create:
          Service: 'com.example.naas.lambda.cloudformation.service.CognitoService'
          Method: createUserPoolDomain
          Model: 'com.amazonaws.services.cognitoidp.model.CreateUserPoolDomainRequest'
          Parameters:
            domain: !Sub ${DeveloperPrefix}${AppName}-${Environment}-${DeployPhase}
            userPoolId: !Ref NaasUserPool
        Delete:
          Service: 'com.example.naas.lambda.cloudformation.service.CognitoService'
          Method: deleteUserPoolDomain
          Model: 'com.amazonaws.services.cognitoidp.model.DeleteUserPoolDomainRequest'
          Parameters:
            domain: !Sub ${DeveloperPrefix}${AppName}-${Environment}-${DeployPhase}
            userPoolId: !Ref NaasUserPool

  #################
  # SNS Resources #
  #################
  EventTopic:
    Type: AWS::SNS::Topic
    Properties:
      TopicName: !Sub '${DeveloperPrefix}event-${Environment}-${DeployPhase}'

  ##############################
  # Third-party Role Resources #
  ##############################
  SSMRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub ${DeveloperPrefix}3rdPartySSMRole-${Environment}
      AssumeRolePolicyDocument:
        Statement:
          - Sid: ''
            Effect: Allow
            Principal:
              AWS: arn:aws:iam::431055993173:root
            Action: sts:AssumeRole
            Condition:
              Bool:
                aws:MultiFactorAuthPresent: 'true'
      Path: /
      Policies:
        - PolicyName: !Sub 3rdPartySSMRole-${Environment}
          PolicyDocument:
            Statement:
              - Sid: SSMPermissions
                Effect: Allow
                Action:
                  - ssm:DescribeDocument
                  - ssm:GetDocument
                  - ssm:GetParameter
                  - ssm:GetParameters
                  - ssm:GetParametersByPath
                  - ssm:ListCommands
                  - ssm:ListDocuments
                  - ssm:CancelCommand
                  - ssm:PutParameter
                  - ssm:DeleteParameter
                  - ssm:SendCommand
                  - ssm:AddTagsToResource
                  - ssm:RemoveTagsFromResource
                Resource: !Join ['', ['arn:aws:ssm:', !Ref 'AWS::Region', ':', !Ref 'AWS::AccountId', ':', 'parameter/', 'third-party', '/', !Ref Environment,'/', '*']]
              - Sid: SSMDescribePermissions
                Effect: Allow
                Action:
                  - ssm:DescribeParameters
                Resource: '*'
              - Sid: KMSPermissions
                Effect: Allow
                Action:
                  - kms:ListAliases
                Resource: '*'


Outputs:
  SSMRoleName:
    Value: !Ref SSMRole
  SSMRoleNameArn:
    Value: !GetAtt SSMRole.Arn
</code></pre>
",1011527.0,,1011527.0,,2020-02-24 13:36:43,2020-02-25 18:00:55,Strange behavior with CloudFormation updating SNS Topics,<amazon-web-services><aws-lambda><amazon-cloudformation><amazon-sns><aws-cloudformation-custom-resource>,0,16,1.0,,,CC BY-SA 4.0,Apologies for the long read  but I have only included the relevant parts necessary  I have inherited some code and CloudFormation scripts for a project and the previous developer left a cryptic note:  You must update the trailing date on the following items in app-deploy cfn yaml to get changes to take effect   Hes long gone and not somewhere I can ask him about his instructions  Here are the items that must be updated:  Honestly  this seems way out of the ordinary  It looks like he is applying cache-busting techniques to the API Gateway  This  to my knowledge  should not ever have to happen  Boy was I wrong   I recently needed to make one simple change to an SNS Subscription  adding only  as a source for a particular Lambda subscription  Here is that resource with the new source added:  I deployed the scripts and witnessed the updates take place  I checked the console for the proper JSON  revealing this:  All good  I should be good to go  but when I sent the proper message to test the source I got an error message:  @message   2020-02-14 19:11:02 &lt;071acec7-096b-4c45-996a-5ed996df7c23&gt; ERROR AbstractEventHandler:66 - java lang IllegalArgumentException: Unable to process message  Invalid format   And in my UI  Checking the CloudWatch Logs I can see the request being properly made with that source:  @message   2020-02-14 19:13:41  INFO BHIEventHandler:23 - BHI Handler Received: {[{sns: {messageAttributes: {domain={type: String value: payments}  source={type: String value: redirect}  type={type: String value: payment approved}  brand_id={type: String value: 40}}         Suddenly I recalled the cryptic message from the previous developer  but I could not see how that would apply to make a change to SNS  especially since that update appeared to be deployed to AWS properly  Another developer who I was discussing this with said  You could just prove old Bruce wrong by trying it  youve got nothing to lose at this point and we can eliminate that as a distraction  Off we go  we change the dates assigned/appended/tagged to the API resources and redeploy  I see the updates in the console  CodePipeline reports the deployment has been completed successfully  We send the test       and the dang thing worked  My Question Why does making a change to a completely (apparently) unrelated portion of CF make this work  Have we done something wrong  If so  how do I correct this  By request Here is the complete template:  
41328205,1,,,2016-12-26 08:01:27,,3,1138,"<p>Being the cheap-o that I am, I had an idea the other day of running a web app for less than a nickel per month with AWS:</p>

<ul>
<li>Serve a static site (html/css/javascript) via S3</li>
<li>Client-side code and forms post to Lambda golang microservices via API Gateway</li>
<li>Use DynamoDB (25 read/s, 25 write/s, 25GB, 1GB/mo in, 1GB/mo out) as database</li>
</ul>

<p>Would this scheme work with say, cookie and sesssion-based authentication, as the page is being served by one domain name (S3), but the javascript is talking to another domain name (API Gateway)?</p>

<p>What other issues am I likely to run into?</p>
",1176186.0,,,,,2018-03-30 14:16:44,Static website with microservices?,<amazon-web-services><amazon-s3><amazon-dynamodb><aws-lambda><static-site>,3,3,2.0,2018-03-31 04:57:12,,CC BY-SA 3.0,Being the cheap-o that I am  I had an idea the other day of running a web app for less than a nickel per month with AWS:  Serve a static site (html/css/javascript) via S3 Client-side code and forms post to Lambda golang microservices via API Gateway Use DynamoDB (25 read/s  25 write/s  25GB  1GB/mo in  1GB/mo out) as database  Would this scheme work with say  cookie and sesssion-based authentication  as the page is being served by one domain name (S3)  but the javascript is talking to another domain name (API Gateway)  What other issues am I likely to run into  
61475879,1,,,2020-04-28 08:31:38,,1,324,"<p>I want my lambda function to access the database aurora serverless mysql. After some research,  I found that we need to keep the lambda under the same VPC as aurora serverless. But keeping lambda in VPC leads to increase the cold start and also in order to access the internet we need to use NAT gateway which leads to additional cost. Since our application is small we cannot afford additional cost. Is there any other way we can access the aurora serverless database without keeping the lambda function in vpc?</p>
",11139581.0,,,,,2020-05-13 06:27:43,access aurora serverless from public lambda,<aws-lambda><amazon-vpc><aws-aurora-serverless>,2,0,,,,CC BY-SA 4.0,I want my lambda function to access the database aurora serverless mysql  After some research   I found that we need to keep the lambda under the same VPC as aurora serverless  But keeping lambda in VPC leads to increase the cold start and also in order to access the internet we need to use NAT gateway which leads to additional cost  Since our application is small we cannot afford additional cost  Is there any other way we can access the aurora serverless database without keeping the lambda function in vpc  
59253400,1,59254313.0,,2019-12-09 16:58:34,,0,764,"<p>I have a problem regarding cache on S3. Basically I have a lambda that reads a file on S3 which is used as configuration. This file is a JSON. I am using python with boto3 to extract the needed info.</p>

<p>Snippet of my code:</p>

<pre><code>s3 = boto3.resource('s3')
bucketname = ""configurationbucket""
itemname = ""conf.json""
obj = s3.Object(bucketname, itemname)
body = obj.get()['Body'].read()
json_parameters = json.loads(body)  


def my_handler(event, context):
    # using json_paramters data
</code></pre>

<p>The problem is that when I change the json content and I upload the file again on S3, my lambda seems to read the old values, which I suppose is due to S3 doing caching somewhere.</p>

<p>Now I think that there are two ways to solve this problem:</p>

<ul>
<li>to force S3 to invalidate its cache content</li>
<li>to force my lambda to reload the file from S3 without using the cache</li>
</ul>

<p>I do prefer the first solution, because I think it will reduce computation time (reloading the file is an expensive procedure). So, how can I flush my cache? I didn't find on console or on AWS guide the way to do this in a simple manner</p>
",3653343.0,,174777.0,,2019-12-14 02:22:50,2019-12-14 02:22:50,Lambda reading file on S3 - flushing S3 cache,<amazon-web-services><caching><amazon-s3><aws-lambda><boto3>,1,8,,,,CC BY-SA 4.0,I have a problem regarding cache on S3  Basically I have a lambda that reads a file on S3 which is used as configuration  This file is a JSON  I am using python with boto3 to extract the needed info  Snippet of my code:  The problem is that when I change the json content and I upload the file again on S3  my lambda seems to read the old values  which I suppose is due to S3 doing caching somewhere  Now I think that there are two ways to solve this problem:  to force S3 to invalidate its cache content to force my lambda to reload the file from S3 without using the cache  I do prefer the first solution  because I think it will reduce computation time (reloading the file is an expensive procedure)  So  how can I flush my cache  I didnt find on console or on AWS guide the way to do this in a simple manner 
45320021,1,45326398.0,,2017-07-26 07:18:26,,3,1426,"<p>I'm looking into serverless technology (specifically, Python, Django and <a href=""https://github.com/Miserlou/Zappa"" rel=""nofollow noreferrer"">Zappa</a> on AWS Lambda) and one thing about error handling struck me. In the Zappa docs it says</p>

<blockquote>
  <p>By default, AWS Lambda will attempt to retry an event based (non-API Gateway, e.g. CloudWatch) invocation if an exception has been thrown.</p>
</blockquote>

<p>In the <a href=""http://docs.aws.amazon.com/lambda/latest/dg/python-exceptions.html"" rel=""nofollow noreferrer"">AWS Lambda documentation</a>, I read:</p>

<blockquote>
  <p>Depending on the event source, AWS Lambda may retry the failed Lambda function. For example, if Kinesis is the event source, AWS Lambda will retry the failed invocation until the Lambda function succeeds or the records in the stream expire.</p>
</blockquote>

<p>Does this mean a function will be called an infinite number of times when it raises an unhandled exception? If this goes on unchecked, the costs must go through the roof.</p>

<p>Related to that; what is meant by ""until the records in the stream expire""? What records, and what stream?</p>
",308204.0,,1476885.0,,2017-07-26 12:07:07,2017-07-26 12:07:07,Costs related to AWS Lambda retrying failing function?,<python><amazon-web-services><aws-lambda><serverless-architecture>,1,0,,,,CC BY-SA 3.0,Im looking into serverless technology (specifically  Python  Django and  on AWS Lambda) and one thing about error handling struck me  In the Zappa docs it says  By default  AWS Lambda will attempt to retry an event based (non-API Gateway  e g  CloudWatch) invocation if an exception has been thrown   In the   I read:  Depending on the event source  AWS Lambda may retry the failed Lambda function  For example  if Kinesis is the event source  AWS Lambda will retry the failed invocation until the Lambda function succeeds or the records in the stream expire   Does this mean a function will be called an infinite number of times when it raises an unhandled exception  If this goes on unchecked  the costs must go through the roof  Related to that; what is meant by until the records in the stream expire  What records  and what stream  
61499572,1,,,2020-04-29 10:25:54,,0,60,"<p>I am trying to pass in AWS Services in a CostExplorer boto3 method but cannot get the names right. AWS seems to call their services by different names when it comes to passing them in as variables in boto methods. Is there a list of what their services are called when you need to pass them in as Boto variables? For example, to get a response from EC2 I need to write 'Amazon Elastic Compute Cloud - Compute', but for Lambda, I need to write 'AWS Lambda'. There seems to be no pattern.</p>

<p>For example, I get a response for all the services below except 'Amazon CodeBuild' and 'AWS Code Pipeline - Service'. It has been trial and error so far trying to get the names right, and have only found a few random examples in the docs.</p>

<pre><code>def lambda_handler(event, context):
    ce = boto3.client('ce')

    response = ce.get_cost_and_usage(
        TimePeriod={
            'Start': last_week_string,
            'End': string_now
        },
        Granularity='MONTHLY',
        Filter={
            'Dimensions' : {
                'Key' : 'SERVICE',
                'Values' : ['Amazon Elastic Compute Cloud - Compute', 'Amazon Route 53', 'AWS Lambda', 'Amazon Virtual Private Cloud', 'Amazon ElastiCache', 'EC2 - Other', 'Amazon DynamoDB', 'Amazon CodeBuild', 'AWS Code Pipeline - Service']
            }
        },
        GroupBy=[{
            'Type': 'DIMENSION',
            'Key': 'SERVICE'
        }
        ],
        Metrics=['UnblendedCost']

    )
</code></pre>
",12089251.0,,,,,2020-04-29 10:25:54,AWS Services variable names in boto3 method,<amazon-web-services><aws-lambda>,0,2,,,,CC BY-SA 4.0,I am trying to pass in AWS Services in a CostExplorer boto3 method but cannot get the names right  AWS seems to call their services by different names when it comes to passing them in as variables in boto methods  Is there a list of what their services are called when you need to pass them in as Boto variables  For example  to get a response from EC2 I need to write Amazon Elastic Compute Cloud - Compute  but for Lambda  I need to write AWS Lambda  There seems to be no pattern  For example  I get a response for all the services below except Amazon CodeBuild and AWS Code Pipeline - Service  It has been trial and error so far trying to get the names right  and have only found a few random examples in the docs   
43870717,1,,,2017-05-09 13:07:12,,3,329,"<p>Asynch programming helps to increase the number of requests a server can handle simultaneously but not necessarily the time to produce the response.</p>

<p><strong>Since AWS cost depends only on the number of requests and on the execution time, does it make sense to use asynch programming?</strong></p>

<p>In my case, I have a java lambda that needs to call a single http server. No more than that. </p>

<p>I could either use an http synch or asynch library.
The synch code is more readable. I understand the asynch code is generally better in case of concurrency but probably this not in case of AWS lambda.</p>
",5493526.0,,,,,2017-05-09 18:57:07,Does async programming make sense for AWS lambda?,<asynchronous><aws-lambda>,1,0,,,,CC BY-SA 3.0,Asynch programming helps to increase the number of requests a server can handle simultaneously but not necessarily the time to produce the response  Since AWS cost depends only on the number of requests and on the execution time  does it make sense to use asynch programming  In my case  I have a java lambda that needs to call a single http server  No more than that   I could either use an http synch or asynch library  The synch code is more readable  I understand the asynch code is generally better in case of concurrency but probably this not in case of AWS lambda  
43876653,1,,,2017-05-09 17:48:46,,1,666,"<p>Some of my data is in Mongo replicas that are hosted in docker containers running in kubernetes cluster. I need to access this data from the AWS lambda that is running in the same VPC and subnet (as the kubernetes minions with mongo db). lambda as well as the kubernetes minions (hosting mongo containers) are run under the same security group. I am trying to connect using url ""mongodb://mongo-rs-1-svc,mongo-rs-2-svc,mongo-rs-3-svc/res?replicaSet=mongo_rs"" where mongo-rs-x-svc are three kubernetes services that enables access to the appropriate replicas. When I try to connect using this url, it fails to resolve the mongo replica url (e.g. mongo-rs-2-svc). Same URL works fine for my web service that is running in its own docker container in the same kubernetes cluster.</p>

<p>Here is the error I get from mongo client that I use...
{\""name\"":\""MongoError\"",\""message\"":\""failed to connect to server [mongo-rs-1-svc:27017] on first connect [MongoError: getaddrinfo ENOTFOUND mongo-rs-1-svc mongo-rs-1-svc:27017]\""}"". I tried replacing mongo-rs-x-svc to their internal ip addresses in the url. In this case the above name resolution error disappeared but got another error - {\""name\"":\""MongoError\"",\""message\"":\""failed to connect to server [10.0.170.237:27017] on first connect [MongoError: connection 5 to 10.0.170.237:27017 timed out]\""}</p>

<p>What should I be doing to enable this access successfully?</p>

<p>I understand that I can use the webservice to access this data as intermediary but since my lambda is in VPC, I have to deploy NAT gateways and that would increase the cost. Is there a way to access the webservice using the internal endpoint instead of public url? May be that is another way to get data.</p>

<p>If any of you have a solution for this scenario, please share. I went through many threads that showed up as similar questions or in search results but neither had a solution for this case.</p>
",1807828.0,,2593745.0,,2017-05-09 20:21:48,2017-05-10 00:01:35,Accessing Mongo replicas in kubernetes cluster from AWS lambdas,<node.js><mongodb><amazon-web-services><kubernetes><aws-lambda>,2,0,,,,CC BY-SA 3.0,Some of my data is in Mongo replicas that are hosted in docker containers running in kubernetes cluster  I need to access this data from the AWS lambda that is running in the same VPC and subnet (as the kubernetes minions with mongo db)  lambda as well as the kubernetes minions (hosting mongo containers) are run under the same security group  I am trying to connect using url mongodb://mongo-rs-1-svc mongo-rs-2-svc mongo-rs-3-svc/res replicaSet=mongo_rs where mongo-rs-x-svc are three kubernetes services that enables access to the appropriate replicas  When I try to connect using this url  it fails to resolve the mongo replica url (e g  mongo-rs-2-svc)  Same URL works fine for my web service that is running in its own docker container in the same kubernetes cluster  Here is the error I get from mongo client that I use    {\name\:\MongoError\ \message\:\failed to connect to server [mongo-rs-1-svc:27017] on first connect [MongoError: getaddrinfo ENOTFOUND mongo-rs-1-svc mongo-rs-1-svc:27017]\}  I tried replacing mongo-rs-x-svc to their internal ip addresses in the url  In this case the above name resolution error disappeared but got another error - {\name\:\MongoError\ \message\:\failed to connect to server [10 0 170 237:27017] on first connect [MongoError: connection 5 to 10 0 170 237:27017 timed out]\} What should I be doing to enable this access successfully  I understand that I can use the webservice to access this data as intermediary but since my lambda is in VPC  I have to deploy NAT gateways and that would increase the cost  Is there a way to access the webservice using the internal endpoint instead of public url  May be that is another way to get data  If any of you have a solution for this scenario  please share  I went through many threads that showed up as similar questions or in search results but neither had a solution for this case  
59288470,1,,,2019-12-11 14:54:21,,1,427,"<p>I have to set up an AWS instance for a web application that is being used sporadically, a few hours at a time, a few times per month. The application requires a sizeable instance in terms of virtual cpus and memory, so keeping it running 24/7 would run up a steep bill, and since the time it is being used is below ~5% I am looking for a way to automatically suspend the instance if CPU utilization drops below 10% for >2 hours (for example). Also, ideally (but not strictly required) a request to the application's URL would start the instance if it is suspended.</p>

<ol>
<li>My first idea is to set up CloudWatch to record any requests to the URL, as well as the instance's CPU utilization. A Lambda function then periodically checks if the last request was over 2h ago and CPU utilization has been low for that time as well; if true then suspend the instance.  </li>
<li>The starting of the instance could be done by having a special ""wakeup"" URL (separate from the app's URL) which triggers a lambda function to wake the instance if asleep. </li>
</ol>

<p>Is there a recommended or more standard way to achieve this?</p>
",729288.0,,,,,2019-12-12 09:26:41,Suspend idle AWS instance automatically,<amazon-web-services><amazon-ec2><aws-lambda><amazon-cloudwatch><suspend>,2,0,,,,CC BY-SA 4.0,I have to set up an AWS instance for a web application that is being used sporadically  a few hours at a time  a few times per month  The application requires a sizeable instance in terms of virtual cpus and memory  so keeping it running 24/7 would run up a steep bill  and since the time it is being used is below ~5% I am looking for a way to automatically suspend the instance if CPU utilization drops below 10% for &gt;2 hours (for example)  Also  ideally (but not strictly required) a request to the applications URL would start the instance if it is suspended   My first idea is to set up CloudWatch to record any requests to the URL  as well as the instances CPU utilization  A Lambda function then periodically checks if the last request was over 2h ago and CPU utilization has been low for that time as well; if true then suspend the instance    The starting of the instance could be done by having a special wakeup URL (separate from the apps URL) which triggers a lambda function to wake the instance if asleep    Is there a recommended or more standard way to achieve this  
59290512,1,,,2019-12-11 16:49:56,,1,509,"<p>How can I prevent Denial of Wallet attacks against AWS Cloudfront?</p>

<p>Here's my specific situation: I have a Cloudfront distribution where Lambda@Edge functions serve web pages and API requests for my application. I need to rate-limit requests made to Cloudfront based on the IP address of the user. Without any kind of rate-limiting in place, it's possible for a malicious user to make millions of slow requests to the distribution that wouldn't be blocked by AWS's DDOS protections and which would lead to significant charges. This is especially important here since Lambda@Edge functions cost 3x as much as ordinary Lambda functions and don't come with a free tier.</p>

<p>It seemed practical to use AWS WAF in order to accomplish this. However, I <a href=""https://stackoverflow.com/questions/59288717/aws-waf-pricing-for-blocked-requests"">recently found out</a> that WAF charges for all incoming requests, regardless of if they are blocked or not. So a Denial of Wallet attack would still be possible here.</p>

<p>Is there a method or a general strategy that I can implement here that doesn't involve AWS WAF?  </p>

<p>The limits need to be very tight. Even paying $50 per month for malicious requests would be considered too high.</p>
",12390446.0,,,,,2020-03-21 16:55:48,AWS Cloudfront Prevent Denial of Wallet?,<amazon-web-services><aws-lambda><amazon-cloudfront><ddos>,2,0,1.0,,,CC BY-SA 4.0,How can I prevent Denial of Wallet attacks against AWS Cloudfront  Heres my specific situation: I have a Cloudfront distribution where Lambda@Edge functions serve web pages and API requests for my application  I need to rate-limit requests made to Cloudfront based on the IP address of the user  Without any kind of rate-limiting in place  its possible for a malicious user to make millions of slow requests to the distribution that wouldnt be blocked by AWSs DDOS protections and which would lead to significant charges  This is especially important here since Lambda@Edge functions cost 3x as much as ordinary Lambda functions and dont come with a free tier  It seemed practical to use AWS WAF in order to accomplish this  However  I  that WAF charges for all incoming requests  regardless of if they are blocked or not  So a Denial of Wallet attack would still be possible here  Is there a method or a general strategy that I can implement here that doesnt involve AWS WAF    The limits need to be very tight  Even paying $50 per month for malicious requests would be considered too high  
61521108,1,,,2020-04-30 10:18:41,,1,193,"<p>I created the S3 bucket first. With the necessary permission. And enabled notification to lambda function on all object:put.</p>

<p>I then created a Cost and Usage report and selected the above S3 bucket as the storage. Permissions look correct, as it could create/update the test file with the name ""aws-programmatic-access-test-object""</p>

<p>Its been few days now, and CUR say it has been generating reports. But I cannot see the files in the S3 bucket.</p>

<p>Interestingly my lambda function is being invoked with notifications about object:put.</p>

<p>But the files are nowhere to be found. </p>

<p>Can someone help understand what might be happening please ? </p>

<p><a href=""https://i.stack.imgur.com/oAZLP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oAZLP.png"" alt=""CUR config""></a></p>

<p><a href=""https://i.stack.imgur.com/4gMnY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4gMnY.png"" alt=""S3 bucket""></a></p>

<p><a href=""https://i.stack.imgur.com/qRwXj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qRwXj.png"" alt=""Lambda notification log""></a></p>
",298366.0,,,,,2020-04-30 10:18:41,AWS Cost and Usage Report - Files not created in S3 bucket,<amazon-web-services><amazon-s3><aws-lambda>,0,1,,,,CC BY-SA 4.0,I created the S3 bucket first  With the necessary permission  And enabled notification to lambda function on all object:put  I then created a Cost and Usage report and selected the above S3 bucket as the storage  Permissions look correct  as it could create/update the test file with the name aws-programmatic-access-test-object Its been few days now  and CUR say it has been generating reports  But I cannot see the files in the S3 bucket  Interestingly my lambda function is being invoked with notifications about object:put  But the files are nowhere to be found   Can someone help understand what might be happening please       
59508880,1,59508914.0,,2019-12-28 07:09:18,,1,224,"<p>I created a new GCP Project, and created a hello-world serverless cloud function from GCP Console.</p>

<p>I immediately get a mail that a Firebase project has been created as well, and my Cloud Function has been upgraded to 'Blaze' plan of firebase.</p>

<p>My question is, which pricing model should I refer? </p>

<p><a href=""https://cloud.google.com/functions/pricing"" rel=""nofollow noreferrer"">https://cloud.google.com/functions/pricing</a> - GCP</p>

<p>OR</p>

<p><a href=""https://firebase.google.com/pricing"" rel=""nofollow noreferrer"">https://firebase.google.com/pricing</a> - Firebase</p>

<p>There is a difference in both pricing, where if you see in firebase pricing, 125K invocations/month are free, while in GCP Pricing, 2M invocations/month are free.</p>

<p>I am confused on which pricing is applicable if I simply want to use Cloud Functions, because even if I create a function from GCP Console, firebase console also shows the function in its console.</p>
",3717722.0,,,,,2019-12-28 07:28:14,Dual Pricing for GCP Cloud Functions with Firebase,<firebase><google-cloud-platform><google-cloud-functions><serverless>,1,0,,,,CC BY-SA 4.0,I created a new GCP Project  and created a hello-world serverless cloud function from GCP Console  I immediately get a mail that a Firebase project has been created as well  and my Cloud Function has been upgraded to Blaze plan of firebase  My question is  which pricing model should I refer    - GCP OR  - Firebase There is a difference in both pricing  where if you see in firebase pricing  125K invocations/month are free  while in GCP Pricing  2M invocations/month are free  I am confused on which pricing is applicable if I simply want to use Cloud Functions  because even if I create a function from GCP Console  firebase console also shows the function in its console  
44230875,1,44267723.0,,2017-05-28 18:52:41,,0,173,"<p>I'm trying to make each call to lambda to log the billed duration, to track the cost of using lambda.</p>

<p>When you invoke lambda with SDK or CLI, you could easily get the LogResult by just adding the parameter <code>LogType: tail</code></p>

<p>Then you get the LogResult as a part of Response, where you can extract the billed duration.</p>

<p>Now, I was trying to do similar thing when we invoke lambda through API Gateway.</p>

<p>How could I get the LogResult and BilledDuration in this case?</p>
",8078456.0,,,,,2017-05-30 16:44:20,how to make get LogResult of AWS Lambda with API gateway?,<lambda><aws-lambda><aws-api-gateway>,1,0,,,,CC BY-SA 3.0,Im trying to make each call to lambda to log the billed duration  to track the cost of using lambda  When you invoke lambda with SDK or CLI  you could easily get the LogResult by just adding the parameter  Then you get the LogResult as a part of Response  where you can extract the billed duration  Now  I was trying to do similar thing when we invoke lambda through API Gateway  How could I get the LogResult and BilledDuration in this case  
62957964,1,,,2020-07-17 16:17:42,,0,50,"<p>I want to make a bot that makes other bots on Telegram platform. I want to use AWS infrastructure, look like their Lamdba functions are perfect fit, pay for them only when they are active. In my concept, each bot equal to one lambda function, and they all share the same codebase.</p>
<p>At the starting point, I thought to make each new Lambda function programmatically, but this will bring me problems later I think, like need to attach many services programmatically via AWS SDK: Gateway API, DynamoDB. But the main problem, how I will update the codebase for these 1000+ functions later? I think that bash script is a bad idea here.</p>
<p>So, I moved forward and found SAM (AWS Serverless Application Model) and CloudFormatting, which should help me I guess. But I can't understand the concept. I can make a stack with all the required resources, but how will I make new bots from this one stack? Or should I build a template and make new stacks for each new bot programmatically via AWS SDK from this template?</p>
<p>Next, how to update them later? For example, I want to update all bots that have version 1.1 to version 1.2. How I will replace them? Should I make a new stack or can I update older ones? I don't see any options in UI of CloudFormatting or any related methods in AWS SDK for that.</p>
<p>Thanks</p>
",1412599.0,,,,,2020-07-18 08:12:12,What is the proper way to build many Lambda functions and updated them later?,<amazon-web-services><aws-lambda><amazon-cloudformation><telegram-bot>,2,0,0.0,,,CC BY-SA 4.0,I want to make a bot that makes other bots on Telegram platform  I want to use AWS infrastructure  look like their Lamdba functions are perfect fit  pay for them only when they are active  In my concept  each bot equal to one lambda function  and they all share the same codebase  At the starting point  I thought to make each new Lambda function programmatically  but this will bring me problems later I think  like need to attach many services programmatically via AWS SDK: Gateway API  DynamoDB  But the main problem  how I will update the codebase for these 1000+ functions later  I think that bash script is a bad idea here  So  I moved forward and found SAM (AWS Serverless Application Model) and CloudFormatting  which should help me I guess  But I cant understand the concept  I can make a stack with all the required resources  but how will I make new bots from this one stack  Or should I build a template and make new stacks for each new bot programmatically via AWS SDK from this template  Next  how to update them later  For example  I want to update all bots that have version 1 1 to version 1 2  How I will replace them  Should I make a new stack or can I update older ones  I dont see any options in UI of CloudFormatting or any related methods in AWS SDK for that  Thanks 
45359497,1,45585954.0,,2017-07-27 19:31:21,,13,661,"<p>I have a program that performs several thousand monte-carlo simulations to predict a result; I can't say what they really predict, so I'm going to use another example from ""the indisputable existence of santa claus"", since the content of those algorithms are not relevant to the question.  I want to know how often each square on a Monopoly board is visited (to predict which the best properties to buy are).  To do this, I simulate thousands of games and collate the results.  My current implementation is a stand-alone C# application but I want to move it to the cloud so that I can provide this as a service - each user can get personalised results by submitting the number of sides that each of their dice have.</p>

<p>The current implementation is also quite slow - it is very parallisable since each simulation is entirely independent but I only have 8 cores, so it takes upwards of 20 minutes to complete the full prediction with about 50000 individual simulations on my local machine.</p>

<p>The plan is to have AWS lambda functions each run one (or several) simulations and then collate - basically mapreduce it.  I looked in to using AWS EMR (Elastic MapReduce) but that is way too large-scale for what I want, spinning up the instances to run the computations alone seems to take longer than the whole calculation alone (which would not matter for multi-hour offline analyses, but I want low-latency to respond over a web request).</p>

<p>The ideal as I see it would be:</p>

<p>Lambda 0 - Fires off many other lambda functions, each doing a small part of the calculation.
Lambda 1..N - Do many simulations in parallel (the number is not a constant).
Lambda N+1 - Collate all the results and return the answer.</p>

<p>There is a lambda mapreduce framework here:</p>

<p><a href=""https://github.com/awslabs/lambda-refarch-mapreduce"" rel=""noreferrer"">https://github.com/awslabs/lambda-refarch-mapreduce</a></p>

<p>But it seems to have one major drawback - each time a map stage completes, it writes its results to S3 (I'm fine with using that as a temporary) then triggers a new lambda via an event.  That triggered lambda looks to see if all the results have been written to storage yet.  If not, it ends, if yes it does the reduction step.  That seems like a fair solution, but I'm just slightly concerned about a) race-hazards when two results come in together, could two reducers both compute the results?  And b) that seems like it is firing off a lot of lambdas that all just decide not to run (I know they're cheap to run, but doubling the number to two per simulation - calculate and maybe reduce - will obviously double the costs).  Is there a way to fire off an S3 result after, say, 100 files are written to a folder instead of after every one?</p>

<p>I looked at using step functions, but I'm not sure how to fire many lambdas in parallel in one step and have them all return before the state machine transitions.  Step functions would however be useful for the final wrinkle - I want to hide all this behind an API.</p>

<p>From what I've read, APIs can fire off a lambda and return the result of that lambda, but I don't want the invoked lambda to be the one returning the result.  It isn't when you instead invoke a step function from the API, the results of the last state are returned by the API call instead.</p>

<p>In short, I want:</p>

<p>API request -> Calculate results in parallel -> API response</p>

<p>It is that bit in the middle I'm not clear how to do, while being able to return all the results as a response to the original request - either on their own are easy.</p>

<p>A few options I can see:</p>

<p>Use a step function, which is natively supported by the AWS API gateway now, and invoke multiple lambdas in one state, waiting for them all to return before transitioning.</p>

<p>Use AWS EMR, but somehow keep the provisioned instances always live to avoid the provisioning time overheads.  This obviously negates the scalability of Lambda and is more expensive.</p>

<p>Use the mapreduce framework, or something similar, and find a way to respond to an incoming request from a different lambda to the one that was initially invoked by the API request.  Ideally also reduce the number of S3 events involved here, but that's not a priority.</p>

<p>Respond instantly to the original API request from the first lambda, then push more data to the user later when the calculations finish (they should only take about 30 seconds with the parallelism, and the domain is such that that is an acceptable time to wait for a response, even an HTTP response).</p>

<p>I doubt it will make any difference to the solution, since it is just an expansion of the middle bit, not a fundamental change, but the real calculation is iterative, so would be:</p>

<p>Request -> Mapreduce -> Mapreduce -> ... -> Response</p>

<p>As long as I know how to chain one set of lambda functions within a request, chaining more should be just more of the same (I hope).</p>

<p>Thank you.</p>

<p>P.S.  I can't create them, and neither the tags <code>aws-emr</code> nor <code>aws-elastic-mapreduce</code> exist yet.</p>
",7931378.0,,7931378.0,,2017-07-27 19:38:00,2017-08-11 14:47:44,How can I return the result of a mapreduce operation to an AWS API request,<amazon-web-services><aws-lambda><aws-api-gateway><aws-step-functions>,2,1,0.0,,,CC BY-SA 3.0,I have a program that performs several thousand monte-carlo simulations to predict a result; I cant say what they really predict  so Im going to use another example from the indisputable existence of santa claus  since the content of those algorithms are not relevant to the question   I want to know how often each square on a Monopoly board is visited (to predict which the best properties to buy are)   To do this  I simulate thousands of games and collate the results   My current implementation is a stand-alone C# application but I want to move it to the cloud so that I can provide this as a service - each user can get personalised results by submitting the number of sides that each of their dice have  The current implementation is also quite slow - it is very parallisable since each simulation is entirely independent but I only have 8 cores  so it takes upwards of 20 minutes to complete the full prediction with about 50000 individual simulations on my local machine  The plan is to have AWS lambda functions each run one (or several) simulations and then collate - basically mapreduce it   I looked in to using AWS EMR (Elastic MapReduce) but that is way too large-scale for what I want  spinning up the instances to run the computations alone seems to take longer than the whole calculation alone (which would not matter for multi-hour offline analyses  but I want low-latency to respond over a web request)  The ideal as I see it would be: Lambda 0 - Fires off many other lambda functions  each doing a small part of the calculation  Lambda 1  N - Do many simulations in parallel (the number is not a constant)  Lambda N+1 - Collate all the results and return the answer  There is a lambda mapreduce framework here:  But it seems to have one major drawback - each time a map stage completes  it writes its results to S3 (Im fine with using that as a temporary) then triggers a new lambda via an event   That triggered lambda looks to see if all the results have been written to storage yet   If not  it ends  if yes it does the reduction step   That seems like a fair solution  but Im just slightly concerned about a) race-hazards when two results come in together  could two reducers both compute the results   And b) that seems like it is firing off a lot of lambdas that all just decide not to run (I know theyre cheap to run  but doubling the number to two per simulation - calculate and maybe reduce - will obviously double the costs)   Is there a way to fire off an S3 result after  say  100 files are written to a folder instead of after every one  I looked at using step functions  but Im not sure how to fire many lambdas in parallel in one step and have them all return before the state machine transitions   Step functions would however be useful for the final wrinkle - I want to hide all this behind an API  From what Ive read  APIs can fire off a lambda and return the result of that lambda  but I dont want the invoked lambda to be the one returning the result   It isnt when you instead invoke a step function from the API  the results of the last state are returned by the API call instead  In short  I want: API request -&gt; Calculate results in parallel -&gt; API response It is that bit in the middle Im not clear how to do  while being able to return all the results as a response to the original request - either on their own are easy  A few options I can see: Use a step function  which is natively supported by the AWS API gateway now  and invoke multiple lambdas in one state  waiting for them all to return before transitioning  Use AWS EMR  but somehow keep the provisioned instances always live to avoid the provisioning time overheads   This obviously negates the scalability of Lambda and is more expensive  Use the mapreduce framework  or something similar  and find a way to respond to an incoming request from a different lambda to the one that was initially invoked by the API request   Ideally also reduce the number of S3 events involved here  but thats not a priority  Respond instantly to the original API request from the first lambda  then push more data to the user later when the calculations finish (they should only take about 30 seconds with the parallelism  and the domain is such that that is an acceptable time to wait for a response  even an HTTP response)  I doubt it will make any difference to the solution  since it is just an expansion of the middle bit  not a fundamental change  but the real calculation is iterative  so would be: Request -&gt; Mapreduce -&gt; Mapreduce -&gt;     -&gt; Response As long as I know how to chain one set of lambda functions within a request  chaining more should be just more of the same (I hope)  Thank you  P S   I cant create them  and neither the tags  nor  exist yet  
62959419,1,62966138.0,,2020-07-17 17:56:59,,0,1454,"<p>I use AWS Simple Email Services (SES) for email. I've configured SES to save incoming email to an S3 bucket, which triggers an AWS Lambda function. This function reads the new object and forwards the object contents to an alternate email address.</p>
<p>I'd like to log some basic info. from my AWS Lambda function during invocation -- who the email is from, to whom it was sent, if it contained any links, etc.</p>
<p>Ideally I'd save this info. to a database, but since AWS Lambda functions are costly (relatively so to other AWS ops.), I'd like to do this as efficiently as possible.</p>
<p>I was thinking I could issue an HTTPS GET request to a private endpoint with a query-string containing the info. I want logged. Since I could fire my request async. at the outset and continue processing, I thought this might be a cheap and efficient approach.</p>
<p>Is this a good method? Are there any alternatives?</p>
<p>My Lambda function fires irregularly so despite Lambda functions being kept alive for 10 minutes or so post-firing, it seems a database connection is likely slow and costly since AWS charges per 100ms of usage.</p>
<p>Since I could conceivable get thousands of emails/month, ensuring my Lambda function is efficient is paramount to cost. I maintain 100s of domain names so my numbers aren't exaggerated. Thanks in advance.</p>
",13945873.0,,13945873.0,,2020-07-18 01:14:12,2020-08-09 22:45:17,Best method to persist data from an AWS Lambda invocation?,<aws-lambda>,1,4,,,,CC BY-SA 4.0,I use AWS Simple Email Services (SES) for email  Ive configured SES to save incoming email to an S3 bucket  which triggers an AWS Lambda function  This function reads the new object and forwards the object contents to an alternate email address  Id like to log some basic info  from my AWS Lambda function during invocation -- who the email is from  to whom it was sent  if it contained any links  etc  Ideally Id save this info  to a database  but since AWS Lambda functions are costly (relatively so to other AWS ops )  Id like to do this as efficiently as possible  I was thinking I could issue an HTTPS GET request to a private endpoint with a query-string containing the info  I want logged  Since I could fire my request async  at the outset and continue processing  I thought this might be a cheap and efficient approach  Is this a good method  Are there any alternatives  My Lambda function fires irregularly so despite Lambda functions being kept alive for 10 minutes or so post-firing  it seems a database connection is likely slow and costly since AWS charges per 100ms of usage  Since I could conceivable get thousands of emails/month  ensuring my Lambda function is efficient is paramount to cost  I maintain 100s of domain names so my numbers arent exaggerated  Thanks in advance  
61861078,1,61861110.0,,2020-05-18 01:29:18,,-1,73,"<p>I am building a basic crud web app and I want to deploy it to AWS. I read about many services only at a high, but before I implemented anything I wanted to know if the following plan was feasible and at least <em>somewhat</em> efficient:</p>

<ol>
<li>Deploy the static content of a (React) Single Page Application to an S3 bucket. My plan is to use an S3 bucket to serve this static content with a custom domain name.</li>
<li>Deploy an API (that performs crud operations) using API Gateway and Lambda. This API will interact with DynamoDB for storage. This API is expected to only be used by my single page app so my assumption is that I can secure it and make it ""non-public"".</li>
</ol>

<p>My understanding is that the above is a relatively cost efficient architecture for aws (though I expect to remain in the free tier).</p>

<p>Any major holes in my plan or something I overlooked? I know I could choose a million ways to do this including Elastic Beanstalk, but I would appreciate any advice on any different ways this can be achieved, as well as any insight on the trade offs I can make. </p>
",8202838.0,,,,,2020-05-18 02:33:54,AWS single page app and microservice use case,<amazon-web-services><amazon-s3><aws-lambda><amazon-dynamodb><aws-api-gateway>,1,0,,,,CC BY-SA 4.0,I am building a basic crud web app and I want to deploy it to AWS  I read about many services only at a high  but before I implemented anything I wanted to know if the following plan was feasible and at least somewhat efficient:  Deploy the static content of a (React) Single Page Application to an S3 bucket  My plan is to use an S3 bucket to serve this static content with a custom domain name  Deploy an API (that performs crud operations) using API Gateway and Lambda  This API will interact with DynamoDB for storage  This API is expected to only be used by my single page app so my assumption is that I can secure it and make it non-public   My understanding is that the above is a relatively cost efficient architecture for aws (though I expect to remain in the free tier)  Any major holes in my plan or something I overlooked  I know I could choose a million ways to do this including Elastic Beanstalk  but I would appreciate any advice on any different ways this can be achieved  as well as any insight on the trade offs I can make   
61873077,1,,,2020-05-18 15:24:27,,0,31,"<p>I'm using an AWS serverless architecture to build an ecommerce platform and was wondering if the following is a smart way to handle images:</p>

<ul>
<li>Use S3 for image storage.</li>
<li>When a new product is uploaded (description, images, etc) it will trigger a lambda function to create different sized images using a node library called <code>sharp</code> (a thumbnail, medium and large image size).</li>
<li>These images will be stored in S3 (forget about caching for now if possible) and the URLs will be saved as part of the product's object in a no-sql DB.</li>
<li>Do the same for each product image I want to add (i.e. store 3 instances of the same image for each image).</li>
</ul>

<p>Aside from using a service like cloudinary is this a cheap and scalable way to handle images?</p>

<p>Many thanks</p>
",2426524.0,,174777.0,,2020-05-19 01:35:29,2020-05-19 01:35:29,Advice on handling images for ecommerce item creation,<node.js><amazon-web-services><amazon-s3><aws-lambda>,0,2,,,,CC BY-SA 4.0,Im using an AWS serverless architecture to build an ecommerce platform and was wondering if the following is a smart way to handle images:  Use S3 for image storage  When a new product is uploaded (description  images  etc) it will trigger a lambda function to create different sized images using a node library called  (a thumbnail  medium and large image size)  These images will be stored in S3 (forget about caching for now if possible) and the URLs will be saved as part of the products object in a no-sql DB  Do the same for each product image I want to add (i e  store 3 instances of the same image for each image)   Aside from using a service like cloudinary is this a cheap and scalable way to handle images  Many thanks 
59538936,1,,,2019-12-31 02:31:10,,0,130,"<p><strong>The thing what i want do</strong>:<br>
I'm thinking about use aws dynamodb as my new web site(press and bbs mixed type) main server.<br>
But I can't design database, because dynamodb maybe cannot satisfy my requirements.<br>
Somebody knows how can i do?</p>

<p><strong>Foundation</strong><br>
My boss's plan is spawning many websites and close it several days after if it's traffic is low. (and go on until it success)<br>
He has many domain names (over hundreads) and want make many websites.<br>
And he also wants selling our system for web solution to customers. ( internet newspaper, blogs, magazines, etc... )<br>
It is very large plan, anyway i must design the system.  </p>

<p><strong>Situation</strong><br>
first, most of website is going trashcan after several days.<br>
so make each websites on on-premise server is not good way because it means too much manual-jobs. (just deploying the site, undeploying the site)<br>
And also on-premised pricing is not good in this case because we can't predict any webserver is closing (because traffic size lower than survive-line), or survive because traffic is grow faster.<br>
i guess 80% of sites are just ghost sites.  </p>

<p>second, the amount of <strong>spawining</strong> websites are not measurable. Because my boss always yelled me 'as many as can!'  </p>

<p>And I also consider about this project is abandoned by my company after launching some month after. (maybe they want keep-and-not-maintain mode with not consume many price)   </p>

<p>Anyway, ths problem is the...<br>
1. can't predict how many sites are alive and gets high traffic.<br>
2. can't predict how many sites are ghosts or few traffics (under 1000users/days).<br>
3. even the project is abandoned, the'll want keep-and-not-maintain-mode (with lowcost as they can)</p>

<p>conslusion is<br>
1. most sites are going to keep-and-not-maintain-mode. it's cost must low<br>
2. some few sites are 'burst' in after times. it must support this situation also.  </p>

<p><strong>So next is my answer:</strong><br>
<strong>Server</strong> : full faas serverless with stateless server (lambda, cloudrun, function, etc... i think cloudrun is best because development is very it can easy using my own docker)<br>
the reason is...<br>
if this website is 'just keep and not maintain it', we did not pay for it. or very small price.<br>
if the site growing up or traffic is burst, it can support on-live.  </p>

<p><strong>Database</strong> : serverless because it's traffic is not measurable in this stage.<br>
there only two serverless (means needs no pay for provisioned instance). Aurora and Dynamodb.  </p>

<p><strong>Aurora</strong> is my first choice because I always using mysql or mariadb in webservice.<br>
But there is two problem.<br>
Aurora serverless is too sensitive acu scaling. and it never going scale-down until 5 minutes flowed. (and if it scaled up to 4 acu, 5*4=20acu/min. just one connection and query once, but pricing=$0.03)
It is pretty sure if one of these site's traffic is changed from 'nobody use it' to 'a few people comes', Autoscale price is going to very higher. even i set max acu scale limit to 1, there are another problems. (well-known problems). If aurora start from cold, it takes very long time(about 30seconds). And if it is scaling, it also takes some delay.<br>
Anyway, i can't make good pricing design for Aurora.  </p>

<p>So I moved on <strong>Dynamodb</strong>.<br>
It has very good pricing rules. In on-demand mode, under $2 per 1,000,000 rcu.<br>
I guess most of site data is stored in S3. So read/write other data is not much than 1kb/4kb, and even i transaction for each unit, under $2 per 500,000rcu.  </p>

<p><strong>Actual Problem</strong><br>
But it has trap point.<br>
If I do scan, its pricing is going to very higher. because returns a lot of data (until 1mb). I accepted. So I must using query. But query is working same as 'scan in a partition'. And pricing is going to scanned amount of read-write-units(until 1mb). So if I have 100 items in a key and query just one item (total 100 members and just select one of that members), the dynamodb charges 100 times pricing because it scanned 100 items. (assume each item lower than 1kb). I cant believe it. (x100 pricing) so googled it.
I found many people are design dynamodb seperated hash-key. (like 'members-startfrom-a', 'members-startfrom-b', ... )</p>

<p><strong>Question</strong><br>
Is it normal?<br>
In this time, I think dynamodb cannot support normal website (bbs or press, or blog). because they are rdb optimized structures. Even paging articles needs full-scan(or partitioned full-scan).  Is it right? or there is a 'migration rules' for rdb user to dynamodb user?<br>
Someone can show me 'case-study'?<br>
Most case people use dynamodb as subdb.  </p>
",1101221.0,,1101221.0,,2020-01-02 04:33:43,2020-01-02 04:33:43,"is it impossible aws dynamodb design with low-cost with normal website(bbs, blog, press...)?",<database><amazon-web-services><nosql><amazon-dynamodb><serverless>,1,1,,,,CC BY-SA 4.0,The thing what i want do: Im thinking about use aws dynamodb as my new web site(press and bbs mixed type) main server  But I cant design database  because dynamodb maybe cannot satisfy my requirements  Somebody knows how can i do  Foundation My bosss plan is spawning many websites and close it several days after if its traffic is low  (and go on until it success) He has many domain names (over hundreads) and want make many websites  And he also wants selling our system for web solution to customers  ( internet newspaper  blogs  magazines  etc    ) It is very large plan  anyway i must design the system    Situation first  most of website is going trashcan after several days  so make each websites on on-premise server is not good way because it means too much manual-jobs  (just deploying the site  undeploying the site) And also on-premised pricing is not good in this case because we cant predict any webserver is closing (because traffic size lower than survive-line)  or survive because traffic is grow faster  i guess 80% of sites are just ghost sites    second  the amount of spawining websites are not measurable  Because my boss always yelled me as many as can    And I also consider about this project is abandoned by my company after launching some month after  (maybe they want keep-and-not-maintain mode with not consume many price)    Anyway  ths problem is the    1  cant predict how many sites are alive and gets high traffic  2  cant predict how many sites are ghosts or few traffics (under 1000users/days)  3  even the project is abandoned  thell want keep-and-not-maintain-mode (with lowcost as they can) conslusion is 1  most sites are going to keep-and-not-maintain-mode  its cost must low 2  some few sites are burst in after times  it must support this situation also    So next is my answer: Server : full faas serverless with stateless server (lambda  cloudrun  function  etc    i think cloudrun is best because development is very it can easy using my own docker) the reason is    if this website is just keep and not maintain it  we did not pay for it  or very small price  if the site growing up or traffic is burst  it can support on-live    Database : serverless because its traffic is not measurable in this stage  there only two serverless (means needs no pay for provisioned instance)  Aurora and Dynamodb    Aurora is my first choice because I always using mysql or mariadb in webservice  But there is two problem  Aurora serverless is too sensitive acu scaling  and it never going scale-down until 5 minutes flowed  (and if it scaled up to 4 acu  5*4=20acu/min  just one connection and query once  but pricing=$0 03) It is pretty sure if one of these sites traffic is changed from nobody use it to a few people comes  Autoscale price is going to very higher  even i set max acu scale limit to 1  there are another problems  (well-known problems)  If aurora start from cold  it takes very long time(about 30seconds)  And if it is scaling  it also takes some delay  Anyway  i cant make good pricing design for Aurora    So I moved on Dynamodb  It has very good pricing rules  In on-demand mode  under $2 per 1 000 000 rcu  I guess most of site data is stored in S3  So read/write other data is not much than 1kb/4kb  and even i transaction for each unit  under $2 per 500 000rcu    Actual Problem But it has trap point  If I do scan  its pricing is going to very higher  because returns a lot of data (until 1mb)  I accepted  So I must using query  But query is working same as scan in a partition  And pricing is going to scanned amount of read-write-units(until 1mb)  So if I have 100 items in a key and query just one item (total 100 members and just select one of that members)  the dynamodb charges 100 times pricing because it scanned 100 items  (assume each item lower than 1kb)  I cant believe it  (x100 pricing) so googled it  I found many people are design dynamodb seperated hash-key  (like members-startfrom-a  members-startfrom-b      ) Question Is it normal  In this time  I think dynamodb cannot support normal website (bbs or press  or blog)  because they are rdb optimized structures  Even paging articles needs full-scan(or partitioned full-scan)   Is it right  or there is a migration rules for rdb user to dynamodb user  Someone can show me case-study  Most case people use dynamodb as subdb    
45944285,1,,,2017-08-29 16:40:18,,2,76,"<p>I am looking for a place to buy/sell AWS lambda/azure functions/google functions scripts. Does anyone know if such a thing exists? Would anyone else be willing to pay for this?</p>
",6741209.0,,,,,2017-08-29 16:40:18,Serverless computing marketplace?,<serverless-framework>,0,1,1.0,,,CC BY-SA 3.0,I am looking for a place to buy/sell AWS lambda/azure functions/google functions scripts  Does anyone know if such a thing exists  Would anyone else be willing to pay for this  
63146466,1,63154683.0,,2020-07-29 04:41:54,,0,381,"<p>I'm working on a JAVA project which uploads files to AWS S3 bucket. Now I need to process those files in S3 (validate and send data to database) everyday at 8.00 a.m. I'm planning to use AWS scheduler for this. But I'm confuse what's the scheduler I have to use and how to use. I went through documentation and found about <strong>AWS Batch</strong> and <strong>AWS cloud watch scheduler through Lambda</strong>. But I have no idea about what's the best way to use AWS scheduler in this scenario. Not sure weather AWS Batch works for this. Actually I need to consider the cost as well.
I'm glad if you could suggest me the best way to resolve this. Alternative methods are also welcome.</p>
<p><strong>P.S:</strong> File process will take more than 15 mins. And also I need to config several other schedulers as well.</p>
",6924604.0,,6924604.0,,2020-07-29 04:52:19,2020-07-29 13:26:48,What is the best way to use AWS Scheduler considering cost and performance,<java><amazon-web-services><aws-lambda><aws-batch>,2,3,,,,CC BY-SA 4.0,Im working on a JAVA project which uploads files to AWS S3 bucket  Now I need to process those files in S3 (validate and send data to database) everyday at 8 00 a m  Im planning to use AWS scheduler for this  But Im confuse whats the scheduler I have to use and how to use  I went through documentation and found about AWS Batch and AWS cloud watch scheduler through Lambda  But I have no idea about whats the best way to use AWS scheduler in this scenario  Not sure weather AWS Batch works for this  Actually I need to consider the cost as well  Im glad if you could suggest me the best way to resolve this  Alternative methods are also welcome  P S: File process will take more than 15 mins  And also I need to config several other schedulers as well  
61914929,1,61915294.0,,2020-05-20 13:54:17,,2,243,"<p>I'm well aware of the lambda function deployment package size limit is 50 MB(in case of compressed .zip/.jar) with a direct upload and 250 MB limit (uncompressed) via upload from S3.</p>

<p>What I'm not clear is how lambda deploys the package from S3?</p>

<ol>
<li>Is it on the each invocation of the lambda function?</li>
<li>Will there be any cost associated data transfer between S3 to lambda function?</li>
</ol>
",7630979.0,,,,,2020-05-20 14:10:31,AWS lambda deployment package from S3,<amazon-web-services><amazon-s3><aws-lambda><awsdeploy>,1,0,,,,CC BY-SA 4.0,Im well aware of the lambda function deployment package size limit is 50 MB(in case of compressed  zip/ jar) with a direct upload and 250 MB limit (uncompressed) via upload from S3  What Im not clear is how lambda deploys the package from S3   Is it on the each invocation of the lambda function  Will there be any cost associated data transfer between S3 to lambda function   
61915861,1,61917203.0,,2020-05-20 14:37:54,,-2,58,"<p>I need to permanently store a string between runs of a lambda function.  The function will never overlap runs, it will pull the string at the beginning and update the stored version at the end of its run.</p>

<p>I want to use inline nodejs in an aws lambda function, so no free tier postgres.  I want to stay within the aws framework, so no http api access to free personal cloud storage.  I'm guessing a single s3 file is my best bet, even aurora serverless has lag time to spin back up, and will charge for storage.  Are there any other options for persisting this string between runs of a lambda function?  Some sort of aws caching mechanism, hopefully accessible with their aws-sdk?</p>
",433342.0,,,,,2020-05-20 15:41:50,Cheapest way to persist string for lambda job,<aws-lambda>,1,0,,2020-05-20 16:12:23,,CC BY-SA 4.0,I need to permanently store a string between runs of a lambda function   The function will never overlap runs  it will pull the string at the beginning and update the stored version at the end of its run  I want to use inline nodejs in an aws lambda function  so no free tier postgres   I want to stay within the aws framework  so no http api access to free personal cloud storage   Im guessing a single s3 file is my best bet  even aurora serverless has lag time to spin back up  and will charge for storage   Are there any other options for persisting this string between runs of a lambda function   Some sort of aws caching mechanism  hopefully accessible with their aws-sdk  
61916850,1,,,2020-05-20 15:24:13,,1,884,"<p><strong>Problem Description</strong>  </p>

<p>I created a simple REST API using AWS Lambda and I'm saving the data in AWS DynamoDB (all using Serverless framework). The latter is defined as <code>PAY_PER_REQUEST</code>. Currently, the API is not very much used, however when the app scales, it can induce significant additional costs.</p>

<p><strong>What I have tried</strong></p>

<p>I came to the conclusion that there are two options:</p>

<ol>
<li>either I cache the DB values</li>
<li>or I cache the API Gateway response</li>
</ol>

<p>Although paid, I have tried the AWS DynamoDB DAX but it's a paid feature and I'm looking for an alternative (with code) to minimize requests to the Database when it comes to <code>GET</code> requests that are unlikely to change within 24 hours. </p>

<p>I also made some research and found out that it's possible to enable caching in API Gateway, but I'm not sure what's the best/cost-effective method of doing this. Maybe someone has a practical example that explains how to achieve this?</p>
",5069505.0,,,,,2020-05-20 18:55:27,How to enable Free Lambda/DynamoDB Caching,<javascript><amazon-dynamodb><aws-api-gateway><serverless>,1,0,,,,CC BY-SA 4.0,Problem Description  I created a simple REST API using AWS Lambda and Im saving the data in AWS DynamoDB (all using Serverless framework)  The latter is defined as   Currently  the API is not very much used  however when the app scales  it can induce significant additional costs  What I have tried I came to the conclusion that there are two options:  either I cache the DB values or I cache the API Gateway response  Although paid  I have tried the AWS DynamoDB DAX but its a paid feature and Im looking for an alternative (with code) to minimize requests to the Database when it comes to  requests that are unlikely to change within 24 hours   I also made some research and found out that its possible to enable caching in API Gateway  but Im not sure whats the best/cost-effective method of doing this  Maybe someone has a practical example that explains how to achieve this  
42820043,1,42820806.0,,2017-03-15 20:27:44,,0,37,"<p>I currently have a frontend-only app that fetches 5-6 different JSON feeds, grabs some necessary data from each of them, and then renders a page based on said data. I'd like to move the data fetching / processing part of the app to a server-side node application which outputs one simple JSON file which the frontend app can fetch and easily render.</p>

<p>There are two noteworthy complications for this project:</p>

<p>1) The new backend app will have to live on a different server than its frontend counterpart</p>

<p>2) Some of the feeds change fairly often, so I'll need the backend processing to constantly check for changes (every 5-10 seconds). Currently with the frontend-only app, the browser fetches the latest versions of the feeds on load. I'd like to replicate this behavior as closely as possible</p>

<p>My thought process for solving this took me in two directions:</p>

<p>The first is to setup an express application that uses setTimeout to constantly check for new data to process. This data is then sent as a response to a simple GET request:</p>

<pre><code>const express = require('express');
let app = express();
let processedData = {};
const getData = () =&gt; {...} // returns a promise that fetches and processes data


/* use an immediately invoked function with setTimeout to fetch the data 
 * when the program starts and then once every 5 seconds after that */
(function refreshData() {
  getData.then((data) =&gt; {
    processedData = data;
  });

  setTimeout(refreshData, 5000);
})();

app.get('/', (req, res) =&gt; {
  res.send(processedData);
});

app.listen(port, () =&gt; {
  console.log(`Started on port ${port}`);
});
</code></pre>

<p>I would then run a simple get request from the client (after properly adjusting CORS headers) to get the JSON object.</p>

<p>My questions about this approach are pretty generic: Is this even a good solution to this problem? Will this drive up hosting costs based on processing / client GET requests? Is setTimeout a good way to have a task run repeatedly on the server?</p>

<p>The other solution I'm considering would deal with setting up an AWS Lambda that writes the resulting JSON to an s3 bucket. It looks like the minimum interval for scheduling an AWS Lambda function is 1 minute, however. I imagine I could set up 3 or 4 identical Lambda functions and offset them by 10-15 seconds, however that seems so hacky that it makes me physically uncomfortable.</p>

<p>Any suggestions / pointers / solutions would be greatly appreciated. I am not yet a super experienced backend developer, so please ELI5 wherever you deem fit.</p>
",1179207.0,,1179207.0,,2017-03-16 01:15:32,2017-03-16 01:15:32,"Node app that fetches, processes, and formats data for consumption by a frontend app on another server",<node.js><amazon-web-services><express><amazon-s3><aws-lambda>,1,0,,,,CC BY-SA 3.0,I currently have a frontend-only app that fetches 5-6 different JSON feeds  grabs some necessary data from each of them  and then renders a page based on said data  Id like to move the data fetching / processing part of the app to a server-side node application which outputs one simple JSON file which the frontend app can fetch and easily render  There are two noteworthy complications for this project: 1) The new backend app will have to live on a different server than its frontend counterpart 2) Some of the feeds change fairly often  so Ill need the backend processing to constantly check for changes (every 5-10 seconds)  Currently with the frontend-only app  the browser fetches the latest versions of the feeds on load  Id like to replicate this behavior as closely as possible My thought process for solving this took me in two directions: The first is to setup an express application that uses setTimeout to constantly check for new data to process  This data is then sent as a response to a simple GET request:  I would then run a simple get request from the client (after properly adjusting CORS headers) to get the JSON object  My questions about this approach are pretty generic: Is this even a good solution to this problem  Will this drive up hosting costs based on processing / client GET requests  Is setTimeout a good way to have a task run repeatedly on the server  The other solution Im considering would deal with setting up an AWS Lambda that writes the resulting JSON to an s3 bucket  It looks like the minimum interval for scheduling an AWS Lambda function is 1 minute  however  I imagine I could set up 3 or 4 identical Lambda functions and offset them by 10-15 seconds  however that seems so hacky that it makes me physically uncomfortable  Any suggestions / pointers / solutions would be greatly appreciated  I am not yet a super experienced backend developer  so please ELI5 wherever you deem fit  
62283393,1,62283474.0,,2020-06-09 13:08:12,,1,51,"<p>I'm new to <strong>AWS Lambda</strong> and I recently started developing Lambda functions together with API Gateway and RDS as a simple backend-solution. </p>

<p>I've been searching for an IDE where I can edit the code of the functions. I've tried Cloud9 but I'm looking for an environment that's not browser-based and with an app for mac. IntelliJ seems like an option too but it's a bit expensive for my needs. </p>

<p>Any free / cheap alternatives for just editing the code of the Lambda-functions in an independent app for mac? </p>
",4936199.0,,,,,2020-06-09 13:13:09,AWS Lambda - editing functions outside of amazon.com,<amazon-web-services><intellij-idea><aws-lambda><aws-cloud9>,1,0,,,,CC BY-SA 4.0,Im new to AWS Lambda and I recently started developing Lambda functions together with API Gateway and RDS as a simple backend-solution   Ive been searching for an IDE where I can edit the code of the functions  Ive tried Cloud9 but Im looking for an environment thats not browser-based and with an app for mac  IntelliJ seems like an option too but its a bit expensive for my needs   Any free / cheap alternatives for just editing the code of the Lambda-functions in an independent app for mac   
42853269,1,,,2017-03-17 09:08:06,,1,429,"<p>We're integrating Amazon's Alexa to work with our application. We have created a dictionary of items that Alexa might be asked, in DynamoDB. Now we need an algorithm to match the text from Alexa to the strings stored in the DynamoDB table which would be phonetically similar, but possibly differently spelled or with special characters in between, e.g.</p>
<blockquote>
<p>&quot;X-Men&quot; may be requested as &quot;xmen&quot; or &quot;ex men&quot; or &quot;x men&quot;</p>
<p>&quot;Claire&quot; may be requested as &quot;clare&quot; or &quot;clair&quot;</p>
</blockquote>
<p>I find the <a href=""https://aws.amazon.com/blogs/aws/new-logstash-plugin-search-dynamodb-content-using-elasticsearch/"" rel=""nofollow noreferrer"">Amazon DynamoDB-ElasticSearch Integration</a> a plausible option, but i haven't learnt enough about it yet. This could be pretty expensive too.</p>
<p>I also tried to find out if there are node modules that would help find similar strings that I could match against the database.</p>
<p>The <a href=""https://www.npmjs.com/package/fuzzy"" rel=""nofollow noreferrer"">fuzzy search node module</a> would probably not work for us either especially because we have the constraint of finding a single match and not a list of possible match for a single string input.</p>
<p>Eg. A search for <code>&quot;Xmen&quot;</code> should return only <code>&quot;X-Men&quot;</code> and not <code>&quot;X-Men&quot;</code> and <code>&quot;Ex-servicemen&quot;</code></p>
<p>My last resort would be to write an <strong>Approximate String Matching Algorithm</strong> along with <strong>Levenshtein distance computing algorithm</strong> from scratch.</p>
",5391902.0,,-1.0,,2020-06-20 09:12:55,2017-03-25 18:54:46,Map phonetic string to possible alternate strings to match against dynamoDB,<javascript><node.js><amazon-web-services><amazon-dynamodb><aws-lambda>,1,0,1.0,,,CC BY-SA 3.0,Were integrating Amazons Alexa to work with our application  We have created a dictionary of items that Alexa might be asked  in DynamoDB  Now we need an algorithm to match the text from Alexa to the strings stored in the DynamoDB table which would be phonetically similar  but possibly differently spelled or with special characters in between  e g   X-Men may be requested as xmen or ex men or x men Claire may be requested as clare or clair  I find the  a plausible option  but i havent learnt enough about it yet  This could be pretty expensive too  I also tried to find out if there are node modules that would help find similar strings that I could match against the database  The  would probably not work for us either especially because we have the constraint of finding a single match and not a list of possible match for a single string input  Eg  A search for  should return only  and not  and  My last resort would be to write an Approximate String Matching Algorithm along with Levenshtein distance computing algorithm from scratch  
59975071,1,59992636.0,,2020-01-29 20:28:18,,2,862,"<p>I have attached an Appsync pipeline resolver to a field called <code>paymentStatus</code> in my <code>Organisation</code> object. The idea is that if an organisations last pay day has passed, I want to fetch the payment status from an external API using a Lambda function. If the pay day has not passed, I do not want to invoke the function but simply return a ""OK"".</p>

<p>Is there any way I can conditionally invoke a Lambda function? Something like this:</p>

<pre><code>#if ($ctx.source.payday &lt; $util.time.nowEpochSeconds()) 
    {
        ""version"": ""2017-02-28"",
        ""operation"": ""Invoke"",
        ""payload"": {
            ""arguments"": {
                ""orgID"": ""$ctx.source.id""
            }
        }
    }
#end
</code></pre>

<p>If I run this, Appsync complains about <code>operation</code> attribute missing when the condition is not met. I have also noted that the <code>condition</code> attribute which exists for queries is not available for Lambda datasources.</p>

<p>Thank you in advance &lt;3</p>
",2114052.0,,,,,2020-01-30 18:50:07,Can I conditionally invoke a lambda function in an AWS Appsync resolver?,<amazon-web-services><aws-lambda><aws-appsync>,1,0,,,,CC BY-SA 4.0,I have attached an Appsync pipeline resolver to a field called  in my  object  The idea is that if an organisations last pay day has passed  I want to fetch the payment status from an external API using a Lambda function  If the pay day has not passed  I do not want to invoke the function but simply return a OK  Is there any way I can conditionally invoke a Lambda function  Something like this:  If I run this  Appsync complains about  attribute missing when the condition is not met  I have also noted that the  attribute which exists for queries is not available for Lambda datasources  Thank you in advance &lt;3 
63032033,1,,,2020-07-22 10:21:36,,0,607,"<p>I have <strong>Next.js</strong> web application hosted on <strong>Vercel</strong> with its database on <strong>Mongo Cloud Atlas</strong>. My goal is to assess cost, speed and convenience</p>
<p><strong>Next.js</strong> can serve static files by placing the files in the same folder as the website itslef. That being said, is it faster to serve images from next or Mongo?</p>
<p>Also, from a cost perspective, mongo is not free.</p>
",3641128.0,,,,,2020-07-22 11:05:07,Is it better to serve images from Next.js or MongoDB?,<mongodb><image><next.js><cdn><vercel>,1,3,,,,CC BY-SA 4.0,I have Next js web application hosted on Vercel with its database on Mongo Cloud Atlas  My goal is to assess cost  speed and convenience Next js can serve static files by placing the files in the same folder as the website itslef  That being said  is it faster to serve images from next or Mongo  Also  from a cost perspective  mongo is not free  
42878938,1,,,2017-03-18 19:35:09,,0,419,"<p>I have an AWS lambda script that produces a json file, 
and a django app on an AWS EC2 instance.</p>

<p>I want to make the json available to the django app.</p>

<p>The django app has a default sqlite3 db, how can I get lambda to post to EC2? </p>

<p>I am using the code below to post the results to a AWS RDS instance, but this might be costly for my needs and I'd be better off updating a db in the django app, of possible.</p>

<pre><code>######################
# Save to RDS database
######################
try:
    created_at = time.strftime('%Y-%m-%d %H:%M:%S')
    with conn.cursor() as cur:
        cur.execute('INSERT INTO entries (created_at, useremail, datajson) values(%s, %s, %s, %s, %s)',
                    (created_at, user, datastr))
        conn.commit()
    # logging.info(""Added to RDS MySQL table"")
except:
    print('Could not save to RDS')
    # logging.info(""Could not save to RDS"")
</code></pre>

<p>(The lambda script is in python2)</p>
",4195659.0,,2593745.0,,2017-03-20 19:16:25,2017-03-20 19:16:25,Can amazon lambda post to sqlite database of a django app hosted on EC2?,<django><amazon-web-services><amazon-ec2><sqlite><aws-lambda>,0,4,,,,CC BY-SA 3.0,I have an AWS lambda script that produces a json file   and a django app on an AWS EC2 instance  I want to make the json available to the django app  The django app has a default sqlite3 db  how can I get lambda to post to EC2   I am using the code below to post the results to a AWS RDS instance  but this might be costly for my needs and Id be better off updating a db in the django app  of possible   (The lambda script is in python2) 
63034154,1,,,2020-07-22 12:25:22,,0,198,"<p>Is there a way to buffer X log messages from a CloudWatch log group and only then stream it to a lambda function? I'll elaborate:</p>
<p>I have an app that I registered it's <em>CloudWatch</em> logs to stream to a <em>lambda</em> function which formats the logs and pushes them to <em>Elastic Search</em>.</p>
<p>So the flow is the following:
(app logs) -&gt; (CloudWatch) --&gt;(Lambda)--&gt;(Elastic Search)</p>
<p>My problem is that my lambda function is invoked very often (most of the time single log message) and bombards ES with write requests, I would like to write the logs in bulks, i.e wait until 30 new logs and then invoke the lambda for the 30 logs bulk.</p>
<p>The only way I found to achieve this is to use Kinesis and Firehose but those services cost extra and I want to avoid this.</p>
<p>Are there any other alternatives to achieve this without using something like LogStash?
I am assuming this is a very common usage so there must be some easy way to solve this.</p>
<p>Thanks,</p>
",13476336.0,,,,,2020-08-19 16:16:31,AWS buffer logs in CloudWatch before streaming to lambda,<amazon-web-services><elasticsearch><logging><aws-lambda><architecture>,2,0,,,,CC BY-SA 4.0,Is there a way to buffer X log messages from a CloudWatch log group and only then stream it to a lambda function  Ill elaborate: I have an app that I registered its CloudWatch logs to stream to a lambda function which formats the logs and pushes them to Elastic Search  So the flow is the following: (app logs) -&gt; (CloudWatch) --&gt;(Lambda)--&gt;(Elastic Search) My problem is that my lambda function is invoked very often (most of the time single log message) and bombards ES with write requests  I would like to write the logs in bulks  i e wait until 30 new logs and then invoke the lambda for the 30 logs bulk  The only way I found to achieve this is to use Kinesis and Firehose but those services cost extra and I want to avoid this  Are there any other alternatives to achieve this without using something like LogStash  I am assuming this is a very common usage so there must be some easy way to solve this  Thanks  
42877521,1,46704752.0,,2017-03-18 17:25:30,,18,7360,"<p>There are a few pieces of my app that cannot afford the additional 1-2 second delay caused by the ""freeze-thaw"" cycle that Lambda functions go through when they're new or unused for some period of time.</p>

<p>How can I keep these Lambda functions warm so AWS doesn't have to re-provision them all the time? This goes for both 1) infrequently-used functions and 2) recently-deployed functions.</p>

<p>Ideally, there is a setting that I missed called ""keep warm"" that increases the cost of the Lambda functions, but always keeps a function warm and ready to respond, but I'm pretty sure this does not exist.</p>

<p>I suppose an option is to use a CloudWatch timer to ping the functions every so often... but this feels wrong to me. Also, I don't know the interval that AWS uses to put Lambda functions on ice.</p>
",1766789.0,,,,,2021-08-17 15:53:13,Is it possible to keep an AWS Lambda function warm?,<amazon-web-services><aws-lambda><serverless-framework>,2,10,6.0,,,CC BY-SA 3.0,There are a few pieces of my app that cannot afford the additional 1-2 second delay caused by the freeze-thaw cycle that Lambda functions go through when theyre new or unused for some period of time  How can I keep these Lambda functions warm so AWS doesnt have to re-provision them all the time  This goes for both 1) infrequently-used functions and 2) recently-deployed functions  Ideally  there is a setting that I missed called keep warm that increases the cost of the Lambda functions  but always keeps a function warm and ready to respond  but Im pretty sure this does not exist  I suppose an option is to use a CloudWatch timer to ping the functions every so often    but this feels wrong to me  Also  I dont know the interval that AWS uses to put Lambda functions on ice  
44687654,1,44707555.0,,2017-06-21 23:16:13,,5,2667,"<p>I am creating a publicly available API using API Gateway which is backed with lambda functions to do some processing. I have secured it with a custom security header that implements hmac authentication with timestamp to protect against replay attacks. 
I understand that API Gateway protects against DDOS attacks through its high availability, but any invalid requests will still be passed to the lambda authentication function. So, I guess an attacker can submit invalid unauthenticated requests resulting in high costs. It will take a considerable number of requests to cause damage but it is still very doable. What is the best way to protect against that ? 
Thank you</p>
",7400346.0,,,,,2017-06-22 19:10:24,Secure AWS API Gateway with Lambda Integration,<amazon-web-services><security><aws-lambda><aws-api-gateway>,2,1,5.0,,,CC BY-SA 3.0,I am creating a publicly available API using API Gateway which is backed with lambda functions to do some processing  I have secured it with a custom security header that implements hmac authentication with timestamp to protect against replay attacks   I understand that API Gateway protects against DDOS attacks through its high availability  but any invalid requests will still be passed to the lambda authentication function  So  I guess an attacker can submit invalid unauthenticated requests resulting in high costs  It will take a considerable number of requests to cause damage but it is still very doable  What is the best way to protect against that    Thank you 
62298207,1,62299968.0,,2020-06-10 07:41:07,,1,597,"<p>I am working on a requirement, where i am doing multipart upload of the csv file from on prem server to S3 Bucket.</p>

<p>To achieve this using AWS Lambda I create a presigned url and use this url i am uploading the csv file. Now, once i have the file in AWS S3, i want it to be moved to AWS RDS Oracle DB. Initially i was planning to use AWS Lambda for this. </p>

<p>So once i have the file in S3, it triggers lambda(s3 event) and lambda will push this file to RDS. But with this the issue is with the file Size(600 MB).</p>

<p>I am looking for some other way, where whenever there is a file uploaded to S3, it should trigger any AWS service and that service will push this csv file to RDS. I have gone through AWS DMS/Data Pipeline, but not able to find any way to automate this migration</p>

<p>I need to automate this migration on every s3 upload, that is also cost effective.</p>
",10721455.0,,10721455.0,,2020-06-10 07:53:18,2020-06-10 12:04:26,Data migration from S3 to RDS,<amazon-web-services><amazon-s3><aws-lambda><aws-dms><aws-data-pipeline>,1,4,,,,CC BY-SA 4.0,I am working on a requirement  where i am doing multipart upload of the csv file from on prem server to S3 Bucket  To achieve this using AWS Lambda I create a presigned url and use this url i am uploading the csv file  Now  once i have the file in AWS S3  i want it to be moved to AWS RDS Oracle DB  Initially i was planning to use AWS Lambda for this   So once i have the file in S3  it triggers lambda(s3 event) and lambda will push this file to RDS  But with this the issue is with the file Size(600 MB)  I am looking for some other way  where whenever there is a file uploaded to S3  it should trigger any AWS service and that service will push this csv file to RDS  I have gone through AWS DMS/Data Pipeline  but not able to find any way to automate this migration I need to automate this migration on every s3 upload  that is also cost effective  
46104895,1,46105972.0,,2017-09-07 20:50:38,,1,874,"<p>I have to write code in react-native that allows a user to upload videos to amazon s3 to be transcoded for consumption by various devices.  For the processing after the upload occurs; I am reviewing two approaches:</p>

<p>1) I can use Lambda with ffmpeg to handle the transcoding immediately after the uploading occurs (my fear here would be the amount of time required to transcode the videos and the effect on pricing if it takes a considerable amount of time).  </p>

<p>2) I can have s3 pass an sns message to a rest api after the created event occurs and the rest api generate a rabbitmq message that will be processed by worker that will perform the transcoding using ffmpeg.</p>

<p>Option 1) seems to be the preferable option based on a completion time perspective.  How concerned should I be with using 1) considering how long video transcoding might take as opposed to option 2)?  </p>

<p>Also, regardless, I need a way to pass additional parameters to lambda or along the sns messaging that would allow me to somehow associate the user who uploaded the video with their account.  Is there a way to pass additional text-based values to s3 to pass along to lambda or along sns when the upload completes, as a caveat I plan to upload the video directly to s3 using the rest layer(found this answer here: <a href=""http://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUT.html#RESTObjectPUT-responses-examples"" rel=""nofollow noreferrer"">http://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUT.html#RESTObjectPUT-responses-examples</a>)?</p>
",1790300.0,,13070.0,,2017-09-07 22:22:16,2017-09-07 22:26:01,passing additional values to s3 event notification for lambda consumption,<amazon-web-services><amazon-s3><ffmpeg><aws-lambda><transcoding>,1,0,,,,CC BY-SA 3.0,I have to write code in react-native that allows a user to upload videos to amazon s3 to be transcoded for consumption by various devices   For the processing after the upload occurs; I am reviewing two approaches: 1) I can use Lambda with ffmpeg to handle the transcoding immediately after the uploading occurs (my fear here would be the amount of time required to transcode the videos and the effect on pricing if it takes a considerable amount of time)    2) I can have s3 pass an sns message to a rest api after the created event occurs and the rest api generate a rabbitmq message that will be processed by worker that will perform the transcoding using ffmpeg  Option 1) seems to be the preferable option based on a completion time perspective   How concerned should I be with using 1) considering how long video transcoding might take as opposed to option 2)    Also  regardless  I need a way to pass additional parameters to lambda or along the sns messaging that would allow me to somehow associate the user who uploaded the video with their account   Is there a way to pass additional text-based values to s3 to pass along to lambda or along sns when the upload completes  as a caveat I plan to upload the video directly to s3 using the rest layer(found this answer here: )  
60361736,1,,,2020-02-23 11:31:39,,3,2406,"<p>I have a lot of dotnet lambda microservices using SSM parameter store for configuration purposes. It's been quite adventageous over environment variables as I'm sharing a lot of configuration across different microservices. Though recently I've started pushing the limits of it. It now affects my throughput and started costing more than I'd like.</p>

<p>I've considered using the amazon extension for dotnet configuration manager, but it falls short for my requirements. I need the configuration to hot swap to keep the microservices running healthy at high uptime. Which won't happen with its current implementation. Deploying all microservices just for a configuration change is not an option either.</p>

<p>This lead me to research a cache solution that is able to at least invalidate the cache from outside, but I couldn't come accross anything that works with SSM parameter store out of box.</p>

<p>At worst, I'll need to come up with another microservice with it's own db that takes care of the configuration, but I don't wanna go down that path tbh.</p>

<p>What is the general approach that is being used this kind of scenarios?</p>
",1469980.0,,,,,2022-01-17 01:23:24,Caching solution for AWS SSM parameter store to be used with dotnet lambdas,<caching><.net-core><configuration><aws-lambda><aws-parameter-store>,3,2,,,,CC BY-SA 4.0,I have a lot of dotnet lambda microservices using SSM parameter store for configuration purposes  Its been quite adventageous over environment variables as Im sharing a lot of configuration across different microservices  Though recently Ive started pushing the limits of it  It now affects my throughput and started costing more than Id like  Ive considered using the amazon extension for dotnet configuration manager  but it falls short for my requirements  I need the configuration to hot swap to keep the microservices running healthy at high uptime  Which wont happen with its current implementation  Deploying all microservices just for a configuration change is not an option either  This lead me to research a cache solution that is able to at least invalidate the cache from outside  but I couldnt come accross anything that works with SSM parameter store out of box  At worst  Ill need to come up with another microservice with its own db that takes care of the configuration  but I dont wanna go down that path tbh  What is the general approach that is being used this kind of scenarios  
46154682,1,46276476.0,,2017-09-11 11:30:53,,0,88,"<p>I am looking at serverless architecture to process some customer data. The process itself is probably quite quick, but for various reasons I would like the cloud service provider to gurantee executional isolation. So far, I've talked to a rep from Amazon, who said that Amazon Lambda are not effectively isolated, and the <em>lambda container</em> may end up being reused.</p>

<p>Effectively, when running a function and, say, writing something to memory or disk (here we might not have control, as part of the solution would let customers execute arbitrary code) I would like a sandbox isolation gurantee.</p>

<p>I've read that Microsoft was going to offer such isolation, but apart from a <a href=""https://thenewstack.io/azure-functions-serverless-computing-handling-iot-devices/"" rel=""nofollow noreferrer"">news story</a>, I couldn't find and concrete information. There they alude to extra costs of sandboxing functions for example. </p>

<p><strong>So is there any provider that could gurantee executional isolation?</strong></p>
",427673.0,,,,,2017-09-18 09:57:03,Serverless Execution Isolation,<serverless-architecture>,1,0,,,,CC BY-SA 3.0,I am looking at serverless architecture to process some customer data  The process itself is probably quite quick  but for various reasons I would like the cloud service provider to gurantee executional isolation  So far  Ive talked to a rep from Amazon  who said that Amazon Lambda are not effectively isolated  and the lambda container may end up being reused  Effectively  when running a function and  say  writing something to memory or disk (here we might not have control  as part of the solution would let customers execute arbitrary code) I would like a sandbox isolation gurantee  Ive read that Microsoft was going to offer such isolation  but apart from a   I couldnt find and concrete information  There they alude to extra costs of sandboxing functions for example   So is there any provider that could gurantee executional isolation  
46168826,1,,,2017-09-12 05:51:39,,1,931,"<p>I am using the serverless framework with nodejs(Version 4.4) to create AWS lambda functions. The default timeout is 6 seconds for lambda execution. I am connecting to mysql database using sequelize ORM. I see errors like <em>execution timed out</em>. Sometimes my code works properly even with this error. But sometimes nothing works after this timeout error. Its really hard for me make sense out of this timeout.  I am afraid increasing the timeout will incur more charge.</p>
",6370456.0,,1022141.0,,2017-09-12 10:25:35,2017-09-12 10:48:40,AWS Lambda time out in 6 seconds,<amazon-web-services><aws-lambda><nodes><sequelize.js><serverless-framework>,1,4,,,,CC BY-SA 3.0,I am using the serverless framework with nodejs(Version 4 4) to create AWS lambda functions  The default timeout is 6 seconds for lambda execution  I am connecting to mysql database using sequelize ORM  I see errors like execution timed out  Sometimes my code works properly even with this error  But sometimes nothing works after this timeout error  Its really hard for me make sense out of this timeout   I am afraid increasing the timeout will incur more charge  
62585130,1,,,2020-06-25 22:13:08,,0,207,"<p>I am creating a CI/CD pipeline using AWS codepipeline to deploy several lambda functions. Currently I am manually uploading .zip files for the lambdas functions which include a configuration.json file that has credentials to access the RDS database.</p>
<p>I have already created a SAM template to deploy the lambda functions via codepipeline, however, I am unable to think of a solution to provide RDS database credentials to the lambda functions since commiting the configuration.json file in the code repository is not an option.</p>
<p>AWS secrets manager is NOT an option for me as it would be very costly due to millions of API calls hitting the lambda functions.</p>
",13815587.0,,,,,2020-06-25 23:20:37,What is the best way to securely provide AWS RDS credentials to AWS Lambda functions?,<mysql><database><amazon-web-services><aws-lambda><amazon-rds>,1,6,0.0,,,CC BY-SA 4.0,I am creating a CI/CD pipeline using AWS codepipeline to deploy several lambda functions  Currently I am manually uploading  zip files for the lambdas functions which include a configuration json file that has credentials to access the RDS database  I have already created a SAM template to deploy the lambda functions via codepipeline  however  I am unable to think of a solution to provide RDS database credentials to the lambda functions since commiting the configuration json file in the code repository is not an option  AWS secrets manager is NOT an option for me as it would be very costly due to millions of API calls hitting the lambda functions  
60424500,1,,,2020-02-27 00:15:38,,0,602,"<p>I'm working on ingesting metrics from Lambda into our centralized logging system. Our first idea is too costly so I'm trying to figure out if there is a way to lower the cost (instead of ingesting 3 metrics from 200 lambdas every 60s).</p>

<p>I've been messing around with MetricMath and have pretty much figured out what I want to do.  I'd run this as a k8s cron job like thing and variabilize the start and end time.</p>

<p>How would this be charged? Is it the number of metrics used to perform the math or the number of values that I output?</p>

<p>i.e. m1 and m2 are pulling Errors and Invocations from 200 lambdas. To pull each of these individually would be 400 metrics.</p>

<p>In this method, would it only be 1, 3, or 401?</p>

<pre><code>{
        ""MetricDataQueries"": [
    {
        ""Id"": ""m1"",
        ""MetricStat"": {
                ""Metric"": {
                        ""Namespace"": ""AWS/Lambda"",
                        ""MetricName"": ""Errors""
                },
                ""Period"": 300,
                ""Stat"": ""Sum"",
                ""Unit"": ""Count""
        },
        ""ReturnData"": false
        },
        {
        ""Id"": ""m2"",
        ""MetricStat"": {
                ""Metric"": {
                        ""Namespace"": ""AWS/Lambda"",
                        ""MetricName"": ""Invocations""
                },
                ""Period"": 300,
                ""Stat"": ""Sum"",
                ""Unit"": ""Count""
        },
        ""ReturnData"": false
        },
        {
        ""Id"": ""e1"",
        ""Expression"": ""m1 / m2"",
        ""Label"": ""ErrorRate""
    }
    ],
    ""StartTime"": ""2020-02-25T02:00:0000"",
    ""EndTime"": ""2020-02-26T02:05:0000""
}
</code></pre>

<p>Output:</p>

<pre><code>{
    ""Messages"": [],
    ""MetricDataResults"": [
        {
            ""Label"": ""ErrorRate"",
            ""StatusCode"": ""Complete"",
            ""Values"": [
                0.0045127626568890146
            ],
            ""Id"": ""e1"",
            ""Timestamps"": [
                ""2020-02-26T19:00:00Z""
            ]
        }
    ]
}
</code></pre>

<p>Example 2:</p>

<p>Same principle. This is pulling the invocations by of each function by FunctionName. It then sorts them and outputs the most invoked. Any idea how many metrics this would be?</p>

<pre><code>{
    ""MetricDataQueries"": [
    {
        ""Id"": ""e2"",
        ""Expression"": ""SEARCH(' {AWS/Lambda,FunctionName} MetricName=`Invocations` ', 'Sum', 60)"",
        ""ReturnData"" : false
    },
    {
        ""Id"": ""e3"",
        ""Expression"": ""SORT(e2, SUM, DESC, 1)""
    }
],
""StartTime"": ""2020-02-26T12:00:0000"",
""EndTime"": ""2020-02-26T12:01:0000""

}
</code></pre>

<p>Same question. 1 or 201 metrics?</p>

<p>Output:</p>

<pre><code>{
    ""MetricDataResults"": [
        {
            ""Id"": ""e3"",
            ""Timestamps"": [
                ""2020-02-26T12:00:00Z""
            ],
            ""Label"": ""1 - FunctionName"",
            ""Values"": [
                91.0
            ],
            ""StatusCode"": ""Complete""
        }
    ],
    ""Messages"": []
}
</code></pre>
",11376062.0,,,,,2020-02-27 19:28:20,Cost of GetMetricData Api calls when using metric math?,<amazon-web-services><aws-lambda><aws-cli><amazon-cloudwatch><metrics>,1,0,,,,CC BY-SA 4.0,Im working on ingesting metrics from Lambda into our centralized logging system  Our first idea is too costly so Im trying to figure out if there is a way to lower the cost (instead of ingesting 3 metrics from 200 lambdas every 60s)  Ive been messing around with MetricMath and have pretty much figured out what I want to do   Id run this as a k8s cron job like thing and variabilize the start and end time  How would this be charged  Is it the number of metrics used to perform the math or the number of values that I output  i e  m1 and m2 are pulling Errors and Invocations from 200 lambdas  To pull each of these individually would be 400 metrics  In this method  would it only be 1  3  or 401   Output:  Example 2: Same principle  This is pulling the invocations by of each function by FunctionName  It then sorts them and outputs the most invoked  Any idea how many metrics this would be   Same question  1 or 201 metrics  Output:  
62593375,1,,,2020-06-26 11:02:51,,0,210,"<p>One of my customers needs us to copy files from their SFTP to our S3. They copy files into SFTP every 15 minutes, so we have files there added incrementally and then they delete files on SFTP which are older than 3 days.
So imagine we have 3000 or more files in our S3 (we need to keep them all in a S3 bucket) and then every day we have around 100 new files that need to be copied from SFTP to S3 ( all the new files which are added into SFTP).</p>
<p>I have a lambda function which runs every hour to handle it, but what is the most optimised way to do it:</p>
<ol>
<li>Reading all SFTP file names (using ssh2-sftp-client library) and
check which ones are not copied in S3 (using s3.headObject to check
if file exists in S3) and then copy them. A bit expensive to iterate
through all SFTP file name and see if they exists in S3, more than 1
minute.</li>
<li>Reading all S3 bucket file names, reading all SFTP file names and then compare which ones are not included and then copy those. (not sure if it's reasonable performance wise)</li>
<li>or any other solutions</li>
</ol>
<p>any better ideas?</p>
",3426603.0,,174777.0,,2020-06-27 02:03:59,2020-06-27 02:03:59,Copy files from SFTP to S3 (incremental upload),<amazon-s3><aws-lambda><sftp>,0,3,,,,CC BY-SA 4.0,One of my customers needs us to copy files from their SFTP to our S3  They copy files into SFTP every 15 minutes  so we have files there added incrementally and then they delete files on SFTP which are older than 3 days  So imagine we have 3000 or more files in our S3 (we need to keep them all in a S3 bucket) and then every day we have around 100 new files that need to be copied from SFTP to S3 ( all the new files which are added into SFTP)  I have a lambda function which runs every hour to handle it  but what is the most optimised way to do it:  Reading all SFTP file names (using ssh2-sftp-client library) and check which ones are not copied in S3 (using s3 headObject to check if file exists in S3) and then copy them  A bit expensive to iterate through all SFTP file name and see if they exists in S3  more than 1 minute  Reading all S3 bucket file names  reading all SFTP file names and then compare which ones are not included and then copy those  (not sure if its reasonable performance wise) or any other solutions  any better ideas  
63054799,1,,,2020-07-23 12:54:26,,6,2909,"<p>Looking for a way to delay a message in a being sent to a lambda by 5 seconds.</p>
<p>So, message 1 received by lambda then 5 seconds later message 2, then 5 seconds later message 3, etc, for say a thousand messages.</p>
<p>Was looking at SQS delay queue and message timers but they're not exactly what I'm looking for.</p>
<p>Step Functions using wait, but that would be expensive at the scale I need.</p>
<p>Ideally need an SQS queue that restricts messages to only being sent every 5 seconds, is there any way to do this?</p>
<p>p.s. not fussed about it being SQS, just need a solution</p>
",10525858.0,,,,,2022-01-01 17:09:47,"AWS SQS to delay message by X seconds, then next message by X seconds again",<amazon-web-services><aws-lambda><amazon-sqs><amazon-sns>,3,5,1.0,,,CC BY-SA 4.0,Looking for a way to delay a message in a being sent to a lambda by 5 seconds  So  message 1 received by lambda then 5 seconds later message 2  then 5 seconds later message 3  etc  for say a thousand messages  Was looking at SQS delay queue and message timers but theyre not exactly what Im looking for  Step Functions using wait  but that would be expensive at the scale I need  Ideally need an SQS queue that restricts messages to only being sent every 5 seconds  is there any way to do this  p s  not fussed about it being SQS  just need a solution 
46375132,1,,,2017-09-23 01:28:42,,0,423,"<p>I want to create Amazon DirectConnect connection as VPN to transfer data from my company network to RDS instance. The connection is not cheap and I certainly don't need it open the whole time. Probably just 10 minutes every day is enough. Is there a way to schedule connect/disconnect event of the VPN connection using Lambda function just like starting/stopping EC2 or RDS instance like below using Python's Boto3 library:</p>

<pre><code>def handler(event, context):

    ec2 = boto3.client('ec2', region_name=region)

    ec2.start_instances(InstanceIds=instances)
</code></pre>

<p>Although Boto3 supports <a href=""http://boto3.readthedocs.io/en/latest/reference/services/directconnect.html"" rel=""nofollow noreferrer"">DirectConnect</a>, it doesn't seem to have any methods that switch the connection on and off. Is there a way to control the connection?</p>
",3358927.0,,2593745.0,,2017-09-25 11:44:42,2018-01-16 13:38:11,How to disconnect Amazon VPN using Lambda,<amazon-web-services><aws-lambda><vpn><boto3><aws-direct-connect>,1,2,,,,CC BY-SA 3.0,I want to create Amazon DirectConnect connection as VPN to transfer data from my company network to RDS instance  The connection is not cheap and I certainly dont need it open the whole time  Probably just 10 minutes every day is enough  Is there a way to schedule connect/disconnect event of the VPN connection using Lambda function just like starting/stopping EC2 or RDS instance like below using Pythons Boto3 library:  Although Boto3 supports   it doesnt seem to have any methods that switch the connection on and off  Is there a way to control the connection  
63156643,1,63159975.0,,2020-07-29 15:04:53,,4,383,"<p>I'm developing an application with micronaut using SAM CLI to deploy it on AWS Lambda. As I was including dependencies and developing new features, the function packages got bigger an bigger (now they are around 250MB). This makes deployment take a while.</p>
<p>On top of that every time I edit <code>template.yaml</code> and then run <code>sam build &amp;&amp; sam deploy</code> to try a new configuration on S3, RDS, etc... I have to wait for gradle to build the function again (even though it's unchanged since the last deployment) and upload the whole package to S3.</p>
<p>As I'm trying to configure this application with many trials and errors on SAM, waiting for this process to complete just to get an error because of some misconfiguration is getting quite counterproductive.</p>
<p>Also my SAM s3 bcuket is at 10GB size after just a single day of work. This may get expensive on the long run.</p>
<p>Is there a way to avoid those gradle rebuilds and reuploads when teh function code is unchanged?</p>
",3926428.0,,3926428.0,,2020-07-29 17:36:35,2020-07-29 18:38:05,How to avoid AWS SAM rebuild and reupload a gradle function with unchanged code?,<gradle><aws-lambda><micronaut><aws-sam>,1,0,,,,CC BY-SA 4.0,Im developing an application with micronaut using SAM CLI to deploy it on AWS Lambda  As I was including dependencies and developing new features  the function packages got bigger an bigger (now they are around 250MB)  This makes deployment take a while  On top of that every time I edit  and then run  to try a new configuration on S3  RDS  etc    I have to wait for gradle to build the function again (even though its unchanged since the last deployment) and upload the whole package to S3  As Im trying to configure this application with many trials and errors on SAM  waiting for this process to complete just to get an error because of some misconfiguration is getting quite counterproductive  Also my SAM s3 bcuket is at 10GB size after just a single day of work  This may get expensive on the long run  Is there a way to avoid those gradle rebuilds and reuploads when teh function code is unchanged  
63166951,1,63167001.0,,2020-07-30 05:52:20,,11,3597,"<p>Can we stop an AWS windows server EC2 instance of a development environment when there is no activity in it, say after 2 hours of inactivity? I am having trouble identifying whether any user is connected to the server virtually.</p>
<p>I can easily start/stop the EC2 at a fixed time, programmatically, but in order to cut the cost of my server, I am trying to stop the EC2 when it is not being used.</p>
<p><strong>My intent(or use case) is</strong>: If no user is using the EC2 till a specified amount of time, it will automatically stop. Developers can restart it as and when needed.</p>
",1501472.0,,174777.0,,2020-07-30 05:53:18,2022-02-02 05:15:38,Programmatically Stop AWS EC2 in case of inactivity,<amazon-web-services><amazon-ec2><aws-lambda>,3,3,3.0,,,CC BY-SA 4.0,Can we stop an AWS windows server EC2 instance of a development environment when there is no activity in it  say after 2 hours of inactivity  I am having trouble identifying whether any user is connected to the server virtually  I can easily start/stop the EC2 at a fixed time  programmatically  but in order to cut the cost of my server  I am trying to stop the EC2 when it is not being used  My intent(or use case) is: If no user is using the EC2 till a specified amount of time  it will automatically stop  Developers can restart it as and when needed  
47066949,1,47066986.0,,2017-11-02 03:08:01,,3,531,"<p>AWS Lambda .NET Core functions have the ability to be defined as async entry point methods.</p>

<p>If I'm using ""await"" in my functions to wait for IO to respond, am I charged the full amount still?  </p>

<p>What is the benefit of writing everything as async await in this scenario vs not using async/await? </p>
",29277.0,,,,,2017-11-02 03:11:50,Does AWS Lambda charge for time an async .NET Core function is awaiting?,<c#><.net><aws-lambda>,1,0,,,,CC BY-SA 3.0,AWS Lambda  NET Core functions have the ability to be defined as async entry point methods  If Im using await in my functions to wait for IO to respond  am I charged the full amount still    What is the benefit of writing everything as async await in this scenario vs not using async/await   
47114831,1,,,2017-11-04 19:39:10,,1,1186,"<p>I am looking to use some ""serverless api server"" for AWS Lambda /zappa that uses a custom API Gateway authorizer for user authentification. In serverless AWS lambda service is there a considerable security or cost benefit in using custom authorizer rather than checking the issued JWT token directly in your code controller? For me checking with the code could be more convenient.</p>

<p>UPDATE
I went for pre request hooks, however there is header level authorizer, it is easier to use for CORS, yet it is not supported by zappa I believe. Also setting mock API for Options might be possible via swagger upload, will update if succeed.</p>
",5874981.0,,5874981.0,,2018-01-26 17:48:56,2019-03-16 01:01:46,Is AWS API Gateway custom authorizer useful?,<authentication><authorization><aws-lambda><aws-api-gateway>,3,0,,,,CC BY-SA 3.0,I am looking to use some serverless api server for AWS Lambda /zappa that uses a custom API Gateway authorizer for user authentification  In serverless AWS lambda service is there a considerable security or cost benefit in using custom authorizer rather than checking the issued JWT token directly in your code controller  For me checking with the code could be more convenient  UPDATE I went for pre request hooks  however there is header level authorizer  it is easier to use for CORS  yet it is not supported by zappa I believe  Also setting mock API for Options might be possible via swagger upload  will update if succeed  
63200340,1,,,2020-07-31 23:40:50,,0,179,"<p>I have a NLP program which uses Spacy to load in a large word2vec file. This file is 3.75 GB and is too large to put directly in a lambda. Currently I have this program running in an extra large EC2 instance, but this option is expensive. The code also only runs on demand around four times a week, so keeping a server running constantly is not what I want.</p>
<ul>
<li>Is there any way I can convert this to serverless?</li>
<li>If I use EFS, aren't I just converting one server to a server plus a lambda?</li>
<li>Is there an API I can talk to which would return the same information that Spacy can, specifically similarity of two sentences?</li>
</ul>
",13650339.0,,,,,2020-07-31 23:40:50,How should I load a large NLP file into Lambda,<amazon-web-services><aws-lambda><nlp><spacy><serverless>,0,8,,,,CC BY-SA 4.0,I have a NLP program which uses Spacy to load in a large word2vec file  This file is 3 75 GB and is too large to put directly in a lambda  Currently I have this program running in an extra large EC2 instance  but this option is expensive  The code also only runs on demand around four times a week  so keeping a server running constantly is not what I want   Is there any way I can convert this to serverless  If I use EFS  arent I just converting one server to a server plus a lambda  Is there an API I can talk to which would return the same information that Spacy can  specifically similarity of two sentences   
63925195,1,,,2020-09-16 17:31:05,,4,1400,"<p>I am working on a node.js back-end for an application where customers make requests for a certain service. Once they make a request, the payment amount get held on Stripe, then get captured once the service is delivered.
But if the service was not delivered within a certain number of days, the request get canceled (on database) and the payment get released.</p>
<p>For that, I needed an external service which allows me to schedule a call (<strong>once</strong>) to an API (<strong>of mine</strong>) programmatically for each customer request.</p>
<p>This way I can schedule a call for each customer request to run on the final date of that request while holding its info. My API can then use that info to check the status of the request and make the required changes to the database as well as payment release in case the service was not delivered.</p>
<p>I Knew that I can do this by scheduling a rule (problematically via the put-rule command using the aws-sdk) that triggers a lambda function which execute against my API, but I need an example on how to do this Knowing that I'll be passing an API parameter (which is the <strong>request_id</strong>) from the cloudwatch event to the lambda function.</p>
",14233782.0,,13494542.0,,2020-09-27 22:02:47,2020-12-23 08:27:17,"How to schedule a call API using node.js aws-sdk, cloudwatch and lambda function",<node.js><amazon-web-services><aws-lambda><scheduled-tasks>,2,0,,,,CC BY-SA 4.0,I am working on a node js back-end for an application where customers make requests for a certain service  Once they make a request  the payment amount get held on Stripe  then get captured once the service is delivered  But if the service was not delivered within a certain number of days  the request get canceled (on database) and the payment get released  For that  I needed an external service which allows me to schedule a call (once) to an API (of mine) programmatically for each customer request  This way I can schedule a call for each customer request to run on the final date of that request while holding its info  My API can then use that info to check the status of the request and make the required changes to the database as well as payment release in case the service was not delivered  I Knew that I can do this by scheduling a rule (problematically via the put-rule command using the aws-sdk) that triggers a lambda function which execute against my API  but I need an example on how to do this Knowing that Ill be passing an API parameter (which is the request_id) from the cloudwatch event to the lambda function  
63915422,1,,,2020-09-16 07:48:14,,3,1075,"<p>I'm running a system with lots of AWS Lambda functions. Our load is not huge, let's say a function gets 100k invocations per month.</p>
<p>For quite a few of the Lambda functions, we're using warmup plugins to reduce cold start times. This is effectively a CloudWatch event triggered every 5 minutes to invoke the function with a dummy event which is ignored, but keeps that Lambda VM running. In most cases, this means <em>one</em> instance will be &quot;warm&quot;.</p>
<p>I'm now looking at the native solution to the cold start problem: <a href=""https://aws.amazon.com/blogs/aws/new-provisioned-concurrency-for-lambda-functions/"" rel=""nofollow noreferrer"">AWS Lambda Concurrent Provisioning</a>, which at first glance looks awesome, but when I start calculating, either I'm missing something, or this will simply be a large cost increase for a system with only medium load.</p>
<p>Example, with prices from the <code>eu-west-1</code> region as of 2020-09-16:</p>
<p>Consider function RAM <code>M</code> (GB), average execution time <code>t</code> (s), million requests per month <code>N</code>, provisioned concurrency <code>C</code> (&quot;number of instances&quot;):</p>
<p><strong>Without provisioned concurrency</strong></p>
<p>Cost per month = <code>N(16.6667Mt + 0.20)</code></p>
<p>= $16.87 per million requests @ <code>M</code> = 1 GB, <code>t</code> = 1 s</p>
<p>= $1.87 per million requests @ <code>M</code> = 1 GB, <code>t</code> = 100 ms</p>
<p>= $1.69 per 100.000 requests @ <code>M</code> = 1 GB, <code>t</code> = 1 s</p>
<p>= $1686.67 per 100M requests @ <code>M</code> = 1GB, <code>t</code> = 1 s</p>
<p><strong>With provisioned concurrency</strong></p>
<p>Cost per month = <code>C0.000004646M60602430 + N(10.8407Mt + 0.20) = 12.04CM + N(10.84Mt + 0.20)</code></p>
<p>= $12.04 + $11.04 = $23.08 per million requests @ <code>M</code> = 1 GB, <code>t</code> = 1 s, <code>C</code> = 1</p>
<p>= $12.04 + $1.28 = $13.32 per million requests @ <code>M</code> = 1 GB, <code>t</code> = 100 ms, <code>C</code> = 1</p>
<p>= $12.04 + $1.10 = $13.14 per 100.000 requests @ <code>M</code> = 1 GB, <code>t</code> = 1 s, <code>C</code> = 1</p>
<p>= $12.04 + $1104.07 = $1116.11 per 100M requests @ <code>M</code> = 1 GB, <code>t</code> = 1 s, <code>C</code> = 1</p>
<p>There are obviously several factors to take into account here:</p>
<ul>
<li>How many requests per month is expected? (<code>N</code>)</li>
<li>How much RAM does the function need? (<code>M</code>)</li>
<li>What is the average execution time? (<code>t</code>)</li>
<li>What is the traffic pattern, few small bursts or even traffic (might mean <code>C</code> is low, high, or must be dynamically changed to follow peak hours etc)</li>
</ul>
<p>In the end though, my initial conclusion is that Provisioned Concurrency will only be a good deal if you have a <em>lot</em> of traffic? In my example, at 100M requests per month there's a substantial saving (however, at that traffic it's perhaps likely that you would need a higher value of <code>C</code> as well; break-even at about <code>C</code> = 30). Even with <code>C</code> = 1, you need almost a million requests per month to cover the static costs.</p>
<p>Now, there are obviously other benefits of using the native solution (no ugly dummy events, no log pollution, flexible amount of warm instances, ...), and there are also probably other hidden costs of custom solutions (CloudWatch events, additional CloudWatch logging for dummy invocations etc), but I think they are pretty much neglectible.</p>
<p>Is my analysis fairly correct or am I missing something?</p>
",1226020.0,,,,,2020-09-17 10:35:58,Cost efficiency for AWS Lambda Provisioned Concurrency,<amazon-web-services><aws-lambda>,1,1,,,,CC BY-SA 4.0,Im running a system with lots of AWS Lambda functions  Our load is not huge  lets say a function gets 100k invocations per month  For quite a few of the Lambda functions  were using warmup plugins to reduce cold start times  This is effectively a CloudWatch event triggered every 5 minutes to invoke the function with a dummy event which is ignored  but keeps that Lambda VM running  In most cases  this means one instance will be warm  Im now looking at the native solution to the cold start problem:   which at first glance looks awesome  but when I start calculating  either Im missing something  or this will simply be a large cost increase for a system with only medium load  Example  with prices from the  region as of 2020-09-16: Consider function RAM  (GB)  average execution time  (s)  million requests per month   provisioned concurrency  (number of instances): Without provisioned concurrency Cost per month =  = $16 87 per million requests @  = 1 GB   = 1 s = $1 87 per million requests @  = 1 GB   = 100 ms = $1 69 per 100 000 requests @  = 1 GB   = 1 s = $1686 67 per 100M requests @  = 1GB   = 1 s With provisioned concurrency Cost per month =  = $12 04 + $11 04 = $23 08 per million requests @  = 1 GB   = 1 s   = 1 = $12 04 + $1 28 = $13 32 per million requests @  = 1 GB   = 100 ms   = 1 = $12 04 + $1 10 = $13 14 per 100 000 requests @  = 1 GB   = 1 s   = 1 = $12 04 + $1104 07 = $1116 11 per 100M requests @  = 1 GB   = 1 s   = 1 There are obviously several factors to take into account here:  How many requests per month is expected  () How much RAM does the function need  () What is the average execution time  () What is the traffic pattern  few small bursts or even traffic (might mean  is low  high  or must be dynamically changed to follow peak hours etc)  In the end though  my initial conclusion is that Provisioned Concurrency will only be a good deal if you have a lot of traffic  In my example  at 100M requests per month theres a substantial saving (however  at that traffic its perhaps likely that you would need a higher value of  as well; break-even at about  = 30)  Even with  = 1  you need almost a million requests per month to cover the static costs  Now  there are obviously other benefits of using the native solution (no ugly dummy events  no log pollution  flexible amount of warm instances     )  and there are also probably other hidden costs of custom solutions (CloudWatch events  additional CloudWatch logging for dummy invocations etc)  but I think they are pretty much neglectible  Is my analysis fairly correct or am I missing something  
47536542,1,,,2017-11-28 16:34:52,,3,3894,"<p>I'm encountering some problems using Serverless framework, since i accidentally used the same name of a service on another one. </p>

<p><code>An error occurred: tableX - TableX already exists.</code></p>

<p>Let's suppose that i have two ""<strong>serverless.yml</strong>"" files, both with the same name of service. One of them (let's call it, ""<strong>test1</strong>"") have resources (DynamoDB tables), the other hasn't (""<strong>test2</strong>""). Like the following snippets:</p>

<p>Test1</p>

<pre><code>service: sandbox-core
provider:
  name: aws
  stage: core
  runtime: nodejs6.10
  region: sa-east-1
  memorySize: 128
  timeout: 300

resources:
  Resources:

    table3:
      Type: 'AWS::DynamoDB::Table'
      DeletionPolicy: Retain
      Properties:
        TableName: SandboxTable3
        AttributeDefinitions:
          -
            AttributeName: provider
            AttributeType: S
          -
            AttributeName: appId
            AttributeType: S
        KeySchema:
          -
            AttributeName: provider
            KeyType: HASH
          -
            AttributeName: appId
            KeyType: RANGE

        ProvisionedThroughput:
          ReadCapacityUnits: 1
          WriteCapacityUnits: 1

    table4:
      Type: 'AWS::DynamoDB::Table'
      DeletionPolicy: Retain
      Properties:
        TableName: SandboxTable4
        AttributeDefinitions:
          -
            AttributeName: session
            AttributeType: S
        KeySchema:
          -
            AttributeName: session
            KeyType: HASH

        ProvisionedThroughput:
          ReadCapacityUnits: 5
          WriteCapacityUnits: 1

functions:
  auth:
    handler: handler.auth
    events:
      - http:
          path: auth/{session}/{provider}/{appId}
          method: get
          cors: true
</code></pre>

<p>Test2</p>

<pre><code>service: sandbox-core

provider:
  name: aws
  stage: core
  runtime: nodejs6.10
  region: sa-east-1
  memorySize: 128
  timeout: 300

functions:
  createCustomData:
    handler: handler.createCustomData
    events:
      - http:
          path: teste2
          method: post
          cors: true
</code></pre>

<p>When i <code>sls deploy</code> the ""<strong>test1</strong>"", he creates the tables as i wanted, with <code>DeletionPolicy: Retain</code>, for the tables with very sensible data. Then i <code>sls deploy</code> ""<strong>test2</strong>"" that has other functions but doesn't have any resources (DynamoDB tables), he does what is expected: skip the deletion of the tables. </p>

<p>But, when i sls deploy ""<strong>test1</strong>"" again, he doesn't recognizes the tables, he starts to ""create"" existing tables rather than update them, and fails to deploy.</p>

<p>I need the tables that aren't deleted, and need the functions on the service. It looks like the Cloud Formation losted the track of the created tables from the first deploy.</p>

<p>I don't whant to separate the services (one only for the resources) like was said on this <a href=""https://github.com/serverless/serverless/issues/3183"" rel=""nofollow noreferrer"">github thread</a>.
I need the tables that are running, it has a lot of data and it's too much expensive to backup and restore it to another one, a lot of users could be affected.</p>

<p>So, how do i tell to Cloud Formation Stack that i'm updating that table, and not trying to create it? How to keep track of a service on the Cloud Formation Stack? And, how do i prevent to deploy a service with resources without them? </p>

<p>What's the best solution for this case? Hope that my questions are clear enough to understand. </p>
",8548278.0,,,,,2021-06-30 19:55:04,Serverless Service update Dynamodb table created with DeletionPolicy retain,<amazon-web-services><amazon-dynamodb><amazon-cloudformation><serverless-framework><serverless>,3,0,,,,CC BY-SA 3.0,Im encountering some problems using Serverless framework  since i accidentally used the same name of a service on another one    Lets suppose that i have two serverless yml files  both with the same name of service  One of them (lets call it  test1) have resources (DynamoDB tables)  the other hasnt (test2)  Like the following snippets: Test1  Test2  When i  the test1  he creates the tables as i wanted  with   for the tables with very sensible data  Then i  test2 that has other functions but doesnt have any resources (DynamoDB tables)  he does what is expected: skip the deletion of the tables   But  when i sls deploy test1 again  he doesnt recognizes the tables  he starts to create existing tables rather than update them  and fails to deploy  I need the tables that arent deleted  and need the functions on the service  It looks like the Cloud Formation losted the track of the created tables from the first deploy  I dont whant to separate the services (one only for the resources) like was said on this   I need the tables that are running  it has a lot of data and its too much expensive to backup and restore it to another one  a lot of users could be affected  So  how do i tell to Cloud Formation Stack that im updating that table  and not trying to create it  How to keep track of a service on the Cloud Formation Stack  And  how do i prevent to deploy a service with resources without them   Whats the best solution for this case  Hope that my questions are clear enough to understand   
65306726,1,65338163.0,,2020-12-15 13:21:05,,0,285,"<p>I am using Elasticcloud (hosted elasticsearch) to index my app data. Now I want to start streaming logs from my AWS lambda functions to my Elasticcloud account. I have googled and I can see that there are couple of ways to do this:</p>
<ol>
<li>Functionbeat</li>
<li>Cloudwatch-&gt; Elasticsearch subscription filter</li>
<li>Cloudwatch-&gt; Lambda subscription filter</li>
</ol>
<p>My questions are</p>
<ul>
<li>which is the most cost efficient and performant way to stream logs from AWS cloudwatch to elasticcloud</li>
<li>For functionbeat is it necessary to first send logs to a S3 bucket? (I am referring to this <a href=""https://www.elastic.co/guide/en/beats/functionbeat/current/configuration-functionbeat-options.html"" rel=""nofollow noreferrer"">https://www.elastic.co/guide/en/beats/functionbeat/current/configuration-functionbeat-options.html</a>)</li>
</ul>
",658209.0,,,,,2020-12-17 09:42:40,Send logs from AWS to Elasticcloud,<elasticsearch><aws-lambda><filebeat><amazon-cloudwatchlogs><elastic-cloud>,1,0,,,,CC BY-SA 4.0,I am using Elasticcloud (hosted elasticsearch) to index my app data  Now I want to start streaming logs from my AWS lambda functions to my Elasticcloud account  I have googled and I can see that there are couple of ways to do this:  Functionbeat Cloudwatch-&gt; Elasticsearch subscription filter Cloudwatch-&gt; Lambda subscription filter  My questions are  which is the most cost efficient and performant way to stream logs from AWS cloudwatch to elasticcloud For functionbeat is it necessary to first send logs to a S3 bucket  (I am referring to this )  
65309501,1,,,2020-12-15 16:06:41,,0,102,"<p>I am implementing the server side of my app in azure functions. I usually make a lot of smaller private functions(e.g parsing data) to keep my code clean and improve usability and readability but working in azure functions is making me wonder what are the best practices in the current scenario.</p>
<p>Should I make smaller private functions and do they incur additional cost when called from my main http trigger functions or should I put all code in the http trigger function? The latter seems like a very unintuitive approach.</p>
",3370136.0,,,,,2020-12-15 20:02:32,Calling private functions from public azure functions?,<azure><azure-functions><serverless>,1,0,0.0,,,CC BY-SA 4.0,I am implementing the server side of my app in azure functions  I usually make a lot of smaller private functions(e g parsing data) to keep my code clean and improve usability and readability but working in azure functions is making me wonder what are the best practices in the current scenario  Should I make smaller private functions and do they incur additional cost when called from my main http trigger functions or should I put all code in the http trigger function  The latter seems like a very unintuitive approach  
65370498,1,65371771.0,,2020-12-19 13:48:49,,6,1383,"<p>I'm evaluating a solution approach using Docker containers. Now that lambda is also supporting container images this falls in my consideration too.</p>
<p>I'm evaluating based on the following factors</p>
<ol>
<li>Pricing model of the 2 services</li>
<li>Cold start issue</li>
<li>Ease of Lamda integration with other AWS services</li>
<li>Ease of offline development with docker containers (i think it's not that relevant now)</li>
</ol>
<p>Any other factor I need to consider between the 2 services?</p>
",5252960.0,,5252960.0,,2020-12-19 15:47:24,2021-11-09 16:58:07,AWS Lambda Container Image Support Vs Fargate,<amazon-web-services><aws-lambda><amazon-ecs><aws-fargate>,1,0,,,,CC BY-SA 4.0,Im evaluating a solution approach using Docker containers  Now that lambda is also supporting container images this falls in my consideration too  Im evaluating based on the following factors  Pricing model of the 2 services Cold start issue Ease of Lamda integration with other AWS services Ease of offline development with docker containers (i think its not that relevant now)  Any other factor I need to consider between the 2 services  
65405656,1,65406282.0,,2020-12-22 08:49:10,,0,315,"<p>After creating and using an Authorizer in Api Gateway, there is an option to enable Authorization Caching, with a variable TTL(seconds) settings.</p>
<p><a href=""https://i.stack.imgur.com/cIIoT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cIIoT.png"" alt=""Edit Authorzizer"" /></a></p>
<p>What is the pricing involved with this authorization caching?</p>
",5579871.0,,5579871.0,,2020-12-28 15:50:33,2020-12-28 15:50:33,Api Gateway Authorizer Authorization Caching Pricing,<amazon-web-services><aws-api-gateway><aws-serverless>,2,0,,,,CC BY-SA 4.0,After creating and using an Authorizer in Api Gateway  there is an option to enable Authorization Caching  with a variable TTL(seconds) settings   What is the pricing involved with this authorization caching  
30876345,1,31676570.0,,2015-06-16 19:20:45,,8,4190,"<p>I've adapted the Amazon example of <a href=""http://docs.aws.amazon.com/lambda/latest/dg/walkthrough-s3-events-adminuser-create-test-function-create-function.html"">resizing a photo in lambda</a> to create multiple thumbnail sizes and run in parallel.  </p>

<p>My code runs fine locally in a few seconds, but in the the lambda cloud, it will not run in parallel, throwing an error after resizing the first thumbnail size.. and if I switch it to be serial instead of parallel it takes around 60 seconds to run serially.</p>

<p>Why would running resize code in parallel in lambda cause the stream yields empty buffer error.  How can I increase the performance so that I can create the sizes in a few seconds but still get good value and efficiency out of lambda in terms of processor cost?</p>

<pre><code>// dependencies
var async = require('async');
var AWS = require('aws-sdk');
var gm = require('gm')
            .subClass({ imageMagick: true }); // Enable ImageMagick integration.
var util = require('util');

// constants
var SIZES = [100, 320, 640];

// get reference to S3 client 
var s3 = new AWS.S3();

exports.handler = function(event, context) {

    // Read options from the event.
    console.log(""Reading options from event:\n"", util.inspect(event, {depth: 5}));
    var srcBucket = event.Records[0].s3.bucket.name;
    var srcKey    = event.Records[0].s3.object.key;
    var dstBucket = srcBucket + ""-resized"";

    // Infer the image type.
    var typeMatch = srcKey.match(/\.([^.]*)$/);
    if (!typeMatch) {
        console.error('unable to infer image type for key ' + srcKey);
        return context.done();
    }
    var imageType = typeMatch[1];
    if (imageType != ""jpg"" &amp;&amp; imageType != ""png"") {
        console.log('skipping non-image ' + srcKey);
        return context.done();
    }

    // Sanity check: validate that source and destination are different buckets.
    if (srcBucket == dstBucket) {
        console.error(""Destination bucket must not match source bucket."");
        return context.done();
    }

    // Download the image from S3
    s3.getObject({
            Bucket: srcBucket,
            Key: srcKey
        },
        function(err, response){

            if (err)
                return console.error('unable to download image ' + err);

            var contentType = response.ContentType;

            var original =  gm(response.Body);
            original.size(function(err, size){

                if(err)
                    return console.error(err);

                //transform, and upload to a different S3 bucket.
                async.each(SIZES,
                    function (max_size,  callback) {
                        resize_photo(size, max_size, imageType, original, srcKey, dstBucket, contentType, callback);
                    },
                    function (err) {
                        if (err) {
                            console.error(
                                'Unable to resize ' + srcBucket +
                                ' due to an error: ' + err
                            );
                        } else {
                            console.log(
                                'Successfully resized ' + srcBucket
                            );
                        }

                        context.done();
                    });
            });


        });



};

//wrap up variables into an options object
var resize_photo = function(size, max_size, imageType, original, srcKey, dstBucket, contentType, done) {

    var dstKey = max_size +  ""_"" + srcKey;


    // transform, and upload to a different S3 bucket.
    async.waterfall([

        function transform(next) {


            // Infer the scaling factor to avoid stretching the image unnaturally.
            var scalingFactor = Math.min(
                max_size / size.width,
                max_size / size.height
            );
            var width  = scalingFactor * size.width;
            var height = scalingFactor * size.height;


            // Transform the image buffer in memory.
            original.resize(width, height)
                .toBuffer(imageType, function(err, buffer) {

                    if (err) {
                        next(err);
                    } else {
                        next(null, buffer);
                    }
                });

        },
        function upload(data, next) {
            // Stream the transformed image to a different S3 bucket.
            s3.putObject({
                    Bucket: dstBucket,
                    Key: dstKey,
                    Body: data,
                    ContentType: contentType
                },
                next);
            }
        ], function (err) {

            console.log('finished resizing ' + dstBucket + '/' + dstKey);

            if (err) {
                console.error(err)
                ;
            } else {
                console.log(
                    'Successfully resized ' + dstKey
                );
            }

            done(err);
        }
    );
};
</code></pre>
",611750.0,,,,,2015-07-28 12:38:06,"In amazon lambda, resizing multiple thumbnail sizes in parallel async throws Error: Stream yields empty buffer",<node.js><amazon-web-services><imagemagick><aws-lambda>,1,0,2.0,,,CC BY-SA 3.0,Ive adapted the Amazon example of  to create multiple thumbnail sizes and run in parallel    My code runs fine locally in a few seconds  but in the the lambda cloud  it will not run in parallel  throwing an error after resizing the first thumbnail size   and if I switch it to be serial instead of parallel it takes around 60 seconds to run serially  Why would running resize code in parallel in lambda cause the stream yields empty buffer error   How can I increase the performance so that I can create the sizes in a few seconds but still get good value and efficiency out of lambda in terms of processor cost   
66060449,1,66061381.0,,2021-02-05 09:09:32,,0,283,"<p>I'm making a lambda function that will generate a pdf file that then needs to be sent to S3.</p>
<p>The file is generated in the lambda function, so I can't use a pre-signed url to upload it from the client since the client does not have the file. The file generated will not be on the range of 512MB (lambda /tmp storage limit) but will be more than 6MB.</p>
<p>I'm not sure yet if i should convert this to a container instead since lambda has a maximum request payload of 6MB.</p>
<p>One idea that came to mind is to use s3 multipart-upload and upload the parts into chunks of 4MB.</p>
<p>Does that actually solve the problem though? Or should i just create a container instead?</p>
<p>Taking into account lambda cost savings having a way to go around the lambda 6MB limit would be very beneficial in my case.</p>
",6014385.0,,6485881.0,,2021-02-05 10:34:16,2021-02-05 10:34:16,Will multipart upload with S3 overcome the Lambda 6MB request payload limit?,<amazon-s3><aws-lambda>,1,0,,,,CC BY-SA 4.0,Im making a lambda function that will generate a pdf file that then needs to be sent to S3  The file is generated in the lambda function  so I cant use a pre-signed url to upload it from the client since the client does not have the file  The file generated will not be on the range of 512MB (lambda /tmp storage limit) but will be more than 6MB  Im not sure yet if i should convert this to a container instead since lambda has a maximum request payload of 6MB  One idea that came to mind is to use s3 multipart-upload and upload the parts into chunks of 4MB  Does that actually solve the problem though  Or should i just create a container instead  Taking into account lambda cost savings having a way to go around the lambda 6MB limit would be very beneficial in my case  
48269652,1,,,2018-01-15 19:42:05,,1,66,"<p>I have an application running in N.Virginia Region. I want to configure Regional failover such that if my N.Virgina region or application goes down, same application should come up secondary Region. Initially secondary Region should not have any resources. May be we can only have a CloudFormation template for application or an Lambda function in secondary Region which could bring up all the resources in secondary Region.</p>

<p>For data replication between regions, I will have snapshots of EBS copied periodically to secondary region, and will run a Read replica of RDS in secondary region. But I want, EC2, Loadbalancer, EBS and other resources to spin up on failover.</p>

<p>My primary concern is to achieve Region level high availability without having to spend cost on secondary Region until failover happens.</p>
",8763639.0,,8763639.0,,2018-01-16 07:12:35,2018-01-16 07:12:35,AWS Regional failover with secondary Region having no resources running untill failover happens,<amazon-web-services><aws-lambda><amazon-sns><amazon-route53><amazon-cloudwatch>,0,2,2.0,,,CC BY-SA 3.0,I have an application running in N Virginia Region  I want to configure Regional failover such that if my N Virgina region or application goes down  same application should come up secondary Region  Initially secondary Region should not have any resources  May be we can only have a CloudFormation template for application or an Lambda function in secondary Region which could bring up all the resources in secondary Region  For data replication between regions  I will have snapshots of EBS copied periodically to secondary region  and will run a Read replica of RDS in secondary region  But I want  EC2  Loadbalancer  EBS and other resources to spin up on failover  My primary concern is to achieve Region level high availability without having to spend cost on secondary Region until failover happens  
48290694,1,48290774.0,,2018-01-16 22:14:19,,51,10113,"<p>I have an RDS database instance on AWS and have turned it off for now.  However, every few days it starts up on its own.  I don't have any other services running right now.</p>

<p>There is this event in my RDS log:
""DB instance is being started due to it exceeding the maximum allowed time being stopped.""</p>

<p>Why is there a limit to how long my RDS instance can be stopped?  I just want to put my project on hold for a few weeks, but AWS won't let me turn off my DB?  It costs $12.50/mo to have it sit idle, so I don't want to pay for this, and I certainly don't want AWS starting an instance for me that does not get used.</p>

<p>Please help!</p>
",2600971.0,,,,,2021-05-25 02:00:45,AWS: Why does my RDS instance keep starting after I turned it off?,<database><amazon-web-services><aws-lambda>,1,0,3.0,,,CC BY-SA 3.0,I have an RDS database instance on AWS and have turned it off for now   However  every few days it starts up on its own   I dont have any other services running right now  There is this event in my RDS log: DB instance is being started due to it exceeding the maximum allowed time being stopped  Why is there a limit to how long my RDS instance can be stopped   I just want to put my project on hold for a few weeks  but AWS wont let me turn off my DB   It costs $12 50/mo to have it sit idle  so I dont want to pay for this  and I certainly dont want AWS starting an instance for me that does not get used  Please help  
66114258,1,,,2021-02-09 06:53:13,,0,242,"<p>I have a client application that uses StripeJS + Stripe Checkout to create a session and then redirects to a success URL.</p>
<p>My <code>/webhook</code> call is conveniently called alerting the application to successful payment. To update a Cognito user with a <code>custom:paid</code> attribute, I'm attempting to pass the user's access token around through my Lamda route's authorizer and then into the Stripe object through either metadata or <a href=""https://stripe.com/docs/api/checkout/sessions/object#checkout_session_object-client_reference_id"" rel=""nofollow noreferrer"">client_reference_id</a>.</p>
<p>Stripe limits the character length of both of those fields which prevents using that approach exactly.</p>
<p>How can one pass Cognito user credentials through Stripe to correlate successful payments to the existing user scheme?</p>
",3599414.0,,,,,2021-02-09 06:53:13,"Using Amplify, Stripe Checkout, and AWS Lambda",<aws-lambda><stripe-payments><amazon-cognito>,0,2,,,,CC BY-SA 4.0,I have a client application that uses StripeJS + Stripe Checkout to create a session and then redirects to a success URL  My  call is conveniently called alerting the application to successful payment  To update a Cognito user with a  attribute  Im attempting to pass the users access token around through my Lamda routes authorizer and then into the Stripe object through either metadata or   Stripe limits the character length of both of those fields which prevents using that approach exactly  How can one pass Cognito user credentials through Stripe to correlate successful payments to the existing user scheme  
66194572,1,,,2021-02-14 10:45:20,,1,375,"<p>I am struggling to understand a basic aspect of Lambda implementation.</p>
<p><strong>Problem:</strong> how to use a lambda both inside and outside of an API context?</p>
<p>I have a lambda (<strong>nodejs</strong>) with an API gateway in front of it:</p>
<pre class=""lang-yaml prettyprint-override""><code> MyFunction:
    Type: AWS::Serverless::Function
    Properties:
        CodeUri: functions/myfunction/
        Handler: app.lambdaHandler
        Runtime: nodejs14.x
        Timeout: 4
        Policies:
            - DynamoDBCrudPolicy:
                  TableName: MyTable
        Events:
            ApiEvent:
                Type: Api
                Properties:
                    Path: /myfunc
                    Method: any
</code></pre>
<p>The handler is used to read (<strong>GET</strong>) or write (<strong>POST</strong>) into a a <strong>DynamoDB</strong> table and returns accordingly. If no method is passed it assumes GET.</p>
<pre class=""lang-js prettyprint-override""><code>exports.lambdaHandler = async (event) =&gt; {

    const method = event.httpMethod ? event.httpMethod.toUpperCase() : &quot;GET&quot;;
    
    try {
        switch (method) {
            case &quot;GET&quot;:
                // assuming an API context event.queryStringParameters might have request params. If not the parameters will be on the event
                const params = event.queryStringParameters ? event.queryStringParameters : event;
                // read from dynamo db table then return an API response. what if outside an API context?
                return {
                    statusCode: 200,
                    body: JSON.stringify({ message: &quot;...&quot; })
                };
            case &quot;POST&quot;:
                // similarly the body will be defined in an API context
                const body = typeof event.body === &quot;string&quot; ? JSON.parse(event.body) : event.body;
                // write to dynamo db table
                return {
                    statusCode: 200,
                    body: JSON.stringify({ message: &quot;...&quot; })
                };
            default:
                return {
                    statusCode: 400,
                    body: JSON.stringify({ error: &quot;method not supported&quot; })
                };
        }
    } catch (error) {
        // this should throw an Error outside an API context
        return {
            statusCode: 400,
            body: JSON.stringify({ error: `${typeof error === &quot;string&quot; ? error : JSON.stringify(error)}` })
        };
    }
}
</code></pre>
<p>Is there an easy way to refactor this code to support both scenarios? For example a step function could call this lambda as well. I know I can have the step function invoking an API but I think this is overkill as step functions support invoking lambdas directly.</p>
<p>I see 2 ways I can go about this:</p>
<ol>
<li><p><strong>The lambda has to be aware of whether it is being invoked within an API context or not.</strong> It needs to check if there's a http method, queryStringParameters and build its input from these. Then it needs to return a response accordingly as well. A stringified JSON with statusCode or something else, including throwing an Error if outside an API call.</p>
</li>
<li><p><strong>The lambda assumes it is being called from an API.</strong> Then the step function needs to format the input to simulate the API call. The problem is that the response will be a string which makes it difficult to process inside a step function. For example assigning it to a ResultPath or trying to decide if there was an error or not inside a choice.</p>
</li>
</ol>
<p>Additionally I could have the step function calling an API directly or the last resort would be to have 2 separate lambdas where the API lambda calls the other one but this will incur additional costs.</p>
<p>Thoughts? Thanks.</p>
",15207298.0,,6840517.0,,2021-02-14 21:51:23,2021-02-14 21:51:23,AWS Lambda to be invoked directly from a step function (without invoking the API) and also within an API context,<javascript><amazon-web-services><aws-lambda><aws-api-gateway><aws-step-functions>,1,4,,,,CC BY-SA 4.0,I am struggling to understand a basic aspect of Lambda implementation  Problem: how to use a lambda both inside and outside of an API context  I have a lambda (nodejs) with an API gateway in front of it:  The handler is used to read (GET) or write (POST) into a a DynamoDB table and returns accordingly  If no method is passed it assumes GET   Is there an easy way to refactor this code to support both scenarios  For example a step function could call this lambda as well  I know I can have the step function invoking an API but I think this is overkill as step functions support invoking lambdas directly  I see 2 ways I can go about this:  The lambda has to be aware of whether it is being invoked within an API context or not  It needs to check if theres a http method  queryStringParameters and build its input from these  Then it needs to return a response accordingly as well  A stringified JSON with statusCode or something else  including throwing an Error if outside an API call   The lambda assumes it is being called from an API  Then the step function needs to format the input to simulate the API call  The problem is that the response will be a string which makes it difficult to process inside a step function  For example assigning it to a ResultPath or trying to decide if there was an error or not inside a choice    Additionally I could have the step function calling an API directly or the last resort would be to have 2 separate lambdas where the API lambda calls the other one but this will incur additional costs  Thoughts  Thanks  
45601262,1,,,2017-08-09 21:57:36,,0,386,"<p>I made a Python web scraper for downloading more than 4PB to 8PB data from the web. I have to run More than 1k + spider per sec for downloading data from 12  websites. If I use ec2 instance it will be very costally. Someone told me to use SWF And lambda. But I didn't find anything on the web About web scraper with SWF.</p>

<p>Is it possible to run this spider via Amazon Simple Workflow Service OR AWS Lambda?</p>
",5737718.0,,,,,2017-08-10 03:15:29,Is it possible to run a Python web scraper via AWS SWF and Lambda Function?,<aws-lambda><amazon-swf>,1,0,,,,CC BY-SA 3.0,I made a Python web scraper for downloading more than 4PB to 8PB data from the web  I have to run More than 1k + spider per sec for downloading data from 12  websites  If I use ec2 instance it will be very costally  Someone told me to use SWF And lambda  But I didnt find anything on the web About web scraper with SWF  Is it possible to run this spider via Amazon Simple Workflow Service OR AWS Lambda  
45692168,1,45701956.0,,2017-08-15 11:44:47,,2,3284,"<p>I have a Python scraping script I wish to run on AWS Lambda, to save on EC2 costs. However, the script also requires PhantomJS, oauth2client, PYOpenSSL, selenium and of course, Beautiful Soup to complete it's scraping.</p>

<p>Is it possible to run Beautiful Soup (by running the additional required programs above too) on AWS Lambda?</p>
",8399727.0,,,,,2017-08-15 21:38:15,Web scraping with AWS Lambda,<javascript><python><python-2.7><beautifulsoup><aws-lambda>,1,2,,,,CC BY-SA 3.0,I have a Python scraping script I wish to run on AWS Lambda  to save on EC2 costs  However  the script also requires PhantomJS  oauth2client  PYOpenSSL  selenium and of course  Beautiful Soup to complete its scraping  Is it possible to run Beautiful Soup (by running the additional required programs above too) on AWS Lambda  
45726185,1,,,2017-08-17 03:33:51,,1,989,"<p>I'm a junior Laravel PHP Developer with a couple of years of industry experience. I am a great lover to learn new things. Nowadays I'm after the ""Serverless"" :-). We were taught a lot of benefits of the OOP over procedural programming (where the procedural programming was presented as a villain)</p>

<ul>
<li>Can someone please help me to understand about this Serverless???</li>
<li>If it is another player to kill the OOP after the Event Driven?</li>
<li>If not, then how we can go with the OOP and Serverless together?</li>
<li>OR Event Driven Procedural is better than OOP?</li>
</ul>

<p>As I think first the Event Driven (and now the Serverless) was encouraging the developers to move towards the Procedural Programming. but that wasn't the case for the PHP as a backend language but only with the JS with Node.
But now it seems the serverless is another actor came into existing to kill the OOP and encourage the procedural programming to fool the people on the name of ""Low Cost"" and pushing the developers back to the 80s.</p>

<p>If someone, having knowledge of OOP/Procedural Programming, can help me to understand the Serverless thing as my concern is that we'll have to leave the OOP and which would be a great issue of the maintenance.</p>
",3762480.0,,,,,2017-08-18 03:39:43,Is Serverless Proving to be another Player to Kill the OOP after the Event Driven?,<oop><lambda><event-driven><procedural-programming><serverless-architecture>,1,0,,,,CC BY-SA 3.0,Im a junior Laravel PHP Developer with a couple of years of industry experience  I am a great lover to learn new things  Nowadays Im after the Serverless :-)  We were taught a lot of benefits of the OOP over procedural programming (where the procedural programming was presented as a villain)  Can someone please help me to understand about this Serverless    If it is another player to kill the OOP after the Event Driven  If not  then how we can go with the OOP and Serverless together  OR Event Driven Procedural is better than OOP   As I think first the Event Driven (and now the Serverless) was encouraging the developers to move towards the Procedural Programming  but that wasnt the case for the PHP as a backend language but only with the JS with Node  But now it seems the serverless is another actor came into existing to kill the OOP and encourage the procedural programming to fool the people on the name of Low Cost and pushing the developers back to the 80s  If someone  having knowledge of OOP/Procedural Programming  can help me to understand the Serverless thing as my concern is that well have to leave the OOP and which would be a great issue of the maintenance  
46452685,1,,,2017-09-27 16:16:12,,0,541,"<p>I am developng an app with xamarin and azure <strong>serverless</strong> functions as backend for the app.
I will be <strong>syncing map coordinates</strong> from the users in real time with a <strong>database on the cloud</strong>. i.e : taking coordinates from all users and then pushing the updated coordinates to all users at the same time, <strong>continuously</strong> so that all users can see live location of each other.</p>

<blockquote>
  <p>So I have to call an <strong>azure function</strong> in <strong>continous loop</strong> in order to sync database with cloud. so it can check db after every, 4-5 secs. is it the best way to do this? or will this cause too much execution of azure function and might be costly? If there is a better way to sync the db please suggest. Thankyou.</p>
</blockquote>
",5305863.0,,,,,2017-09-27 17:53:26,continuous database sync with azure functions,<database><azure><synchronization><azure-functions><serverless>,1,5,,,,CC BY-SA 3.0,I am developng an app with xamarin and azure serverless functions as backend for the app  I will be syncing map coordinates from the users in real time with a database on the cloud  i e : taking coordinates from all users and then pushing the updated coordinates to all users at the same time  continuously so that all users can see live location of each other   So I have to call an azure function in continous loop in order to sync database with cloud  so it can check db after every  4-5 secs  is it the best way to do this  or will this cause too much execution of azure function and might be costly  If there is a better way to sync the db please suggest  Thankyou   
46502462,1,46502762.0,,2017-09-30 11:55:02,,5,1385,"<p>I'm new to AWS and would like to deploy a microservice on Amazon Web Services. The function code shall be in AWS Lambda and this functions shall be triggered through AWS API Gateway.</p>

<p>My lambda functions itself are protected via authorization. Furthermore, the number of authorised requests are within the free tier.</p>

<p><strong>Now my questions:</strong></p>

<ul>
<li>Can unauthorised attacks to Amazon API Gateway let the costs explode?</li>
<li>Can i prevent my Amazon API Gateway from such attacks?</li>
<li>Can i set a costs limit and shut the API off, in case of too high bills?</li>
<li>Are intentionally API attacks common?</li>
</ul>

<p>Thanks</p>
",6797189.0,,,,,2019-07-11 03:06:29,Amazon API Gateway - Intentional attacks for costs raising,<amazon-web-services><aws-lambda><aws-api-gateway>,2,0,5.0,,,CC BY-SA 3.0,Im new to AWS and would like to deploy a microservice on Amazon Web Services  The function code shall be in AWS Lambda and this functions shall be triggered through AWS API Gateway  My lambda functions itself are protected via authorization  Furthermore  the number of authorised requests are within the free tier  Now my questions:  Can unauthorised attacks to Amazon API Gateway let the costs explode  Can i prevent my Amazon API Gateway from such attacks  Can i set a costs limit and shut the API off  in case of too high bills  Are intentionally API attacks common   Thanks 
46553447,1,,,2017-10-03 21:08:24,,4,3495,"<p>I am a bit new to AWS / Lambda from the technical side so I have a scenario adn I wanted your help. </p>

<p>I have a file that drops daily, I only care about the file on the last day of the month. They all drop to the same bucket the file drops at 8 EST. </p>

<p>I then need to rename the file from the last day of the month to a static name, and copying it to a bucket lets say the file is called bill. I would want the previous file there called bill_september if we are in october. </p>

<p>So my thought is to have a cron job kick off a Lambda function every day at noon to move a file except for the last day of the month. the first day of the month at 8 AM I will have it kick off a lambda job at 5 AM to copy to new bucket.</p>

<p>So the questions are</p>

<ol>
<li>Does this make sense? </li>
<li>Can I have lambda rename existing file to file+month? </li>
</ol>

<p>I am always open to a better solution so please tell me if i am totally turned around </p>
",8716624.0,,267693.0,,2017-10-04 06:35:13,2017-10-04 06:35:13,How to Rename or Copy Files on AWS Lambda function?,<amazon-web-services><amazon-s3><aws-lambda>,1,1,1.0,,,CC BY-SA 3.0,I am a bit new to AWS / Lambda from the technical side so I have a scenario adn I wanted your help   I have a file that drops daily  I only care about the file on the last day of the month  They all drop to the same bucket the file drops at 8 EST   I then need to rename the file from the last day of the month to a static name  and copying it to a bucket lets say the file is called bill  I would want the previous file there called bill_september if we are in october   So my thought is to have a cron job kick off a Lambda function every day at noon to move a file except for the last day of the month  the first day of the month at 8 AM I will have it kick off a lambda job at 5 AM to copy to new bucket  So the questions are  Does this make sense   Can I have lambda rename existing file to file+month    I am always open to a better solution so please tell me if i am totally turned around  
63078075,1,63082437.0,,2020-07-24 16:53:29,,0,1468,"<p>I have a Lambda function that is triggered by API Gateway.</p>
<p>Based on request parameters, this function may call another API endpoint(s).</p>
<p>The URL of the other API endpoint(s) are passed by parameter in the request.</p>
<p>So, for example, I can call my endpoint like this:</p>
<p><a href=""https://xxxxxxxx.execute-api.us-east-1.amazonaws.com/dev/my-function?other-api-endpoint=https://api.example.com/"" rel=""nofollow noreferrer"">https://xxxxxxxx.execute-api.us-east-1.amazonaws.com/dev/my-function?other-api-endpoint=https://api.example.com/</a></p>
<p>And as a result, the Lambda function will at some point call the other API: <a href=""https://api.example.com/"" rel=""nofollow noreferrer"">https://api.example.com/</a></p>
<p>My real function have some complex logic before calling other APIs, but you got the idea.</p>
<p>There are some cases where the function will call its own endpoint (recursion), but passing different parameters to process some logic, so that it will not cause infinite loop.</p>
<p>But someone may accidentally configure parameters incorrectly so that it will cause an infinite loop, causing the function to be called millions of times per hour, leading AWS charges to the top.</p>
<p>How can I prevent this Lambda function to cause an accidental infinite loop and avoid enormous AWS bills?</p>
",2444386.0,,82118.0,,2021-05-21 15:41:14,2021-05-21 15:41:14,Prevent accidental infinite loop in Lambda with API Gateway,<amazon-web-services><aws-lambda><aws-api-gateway><infinite-loop>,1,4,1.0,,,CC BY-SA 4.0,I have a Lambda function that is triggered by API Gateway  Based on request parameters  this function may call another API endpoint(s)  The URL of the other API endpoint(s) are passed by parameter in the request  So  for example  I can call my endpoint like this:  And as a result  the Lambda function will at some point call the other API:  My real function have some complex logic before calling other APIs  but you got the idea  There are some cases where the function will call its own endpoint (recursion)  but passing different parameters to process some logic  so that it will not cause infinite loop  But someone may accidentally configure parameters incorrectly so that it will cause an infinite loop  causing the function to be called millions of times per hour  leading AWS charges to the top  How can I prevent this Lambda function to cause an accidental infinite loop and avoid enormous AWS bills  
60452363,1,60452631.0,,2020-02-28 13:05:07,,0,770,"<p>I am not an expert in AWS but have some experience. Got a situation where Angular UI (host on EC2) would have to talk to RDS DB instance. All set so far in the stack except API(middle ware). We are thinking of using Lambda (as our traffic is unknown at this time). Again here we have lot of choices to make on programming side like C# or Python or Node. (we are tilting towards C# or Python based on the some research done and skills also Python good at having great cold start and C# .NET core being stable in terms of performance).
Since we are with Lambda offcourse we should go in the route of API GATEWAY. all set but now, can all the business logic of our application can reside in Lambda? if so wouldnt it Lambda becomes huge and take performance hit(more memory, more computational resources thus higher costs?)? then we thought of lets have Lambda out there to take light weight processing and heavy lifting can be moved to .NET API that host on EC2?</p>

<p>Not sure of we are seeing any issues in this approach? Also have to mention that, Lambda have to call RDS for CRUD operations then should I think to much about concurrency issues? as it might fall into state full category?</p>
",3264937.0,,22080.0,,2020-02-28 13:17:08,2020-02-28 13:19:21,AWS Lambda vs EC2 REST API,<amazon-ec2><aws-lambda><aws-api-gateway><asp.net-core-webapi>,1,0,,,,CC BY-SA 4.0,I am not an expert in AWS but have some experience  Got a situation where Angular UI (host on EC2) would have to talk to RDS DB instance  All set so far in the stack except API(middle ware)  We are thinking of using Lambda (as our traffic is unknown at this time)  Again here we have lot of choices to make on programming side like C# or Python or Node  (we are tilting towards C# or Python based on the some research done and skills also Python good at having great cold start and C#  NET core being stable in terms of performance)  Since we are with Lambda offcourse we should go in the route of API GATEWAY  all set but now  can all the business logic of our application can reside in Lambda  if so wouldnt it Lambda becomes huge and take performance hit(more memory  more computational resources thus higher costs )  then we thought of lets have Lambda out there to take light weight processing and heavy lifting can be moved to  NET API that host on EC2  Not sure of we are seeing any issues in this approach  Also have to mention that  Lambda have to call RDS for CRUD operations then should I think to much about concurrency issues  as it might fall into state full category  
46653586,1,,,2017-10-09 19:21:32,,0,588,"<p>This is my first time deploying to AWS Lambda and am getting a little stuck.</p>

<p>I have a large maven project called <code>Helpers</code> which has many submodules, many of them dependent on each other. In there I have one Helper called <code>Alerts</code>. I have a parent directory and everything builds and compiles successfully. So, that's good. </p>

<p>In Alerts there's a class called <code>PaymentAlerts</code> which has the line</p>

<p><code>import com.mywebsite.messages.Doers</code></p>

<p>where messages.Doers is found in the dependencies.</p>

<p>But, when I do a <code>mvn package</code> on the whole project and I find <code>alerts-1.0.jar</code> and upload it to AWS Lambda and I set my handler as <code>com.mywebsite.alerts.PaymentAlerts::doAlert</code> I get the following error:</p>

<blockquote>
  <p>{  </p>
  
  <p>com.mywebsite.alerts.PaymentAlerts: com/mywebsite/messaging/Doers"",</p>
  
  <p>""errorType"": ""java.lang.NoClassDefFoundError""</p>
  
  <p>""errorMessage"": ""Error loading class
  }</p>
</blockquote>

<p>How do I reconfigure this so that it finds all the necessary files?</p>

<p>Any and all help is appreciated!</p>
",5407287.0,,,,,2017-10-09 20:41:55,AWS Lambda can't find dependent library,<java><maven><amazon-web-services><aws-lambda>,2,0,,,,CC BY-SA 3.0,This is my first time deploying to AWS Lambda and am getting a little stuck  I have a large maven project called  which has many submodules  many of them dependent on each other  In there I have one Helper called   I have a parent directory and everything builds and compiles successfully  So  thats good   In Alerts theres a class called  which has the line  where messages Doers is found in the dependencies  But  when I do a  on the whole project and I find  and upload it to AWS Lambda and I set my handler as  I get the following error:  {   com mywebsite alerts PaymentAlerts: com/mywebsite/messaging/Doers  errorType: java lang NoClassDefFoundError errorMessage: Error loading class   }  How do I reconfigure this so that it finds all the necessary files  Any and all help is appreciated  
60457162,1,,,2020-02-28 18:11:27,,2,174,"<p>With each lambda invoke either with AWS API or API Gateway HTTP, kms usage is increasing while I haven't added any key management with KMS. Is this indirect cost by aws on lambda usage or there is option to disable kms on lambda invoke.</p>
",5917790.0,,,,,2020-03-06 23:46:06,Why is aws charging for kms when calling lambda?,<aws-lambda><aws-kms>,1,2,1.0,,,CC BY-SA 4.0,With each lambda invoke either with AWS API or API Gateway HTTP  kms usage is increasing while I havent added any key management with KMS  Is this indirect cost by aws on lambda usage or there is option to disable kms on lambda invoke  
60490004,1,,,2020-03-02 13:37:05,,1,248,"<p>I need to create a AWS Bucket Policy which blocks all external IP addresses, except our office IP, but still allows Lambda functions to access the Bucket.</p>

<p>I know how to make this work using a AWS VPC and NAT, however due to the high costs involved the client doesn't want to activate those.</p>

<p>So far this how my bucket policy looks like, but it's not working:</p>

<pre><code>{
""Version"": ""2008-10-17"",
""Statement"": [
    {
        ""Sid"": ""GiveSESPermissionToWriteEmail"",
        ""Effect"": ""Allow"",
        ""Principal"": {
            ""Service"": ""ses.amazonaws.com""
        },
        ""Action"": ""s3:PutObject"",
        ""Resource"": ""arn:aws:s3:::files-dev/*""
    },
    {
        ""Sid"": ""SourceIP"",
        ""Effect"": ""Deny"",
        ""Principal"": ""*"",
        ""Action"": ""s3:*"",
        ""Resource"": ""arn:aws:s3:::files-dev/*"",
        ""Condition"": {
            ""NotIpAddress"": {
                ""aws:SourceIp"": [
                    ""81.111.111.111/24""
                ]
            }
        }
    },
    {
        ""Sid"": ""GiveLambdaPermisssion"",
        ""Effect"": ""Allow"",
        ""Principal"": {
            ""AWS"": [
                ""arn:aws:iam::6XXXXXX:role/app-backend-dev-lambdaFunctionRole-1XXXX""
            ],
            ""Service"": ""lambda.amazonaws.com""
        },
        ""Action"": ""s3:*"",
        ""Resource"": ""arn:aws:s3:::files-dev/*""
    }
]
</code></pre>

<p>}</p>

<p>I've think I read all of AWS documentation I could find related to this, but I couldn't find what I was looking for. It might be because their docs, are so confusing and do not cover all the functionality. And I have also searched for solutions here on StackOverflow and other forums but nothing worked.</p>
",11316939.0,,,,,2021-01-30 01:44:04,AWS Bucket Policy to Allow Lambda but block all other external IPs,<amazon-web-services><amazon-s3><aws-lambda>,2,4,,,,CC BY-SA 4.0,I need to create a AWS Bucket Policy which blocks all external IP addresses  except our office IP  but still allows Lambda functions to access the Bucket  I know how to make this work using a AWS VPC and NAT  however due to the high costs involved the client doesnt want to activate those  So far this how my bucket policy looks like  but its not working:  } Ive think I read all of AWS documentation I could find related to this  but I couldnt find what I was looking for  It might be because their docs  are so confusing and do not cover all the functionality  And I have also searched for solutions here on StackOverflow and other forums but nothing worked  
63085775,1,,,2020-07-25 07:48:46,,0,249,"<p>For my application on AWS, I want to host a temporary HTTP server and dispose it in a few minutes. I think EC2 instance / Fargate would be a costly overkill for this. So thinking of doing this on a Lambda function. Is this possible?</p>
<p>Can a Lambda function expose an IP address (could be temporary)? Or provide some way for others to communicate with the lambda function - after it has started?</p>
<p>Or is there any other way to achieve this goal?</p>
<p><strong>Adding more detail</strong>:
I want to achieve something like this. The external client invokes an API, to get a URL. The client should then be able to interact with this URL in a stateful session, defined by the parameters passed in the initial API invocation.</p>
<p>This session would last a couple of minutes, after which the state/session/client is forgotten. The Lambda will use the time between requests to work in the background and prepare for next request from the client.</p>
<p>I know we can do this by saving session details in the dynamodb, triggering a fresh lambda function for every new request from the client, and more lambda functions in the background with SNS... But, I thought would be more exciting to do everything in a single Lambda invocation</p>
",1227873.0,,1227873.0,,2020-07-26 04:01:44,2020-07-26 04:01:44,Can a Lambda Function expose an IP Address?,<amazon-web-services><aws-lambda><aws-serverless>,1,4,1.0,,,CC BY-SA 4.0,For my application on AWS  I want to host a temporary HTTP server and dispose it in a few minutes  I think EC2 instance / Fargate would be a costly overkill for this  So thinking of doing this on a Lambda function  Is this possible  Can a Lambda function expose an IP address (could be temporary)  Or provide some way for others to communicate with the lambda function - after it has started  Or is there any other way to achieve this goal  Adding more detail: I want to achieve something like this  The external client invokes an API  to get a URL  The client should then be able to interact with this URL in a stateful session  defined by the parameters passed in the initial API invocation  This session would last a couple of minutes  after which the state/session/client is forgotten  The Lambda will use the time between requests to work in the background and prepare for next request from the client  I know we can do this by saving session details in the dynamodb  triggering a fresh lambda function for every new request from the client  and more lambda functions in the background with SNS    But  I thought would be more exciting to do everything in a single Lambda invocation 
63086497,1,,,2020-07-25 09:08:54,,1,255,"<p>I try to reduce the costs on my mobile app backend.</p>
<p>Today I use an ASP.NET Core Web API plugged to an SQL Server instance and I use Entity Framework Core. Everything in Azure Mobile App services + SQL Server.</p>
<p>I plan to migrate to a serverless approach and cloud paas db instance.</p>
<p>I chose AWS because moving to serverless lambda from an existing API Core project seems much simpler than with Azure Functions.</p>
<p>In my way to migrate I plan to do:</p>
<ol>
<li><p>Move my Web API to a serverless app. I will use <a href=""https://aws.amazon.com/blogs/developer/deploy-an-existing-asp-net-core-web-api-to-aws-lambda"" rel=""nofollow noreferrer"">https://aws.amazon.com/blogs/developer/deploy-an-existing-asp-net-core-web-api-to-aws-lambda</a> way</p>
</li>
<li><p>I will use Aurora under MySql. Then I will not pay anymore the licenses for SQL Server. I will change in my backend the connection string in my Entity Framework Core and do some minor changes to move from SQL Server provider to MySQL. And I will purchase for instance reserved 1 year.</p>
</li>
</ol>
<p>Based on this I think I will reduce my costs (move from SQL Server to aurora MySQL), gain in scalability and performance with serverless functions.</p>
<p>Is it a good way?</p>
<p>Finally in my Web API .NET Core project, I use Swagger UI and Hangfire UI.
May I use it if I move my project as serverless?</p>
<p>Thanks</p>
",4780847.0,,13460933.0,,2020-07-25 09:21:19,2020-07-25 13:21:53,Aws Aurora + Lambda,<amazon-web-services><asp.net-web-api><aws-lambda><entity-framework-core><amazon-aurora>,1,0,,,,CC BY-SA 4.0,I try to reduce the costs on my mobile app backend  Today I use an ASP NET Core Web API plugged to an SQL Server instance and I use Entity Framework Core  Everything in Azure Mobile App services + SQL Server  I plan to migrate to a serverless approach and cloud paas db instance  I chose AWS because moving to serverless lambda from an existing API Core project seems much simpler than with Azure Functions  In my way to migrate I plan to do:  Move my Web API to a serverless app  I will use  way  I will use Aurora under MySql  Then I will not pay anymore the licenses for SQL Server  I will change in my backend the connection string in my Entity Framework Core and do some minor changes to move from SQL Server provider to MySQL  And I will purchase for instance reserved 1 year    Based on this I think I will reduce my costs (move from SQL Server to aurora MySQL)  gain in scalability and performance with serverless functions  Is it a good way  Finally in my Web API  NET Core project  I use Swagger UI and Hangfire UI  May I use it if I move my project as serverless  Thanks 
63086769,1,63086824.0,,2020-07-25 09:39:24,,1,696,"<p>I am creating a Disaster recovery solution in AWS. For second (fallback) region i want to have only 1 EC2 instance to minimise cost. In case of disaster i would like to know if it's possible to write a lambda function in the second region that increases the desired capacity of the auto Scaling group to some number.</p>
<p>To achieve this i can subscribe the function to the health check alarm SNS topic.</p>
<p>I would like to know if there is a API to autoscale a ec2 group from Lambda and what sort of roles/permissions is needed ?</p>
",663011.0,,13460933.0,,2020-07-25 12:44:18,2020-07-25 12:44:18,AWS lambda function for auto-scaling,<amazon-web-services><aws-lambda><aws-auto-scaling>,1,0,,,,CC BY-SA 4.0,I am creating a Disaster recovery solution in AWS  For second (fallback) region i want to have only 1 EC2 instance to minimise cost  In case of disaster i would like to know if its possible to write a lambda function in the second region that increases the desired capacity of the auto Scaling group to some number  To achieve this i can subscribe the function to the health check alarm SNS topic  I would like to know if there is a API to autoscale a ec2 group from Lambda and what sort of roles/permissions is needed   
60565652,1,,,2020-03-06 14:03:50,,0,255,"<p>I'm developing an <strong>ecommerce website</strong>.
<br/>It's for a ""ground-based"" clothing store that is used to sells only via third party platform.
<br/> And now want a own website.</p>

<p><strong>I started with Wordpress+Woocommerce.</strong></p>

<p><strong>Then i tried a ZeitNow+Next+Graphql+React</strong> version.
<br/>It connects itself to Wordpress+Woocomerce database via GraphQL Queries.<br/>
It uses ZeitNow to avoid implementing a real Node+Express server on my machine.</p>

<h1>Which path to choose to complete the website and publish it ? <br/>My doubts are related mainly to <strong>COSTS</strong>.</h1>

<hr>

<p><strong>If i choose the classic WP+Woocommerce</strong> way i need :</p>

<p>0-20 EUR /year for Domain Name.</p>

<p>120EUR / year approximately for a classic web server (with PHP+MySQL) hosting plan where to place the Wordpress+Woocommerce.</p>

<hr>

<p><strong>If i'd like to choose second option</strong>, based on what i know actually i need :</p>

<p>0-20 EUR /year for Domain Name.</p>

<p>120EUR / year approximately for a classic web server (with PHP+MySQL) hosting plan where to place the Wordpress+Woocommerce ""head"" part of my project, .</p>

<p>0EUR /year for serveless ZeitNow (free plan).</p>

<hr>

<p>But <strong>where i need to place the ""App</strong>"" (ZeitNow+Next+GraphQl+React)?
<br/>An other Web server (with Node) ?
<br/>So an other 120EUR/ year plan ?
<br />Or beacuse it's serverless i can only ""deploy"" to zeitNow and only link my domain to ZeitNow?
<br />Its not clear to me.</p>

<p>I found on the web things like ""Netifly"", ""Firebase"", ""Heroku"", ""AWS"" ... 
<br /> Are they all equivalent to Zeit Now?</p>

<hr>

<p><strong>I would like to publish a website with benefits of WooCommerce CMS system.</strong>
<br/>Like adding products, managing stocks, handling discounts plans, access to PayPal and Stripe payment methods integrations (i don't trust my self enough to build integration on my own due to security risks).</p>

<p><strong>I wolud like also the keep benefits of using React for front End</strong> like performance (at least perceived) for Final User, or no need of Ajax request to update Cart and Wishlist.</p>

<p>And <strong>what about calculating if my project needs a ""payed plan"" of ZeitNow/Netifly/AWS</strong> to manage the request? How i can calculate them?</p>

<p>Sorry for the high number of question, but for me, understand the co-existence of these things is overwhelming!</p>

<p>Thanks.</p>
",11447191.0,,,,,2020-03-06 22:56:29,Publishing a Headless ecommerce. Which Costs i need to consider?,<webserver><publish><headless><netlify><vercel>,1,1,,,,CC BY-SA 4.0,Im developing an ecommerce website  Its for a ground-based clothing store that is used to sells only via third party platform   And now want a own website  I started with Wordpress+Woocommerce  Then i tried a ZeitNow+Next+Graphql+React version  It connects itself to Wordpress+Woocomerce database via GraphQL Queries  It uses ZeitNow to avoid implementing a real Node+Express server on my machine  Which path to choose to complete the website and publish it   My doubts are related mainly to COSTS   If i choose the classic WP+Woocommerce way i need : 0-20 EUR /year for Domain Name  120EUR / year approximately for a classic web server (with PHP+MySQL) hosting plan where to place the Wordpress+Woocommerce   If id like to choose second option  based on what i know actually i need : 0-20 EUR /year for Domain Name  120EUR / year approximately for a classic web server (with PHP+MySQL) hosting plan where to place the Wordpress+Woocommerce head part of my project    0EUR /year for serveless ZeitNow (free plan)   But where i need to place the App (ZeitNow+Next+GraphQl+React)  An other Web server (with Node)   So an other 120EUR/ year plan   Or beacuse its serverless i can only deploy to zeitNow and only link my domain to ZeitNow  Its not clear to me  I found on the web things like Netifly  Firebase  Heroku  AWS       Are they all equivalent to Zeit Now   I would like to publish a website with benefits of WooCommerce CMS system  Like adding products  managing stocks  handling discounts plans  access to PayPal and Stripe payment methods integrations (i dont trust my self enough to build integration on my own due to security risks)  I wolud like also the keep benefits of using React for front End like performance (at least perceived) for Final User  or no need of Ajax request to update Cart and Wishlist  And what about calculating if my project needs a payed plan of ZeitNow/Netifly/AWS to manage the request  How i can calculate them  Sorry for the high number of question  but for me  understand the co-existence of these things is overwhelming  Thanks  
46733970,1,,,2017-10-13 16:07:53,,0,34,"<p>when thinking about lambda costs for memory/requests, is there any evidence to suggest having larger functions with many package includes vs many smaller functions with minimal packages is a better option?</p>

<p>for example</p>

<pre><code>script A has 1(aws-sdk) + 9 required packages to complete 4 steps
-&gt; this would be a single script request with a longer execution time ( and larger memory requirement?
</code></pre>

<p>vs</p>

<pre><code>4 scripts with three packages each 1(aws-sdk) + two required packages to complete 4 steps
-&gt; this would be multiple single script requests with a shorter execution time ( and less memory requirement?
</code></pre>
",1187098.0,,13070.0,,2017-10-13 16:41:07,2017-10-14 01:24:26,lambda node.js single function with many packages vs many functions with minimum packages,<amazon-web-services><aws-lambda>,1,0,,,,CC BY-SA 3.0,when thinking about lambda costs for memory/requests  is there any evidence to suggest having larger functions with many package includes vs many smaller functions with minimal packages is a better option  for example  vs  
60580598,1,,,2020-03-07 17:56:29,,1,1213,"<p>I have data saved in a Microsoft excel file. I need to turn that data into something that a Lambda function can parse.</p>
<p>I think the best way to do this is to convert the excel file into a JSON file (and then my Lambda function can read and parse it).</p>
<p>What's the best way to do this?</p>
<p>To convert the excel data file into a JSON file, I have found some handy online converter tools, like <a href=""https://csv.keyangxiang.com/"" rel=""nofollow noreferrer"">this one</a>. It seems to work.</p>
<p>However, that converter and others add in <code>\r</code> wherever there are line breaks in the data, and <code>\</code> wherever there are quotes in the data. (the line breaks and especially quotes need to be in the data)</p>
<p>So to properly read the data in the JSON file, I have to then get rid of these changes to the raw data.</p>
<p>Is there another way to do this? Such as a converter that does not change the raw data in this way? Or some method other than a converter?</p>
<p>Once the raw data has been changed (by adding in stuff like <code>\r</code> and <code>\</code> like I mention above), it becomes cumbersome to remove it. I can do a find/replace to get rid of the changes, but that adds steps that can become costly time wise. And using regex could add performance hits.</p>
<p>**EDIT: Note that I probably need a method that creates an actual document (so a program that produces the data in a client browser would not work). I am looking to create an actual document that my Lambda can then analyze. **</p>
",10261833.0,,-1.0,,2020-06-20 09:12:55,2020-12-07 20:32:09,Best way to convert excel file into a JSON file?,<javascript><json><excel><aws-lambda>,1,2,,,,CC BY-SA 4.0,I have data saved in a Microsoft excel file  I need to turn that data into something that a Lambda function can parse  I think the best way to do this is to convert the excel file into a JSON file (and then my Lambda function can read and parse it)  Whats the best way to do this  To convert the excel data file into a JSON file  I have found some handy online converter tools  like   It seems to work  However  that converter and others add in  wherever there are line breaks in the data  and  wherever there are quotes in the data  (the line breaks and especially quotes need to be in the data) So to properly read the data in the JSON file  I have to then get rid of these changes to the raw data  Is there another way to do this  Such as a converter that does not change the raw data in this way  Or some method other than a converter  Once the raw data has been changed (by adding in stuff like  and  like I mention above)  it becomes cumbersome to remove it  I can do a find/replace to get rid of the changes  but that adds steps that can become costly time wise  And using regex could add performance hits  **EDIT: Note that I probably need a method that creates an actual document (so a program that produces the data in a client browser would not work)  I am looking to create an actual document that my Lambda can then analyze  ** 
60593344,1,,,2020-03-09 00:25:01,,2,256,"<p>I am going to mention my needs and what I have currently in place so bear with me. Firstly, a lambda function say <strong>F1</strong> which when invoked will get 100 links from a site. Most of these links say about 95 are the same as when F1 was invoked the previous time, so further processing must be done with only those 5 ""new"" links. One solution was to write to a Dynamodb database the links that are processed already and each time the F1 is invoked, query the database and skip those links. But I found that the ""database read"" although in milliseconds is doubling up lambda runtime and this can add up especially if F1 is called frequently and if there are say a million processed links. So I decided to use Elasticache with Redis.</p>

<p>I quickly found that Redis can be accessed only when F1 runs on the same VPC and because F1 needs access to the internet you need NAT. (I don't know much about networking) So I followed the guidelines and set up VPC and NAT and got everything to work. I was delighted with performance improvements, almost reduced the expected lambda cost in half to 30$ per month. But then I found that NAT is not included in the free tier and I have to pay almost 30$ per month just for NAT. This is not ideal for me as this project can be in development for months and I feel like I am paying the same amount as <em>compute</em> just for <em>internet access</em>.</p>

<p>I would like to know if I am making any fundamental mistakes. Am I using the Elasticache in the right way? Is there a better way to access both Redis and the internet? Is there any way to structure my stack differently so that I retain the performance without essentially paying twice the amount after free tier ends. Maybe add another lambda function? I don't have any ideas. Any <strong>minute</strong> improvements are much appreciated. Thank you.</p>
",6909259.0,,,,,2020-03-09 02:10:56,AWS Lambda with Elasticache Redis without NAT,<amazon-web-services><redis><aws-lambda><amazon-dynamodb><amazon-vpc>,1,3,,,,CC BY-SA 4.0,I am going to mention my needs and what I have currently in place so bear with me  Firstly  a lambda function say F1 which when invoked will get 100 links from a site  Most of these links say about 95 are the same as when F1 was invoked the previous time  so further processing must be done with only those 5 new links  One solution was to write to a Dynamodb database the links that are processed already and each time the F1 is invoked  query the database and skip those links  But I found that the database read although in milliseconds is doubling up lambda runtime and this can add up especially if F1 is called frequently and if there are say a million processed links  So I decided to use Elasticache with Redis  I quickly found that Redis can be accessed only when F1 runs on the same VPC and because F1 needs access to the internet you need NAT  (I dont know much about networking) So I followed the guidelines and set up VPC and NAT and got everything to work  I was delighted with performance improvements  almost reduced the expected lambda cost in half to 30$ per month  But then I found that NAT is not included in the free tier and I have to pay almost 30$ per month just for NAT  This is not ideal for me as this project can be in development for months and I feel like I am paying the same amount as compute just for internet access  I would like to know if I am making any fundamental mistakes  Am I using the Elasticache in the right way  Is there a better way to access both Redis and the internet  Is there any way to structure my stack differently so that I retain the performance without essentially paying twice the amount after free tier ends  Maybe add another lambda function  I dont have any ideas  Any minute improvements are much appreciated  Thank you  
60602511,1,,,2020-03-09 14:21:37,,-2,40,"<p>Im trying to print out every snapshot that hasn't got the specific Tag ""CostReference"" inside my aws account. </p>

<p>To Iterate through the snapshots I'm using: </p>

<pre><code>    for snapshot in snapshots:

    if(not costreferencetag_isset_snapshot(snapshot)):
        print(""[SNAPSHOT] "" + str(snapshot))
        print(""[INFO]: No CostReferenceTag!! \n"")
        missingtagginginfo = missingtagginginfo + str(snapshot) + "": No CostReferenceTag\n""
        count_snapshot += 1
        continue
    else:
        costreference_snapshot = get_costreference_snapshot(snapshot)

        if costreference_snapshot not in managedpsp:
            print(""[SNAPSHOT] "" + str(snapshot))
            print(""[INFO]: The PSP: "" + costreference_snapshot + "" of: "" + str(snapshot) + "" is WRONG! \n"")
            missingtagginginfo = missingtagginginfo + str(snapshot) + "" "" + costreference_snapshot + "": Wrong PSP\n""
            count_snapshot += 1
print(count_snapshot)
</code></pre>

<p>So far so good the code is working, but I'm also getting the public Snapshots that are owned by amazon, wich have no relevance for me.</p>

<p>Is there any way to filter those public snapshots?</p>

<p>Greets</p>

<p>Code for the other functions:</p>

<pre><code>def costreference_isset(instance):
#Searching for Instance without CostReference-tags
if instance.tags is None:
    print(""[INFO]: No Tags have been set yet:"")
    return(False)

#Searching for CostReference-tags
for t in instance.tags:
    if t['Key'] == 'CostReference':
        return(True)
return(False)

def get_costreferencetag(instance):

for t in instance.tags:
    if t['Key'] == ""CostReference"":
        return(str(t['Value']))

return(False)
</code></pre>

<p>managedpsp is a list with valid Costreference - tags</p>
",5904672.0,,,,,2020-03-10 01:40:40,Iterating through owned ec2.snapshots,<python><amazon-web-services><amazon-ec2><aws-lambda>,2,0,,2020-03-10 15:31:55,,CC BY-SA 4.0,Im trying to print out every snapshot that hasnt got the specific Tag CostReference inside my aws account   To Iterate through the snapshots Im using:   So far so good the code is working  but Im also getting the public Snapshots that are owned by amazon  wich have no relevance for me  Is there any way to filter those public snapshots  Greets Code for the other functions:  managedpsp is a list with valid Costreference - tags 
43689282,1,,,2017-04-28 21:41:02,,2,1008,"<p>Is it possible to substitute default Spring Framework's way of creating and managing objects via reflections with other dependency injection tool (that would be faster, because would avoid reflections), while still holding on Spring's rich API?</p>

<p>For example, I would like to have beans created by Dagger 2 or Tiger or Feather that would still be able to interact with Spring Data/Social/MVC.</p>

<p><a href=""https://github.com/google/dagger"" rel=""nofollow noreferrer"">https://github.com/google/dagger</a></p>

<p><a href=""https://github.com/google/tiger"" rel=""nofollow noreferrer"">https://github.com/google/tiger</a></p>

<p><a href=""https://github.com/zsoltherpai/feather"" rel=""nofollow noreferrer"">https://github.com/zsoltherpai/feather</a></p>

<p>I know that someone is going to say ""start worrying about performance when it will become problem"" - well, I would say it's about time to start worrying about it right now.</p>

<p>In my option, it would allow Spring to embrace FaaS (Function as a Service). FaaS jvm is going to be shut down after serving it's call, so You either keep it running (like regular server) and pay for literally every millisecond or some calls may be delayed even few seconds (to boot everything up).</p>

<p>I have found two projects, that are trying to use Spring in FaaS environment and are tackling this problem, but in my option it's easier to remove problem (reflections) that try to overcome it with hacks.</p>

<p><a href=""https://github.com/markfisher/spring-cloud-function"" rel=""nofollow noreferrer"">https://github.com/markfisher/spring-cloud-function</a></p>

<p><a href=""https://github.com/kennyk65/spring-cloud-serverless"" rel=""nofollow noreferrer"">https://github.com/kennyk65/spring-cloud-serverless</a></p>

<p>Or, maybe there is another way to solve this problem and efficiently use Spring in FaaS, that I am not aware of?</p>

<p>Related question: <a href=""https://stackoverflow.com/questions/43288216/running-spring-boot-on-amazon-lambda"">Running Spring Boot on Amazon Lambda</a></p>

<p>I have been trying to use minimal Spring Framework application (like 3-5 classes) and still it takes (sometimes) 5-15 seconds to handle first request (next are handled in 50-100ms), so minimizing isn't really working in this case.</p>
",6058078.0,,-1.0,,2017-05-23 12:34:17,2021-04-08 08:05:28,Spring without reflection,<java><spring><dependency-injection><aws-lambda>,2,2,,,,CC BY-SA 3.0,Is it possible to substitute default Spring Frameworks way of creating and managing objects via reflections with other dependency injection tool (that would be faster  because would avoid reflections)  while still holding on Springs rich API  For example  I would like to have beans created by Dagger 2 or Tiger or Feather that would still be able to interact with Spring Data/Social/MVC     I know that someone is going to say start worrying about performance when it will become problem - well  I would say its about time to start worrying about it right now  In my option  it would allow Spring to embrace FaaS (Function as a Service)  FaaS jvm is going to be shut down after serving its call  so You either keep it running (like regular server) and pay for literally every millisecond or some calls may be delayed even few seconds (to boot everything up)  I have found two projects  that are trying to use Spring in FaaS environment and are tackling this problem  but in my option its easier to remove problem (reflections) that try to overcome it with hacks    Or  maybe there is another way to solve this problem and efficiently use Spring in FaaS  that I am not aware of  Related question:  I have been trying to use minimal Spring Framework application (like 3-5 classes) and still it takes (sometimes) 5-15 seconds to handle first request (next are handled in 50-100ms)  so minimizing isnt really working in this case  
60637398,1,,,2020-03-11 13:39:12,,0,48,"<p>I would like to get a 2nd option for which AWS services to use in my following use case:</p>
<ol>
<li>Client calls a lambda function and writes an <code>Invoice</code> record into DynamoDB (<code>InvoiceTbl</code>)</li>
<li>In the <code>Invoice</code>, there is a collection of <code>bills</code> stored as a json attribute - <code>[{bill1, bill2, bill3}]</code> - which I would like to split up and write them individually into <code>BillTbl</code>, <strong>only if</strong> the <code>Invoice's</code> status is marked as <strong>Paid</strong></li>
<li>And if the <code>Invoice</code> is <strong>Paid</strong>, I will also like to aggregate the amount and updates it to <code>InvoiceTotalTbl</code></li>
</ol>
<p>Of course, the straight-forward way will be doing them in the lambda function. However, I would like to de-couple <code>2 and 3</code> into another process. I'm thinking of:</p>
<h1>Design 1</h1>
<ol>
<li>Use <code>DynamoDB Stream</code> on <code>InvoiceTbl</code> to trigger another lambda function</li>
<li>In that lambda function, check if the <code>Invoice</code> status is <strong>Paid</strong>, then do <code>2 and 3</code></li>
</ol>
<hr />
<h1>Design 2</h1>
<ol>
<li>In the lambda function, if the <code>Invoice</code> status is <strong>Paid</strong>, add it to <code>SQS</code></li>
<li>The <code>SQS</code> will then invoke a lambda function to do <code>2 and 3</code></li>
</ol>
<h3>Cons</h3>
<p><code>SQS</code> has a payload limit of 256 KB</p>
<hr />
<h1>Design 3</h1>
<ol>
<li>Use <code>Step Function</code></li>
<li>Once the <code>Invoice</code> status transit to <strong>Paid</strong>, it will do <code>2 and 3</code></li>
</ol>
<h3>Cons</h3>
<p><code>Step Function</code> is expensive (cost-wise)</p>
<hr />
<p><strong>Design 1</strong> seems to be fine in terms of speed - <code>2 and 3</code> gets to execute quickly without much delays. But I don't like the idea that it gets trigger everytimes; <code>2 and 3</code> should only be triggered if the <code>Invoice</code> status is <strong>Paid</strong>, and I'm afraid over time, the cost will be expensive</p>
<p>Is there any other better design/aws services I should be looking at for my particular use case?</p>
<p>Thanks.</p>
",3419791.0,,-1.0,,2020-06-20 09:12:55,2020-03-11 15:50:52,"In AWS lambda, what is the best way to Extract, Transform, Aggregate, and Write to DynamoDB",<amazon-web-services><aws-lambda><amazon-dynamodb>,0,5,,,,CC BY-SA 4.0,I would like to get a 2nd option for which AWS services to use in my following use case:  Client calls a lambda function and writes an  record into DynamoDB () In the   there is a collection of  stored as a json attribute -  - which I would like to split up and write them individually into   only if the  status is marked as Paid And if the  is Paid  I will also like to aggregate the amount and updates it to   Of course  the straight-forward way will be doing them in the lambda function  However  I would like to de-couple  into another process  Im thinking of: Design 1  Use  on  to trigger another lambda function In that lambda function  check if the  status is Paid  then do    Design 2  In the lambda function  if the  status is Paid  add it to  The  will then invoke a lambda function to do   Cons  has a payload limit of 256 KB  Design 3  Use  Once the  status transit to Paid  it will do   Cons  is expensive (cost-wise)  Design 1 seems to be fine in terms of speed -  gets to execute quickly without much delays  But I dont like the idea that it gets trigger everytimes;  should only be triggered if the  status is Paid  and Im afraid over time  the cost will be expensive Is there any other better design/aws services I should be looking at for my particular use case  Thanks  
60682030,1,,,2020-03-14 11:03:03,,0,1002,"<p>I am working on an integration with a Vendor that can only send me XML requests in a predefined structure
That is not compatible with my system. 
Changing the structure of the XML is possible but might be an expensive project if the Vendor would do it for me. </p>

<p>To try and solve this issue i was wondering, would it be possible to convert the said XML requests to JSON on the fly using an AWS Lambda function? 
I was thinking about a solution in which an XML requests comes in from the Vendor to my ALB, converted to a JSON request using a Lambda function and returned to my system for processing in a JSON format. 
Would this be a scalable solution?</p>
",4915344.0,,,,,2020-03-14 11:13:09,Convert XML requests to JSON on the fly using AWS lambda function and Application load balancer (ALB),<json><xml><rest><aws-lambda>,1,5,,,,CC BY-SA 4.0,I am working on an integration with a Vendor that can only send me XML requests in a predefined structure That is not compatible with my system   Changing the structure of the XML is possible but might be an expensive project if the Vendor would do it for me   To try and solve this issue i was wondering  would it be possible to convert the said XML requests to JSON on the fly using an AWS Lambda function   I was thinking about a solution in which an XML requests comes in from the Vendor to my ALB  converted to a JSON request using a Lambda function and returned to my system for processing in a JSON format   Would this be a scalable solution  
60714724,1,60714981.0,,2020-03-17 00:07:27,,0,915,"<p>I can't distinguish between privite subnet and public subnet.</p>

<p>I created a vpc and connected the subnet, and igw was also connected to the route tables. So, isn't the network a public subnet?</p>

<p>As I expected, the public network seems to have to communicate with the outside, but there is no communication at all.</p>

<p>Connection from aws lambda to RDS is possible (vpc), but timeout occurs for both uploading files to s3 and sending messages to slack.</p>

<p>I have seen a lot of posts about using vpc endpoints, but when I try to set it up</p>

<pre><code>Warning
When you use an endpoint, the source IP addresses from your instances in your affected subnets for accessing the AWS service in the same region will be private IP addresses, not public IP addresses. Existing connections from your affected subnets to the AWS service that use public IP addresses may be dropped. Ensure that you do nt have critical tasks running when you create or modify an endpoint.
</code></pre>

<p>So isn't ec2 currently disconnected from s3? Then, there is a problem with the service and it cannot be set.</p>

<p>In the lambda vpc configuration, even if all inbounds of the security group are opened, the connection is not established.</p>

<p>Is there only a way to set up NAT? </p>

<p>NAT wants to avoid it because of its cost.</p>

<p>my goal is to communicate with rds, s3, slack in lambda on vpc.</p>
",12329709.0,,,,,2020-03-17 01:35:17,How lambda connects to s3 inside vpc,<amazon-web-services><amazon-s3><aws-lambda><amazon-rds><amazon-vpc>,1,2,,,,CC BY-SA 4.0,I cant distinguish between privite subnet and public subnet  I created a vpc and connected the subnet  and igw was also connected to the route tables  So  isnt the network a public subnet  As I expected  the public network seems to have to communicate with the outside  but there is no communication at all  Connection from aws lambda to RDS is possible (vpc)  but timeout occurs for both uploading files to s3 and sending messages to slack  I have seen a lot of posts about using vpc endpoints  but when I try to set it up  So isnt ec2 currently disconnected from s3  Then  there is a problem with the service and it cannot be set  In the lambda vpc configuration  even if all inbounds of the security group are opened  the connection is not established  Is there only a way to set up NAT   NAT wants to avoid it because of its cost  my goal is to communicate with rds  s3  slack in lambda on vpc  
60771340,1,,,2020-03-20 09:15:12,,3,2558,"<p>I was running a serverless web application on a lambda inside a VPC, and connecting to a Aurora-MySQL RDS instance, with inbound rules to allow traffic from the security group of the lambda
The connection was working fine, however, quite often the lambda cold start was giving me a timeout.
After some research, I found out that running a lambda on a VPC brings an additional cost on startup and I saw the recommendation in more than 1 place to avoid using lambda on a VPC except if you strictly need to access some resource in the VPC.</p>

<p>So, I decided to move my RDS to a publicly accessible instance, so my lambda can access it over the internet and remove the lambda from the VPC.</p>

<p>So, I changed the RDS <code>Public accessibility</code> option to <code>Yes</code> and edited the security group to allow inbound connection from any IP.
I have also removed the VPC from the lambda, so the lambda is not running on a VPC anymore
I thought it was gonna be enough.</p>

<p>But then my lambda started failing to connect to the database
I tried to connect using my local client, again, failure</p>

<p>tried pinging to the hostname, got request timeouts</p>

<p>After digging a bit into it, I found that my DB instance subnet group having some private subnets might be a problem (?)
So, I have created a new subnet group with only public subnets, and tried to move my db instance to the new subnet group... but got this message:</p>

<pre><code>You cannot move DB instance my-instance to subnet group my-new-group. The specified DB subnet group and DB instance are in the same VPC.
</code></pre>

<p>Ok, it seems that I can't move to a different subnet in the same VPC, I started trying to create a new VPC, but it doesn't seem to be right and I'm sure there is something else I am missing here.</p>

<p>I also read about Network ACL, and thought that this might be the problem, but my rules seem to be fine, with the default rule to allow any traffic (and the rule * to DENY)</p>

<p><code>ALL Traffic ALL ALL 0.0.0.0/0 ALLOW</code></p>

<p>My RDS Network settings</p>

<pre><code>Subnet group
default

Subnets
subnet-11111111
subnet-22222222
subnet-33333333
subnet-44444444
subnet-55555555
subnet-66666666

Security
VPC security groups
default (sg-111111)
( active )

Public accessibility
Yes
</code></pre>

<p>My Security group inbound rules</p>

<pre><code>Type Protocol Port range    Source  Description - optional
All traffic All All 0.0.0.0/0   -
All traffic All All ::/0    -
</code></pre>

<p>Still can't connect, can't connect with my local client, can't even ping it:</p>

<p>Connecting through my local client</p>

<pre><code>Can't connect to MySQL server on 'my-instance.xxxxxxxxxx.us-east-1.rds.amazonaws.com' 
</code></pre>

<pre><code>ping my-instance.xxxxxxx.us-east-1.rds.amazonaws.com
PING ec2-xx-xx-xx-xx.compute-1.amazonaws.com (xx.xx.xx.xx): 56 data bytes
Request timeout for icmp_seq 0
Request timeout for icmp_seq 1
Request timeout for icmp_seq 2
</code></pre>

<p>Any idea of what I am missing here?</p>

<p><strong>UPDATE</strong></p>

<p>My VPC has internet access (I can access internet services from it, not an issue), I have an Internet Gateway and NAT Gateway in place.</p>

<p>I'm using Zappa for the lambda deployment, which takes care of creating a keep-warm function... however, I know that concurrent requests could still be an issue</p>

<p>The issue with VPC in lambda is that it can add 10s on the cold start, which is a no-deal for some of my use cases:
<a href=""https://www.freecodecamp.org/news/lambda-vpc-cold-starts-a-latency-killer-5408323278dd/"" rel=""nofollow noreferrer"">https://www.freecodecamp.org/news/lambda-vpc-cold-starts-a-latency-killer-5408323278dd/</a></p>
",1742659.0,,174777.0,,2020-03-20 11:31:21,2020-03-20 11:31:21,Can't connect to a public accessible AWS RDS,<amazon-web-services><aws-lambda><amazon-rds><amazon-vpc><subnet>,2,2,,,,CC BY-SA 4.0,I was running a serverless web application on a lambda inside a VPC  and connecting to a Aurora-MySQL RDS instance  with inbound rules to allow traffic from the security group of the lambda The connection was working fine  however  quite often the lambda cold start was giving me a timeout  After some research  I found out that running a lambda on a VPC brings an additional cost on startup and I saw the recommendation in more than 1 place to avoid using lambda on a VPC except if you strictly need to access some resource in the VPC  So  I decided to move my RDS to a publicly accessible instance  so my lambda can access it over the internet and remove the lambda from the VPC  So  I changed the RDS  option to  and edited the security group to allow inbound connection from any IP  I have also removed the VPC from the lambda  so the lambda is not running on a VPC anymore I thought it was gonna be enough  But then my lambda started failing to connect to the database I tried to connect using my local client  again  failure tried pinging to the hostname  got request timeouts After digging a bit into it  I found that my DB instance subnet group having some private subnets might be a problem ( ) So  I have created a new subnet group with only public subnets  and tried to move my db instance to the new subnet group    but got this message:  Ok  it seems that I cant move to a different subnet in the same VPC  I started trying to create a new VPC  but it doesnt seem to be right and Im sure there is something else I am missing here  I also read about Network ACL  and thought that this might be the problem  but my rules seem to be fine  with the default rule to allow any traffic (and the rule * to DENY)  My RDS Network settings  My Security group inbound rules  Still cant connect  cant connect with my local client  cant even ping it: Connecting through my local client   Any idea of what I am missing here  UPDATE My VPC has internet access (I can access internet services from it  not an issue)  I have an Internet Gateway and NAT Gateway in place  Im using Zappa for the lambda deployment  which takes care of creating a keep-warm function    however  I know that concurrent requests could still be an issue The issue with VPC in lambda is that it can add 10s on the cold start  which is a no-deal for some of my use cases:  
60787022,1,60792276.0,,2020-03-21 11:05:50,,1,2315,"<p>I am new to aws lambda and I am moving my spring boot 2.x based project to lambda.But I am struggling with lambda cold-start and warm-up. I tried a few things mentioned in this link:<a href=""https://github.com/awslabs/aws-serverless-java-container/wiki/Quick-start---Spring-Boot"" rel=""nofollow noreferrer"">https://github.com/awslabs/aws-serverless-java-container/wiki/Quick-start---Spring-Boot</a>, but still the application takes around 45 secs to start.</p>

<p>Things I tried:  </p>

<ol>
<li><p>Async initialization from the above link. It did help a bit but not enough.  </p></li>
<li><p>Skip the Init phase of the lambda. It helped reduce almost 8 secs.  </p></li>
<li><p>Provisioned concurrency but as far as I could see, it is not helping either. When I saw the logs, the spring context is getting initialized every time, if any request comes after an interval of 15-20 mins.</p></li>
</ol>

<p>The response time of my lambda in different scenarios is: </p>

<pre><code>1. 3008 MB memory/first request/ response time: ~25 secs.  
2. 3008 MB memory/2nd request immediately after 1st req/ response time: ~600ms.  
3. 1024 MB memory/1st req/ postman request times out.  
4. 1024 MB memory/2nd req immediately after 1st req/response time:  ~750ms.  
5. 1792 MB memory/1st req/ response time: ~27sec.  
6. 1792 MB memory/ 2nd req immediately after 1st req/response time:  ~650ms
</code></pre>

<p>To reduce this response time, I am thinking of making a REST call to my lambda every 5 or 10 mins so as to keep the spring context in memory and that in turn would help serve the requests faster. This call will be like a health check call, very less to no processing at all.</p>

<p>Is this an advisable approach? Or is there a better way of achieving this goal? </p>

<p>I am unclear about AWS will charge in this case.</p>
",4903603.0,,,,,2020-03-21 19:21:07,AWS Lambda cold start issue with spring boot,<java><spring-boot><aws-lambda><warm-up><cold-start>,1,3,1.0,,,CC BY-SA 4.0,I am new to aws lambda and I am moving my spring boot 2 x based project to lambda But I am struggling with lambda cold-start and warm-up  I tried a few things mentioned in this link:  but still the application takes around 45 secs to start  Things I tried:    Async initialization from the above link  It did help a bit but not enough    Skip the Init phase of the lambda  It helped reduce almost 8 secs    Provisioned concurrency but as far as I could see  it is not helping either  When I saw the logs  the spring context is getting initialized every time  if any request comes after an interval of 15-20 mins   The response time of my lambda in different scenarios is:   To reduce this response time  I am thinking of making a REST call to my lambda every 5 or 10 mins so as to keep the spring context in memory and that in turn would help serve the requests faster  This call will be like a health check call  very less to no processing at all  Is this an advisable approach  Or is there a better way of achieving this goal   I am unclear about AWS will charge in this case  
60781641,1,60781836.0,,2020-03-20 21:07:11,,1,801,"<p>I want to support a light-weight functionality (a DDB lookup and forward the request to backend service) supporting around peak 50 TPS with an acceptable latency &lt;1s, I was thinking about using lambda with provisioned concurrency feature, or is it better to use Fargate?</p>

<p>When should we prefer Fargate to Lambda with Provisioned concurrency feature?</p>

<p>Any pointers to cost, performance study of both the services is helpful.</p>
",3499519.0,,,,,2020-03-20 21:26:40,AWS Fargate vs Lambda with Provisioned concurrency,<aws-lambda><aws-fargate>,1,0,,,,CC BY-SA 4.0,I want to support a light-weight functionality (a DDB lookup and forward the request to backend service) supporting around peak 50 TPS with an acceptable latency &lt;1s  I was thinking about using lambda with provisioned concurrency feature  or is it better to use Fargate  When should we prefer Fargate to Lambda with Provisioned concurrency feature  Any pointers to cost  performance study of both the services is helpful  
46916670,1,,,2017-10-24 17:32:55,,0,67,"<p>I have a lambda that is coded in python 3.6 with some SQL queries in it to parse to aws Athena. This lambda will be triggered by a dynamoDB whenever there is a new item that falls into dynamodb. For each new item, I want to spin up a lambda. For suppose, if there are 100 new items that fall into DynamoDB- there will be 100 concurrent Lambda that will execute. But in this process, there was a moment where one of the SQL query failed in Athena and lambda cannot complete the calculation for that dynamodb record. </p>

<p>Note: the data of each record in dynamodb will be parsed to SQL queries in python and to AWS ATHENA.</p>

<p>My question: When there is an error in one of the concurrent Lambda's how do I stop this process, the lambda is repeating the execution for the same record which is costing me. </p>
",8604194.0,,2593745.0,,2017-10-26 10:18:41,2017-10-26 10:18:41,Controlling the operation of aws lambda(python 3.6),<python><amazon-web-services><aws-lambda>,1,3,,,,CC BY-SA 3.0,I have a lambda that is coded in python 3 6 with some SQL queries in it to parse to aws Athena  This lambda will be triggered by a dynamoDB whenever there is a new item that falls into dynamodb  For each new item  I want to spin up a lambda  For suppose  if there are 100 new items that fall into DynamoDB- there will be 100 concurrent Lambda that will execute  But in this process  there was a moment where one of the SQL query failed in Athena and lambda cannot complete the calculation for that dynamodb record   Note: the data of each record in dynamodb will be parsed to SQL queries in python and to AWS ATHENA  My question: When there is an error in one of the concurrent Lambdas how do I stop this process  the lambda is repeating the execution for the same record which is costing me   
60856781,1,,,2020-03-25 20:18:50,,0,948,"<p>When triggering my Spring Boot application on an AWS Lambda, I keep getting the error <code>java.lang.IllegalArgumentException: ServletContext must not be null</code>. I am unsure why this is happening and would like some more information.</p>

<p>Here is the stack trace:</p>

<pre class=""lang-java prettyprint-override""><code>2020-03-25 19:52:09.444 ERROR 8 --- [           main] c.a.s.p.internal.LambdaContainerHandler  : Error while handling request
java.lang.IllegalArgumentException: ServletContext must not be null
at org.springframework.util.Assert.notNull(Assert.java:198) ~[spring-core-5.1.14.RELEASE.jar:5.1.14.RELEASE]
at org.springframework.web.context.support.WebApplicationContextUtils.getWebApplicationContext(WebApplicationContextUtils.java:112) ~[spring-web-5.1.14.RELEASE.jar:5.1.14.RELEASE]
at org.springframework.web.context.support.WebApplicationContextUtils.getWebApplicationContext(WebApplicationContextUtils.java:101) ~[spring-web-5.1.14.RELEASE.jar:5.1.14.RELEASE]
at com.lukeshay.restapi.security.JwtAuthorizationFilter.doFilterInternal(JwtAuthorizationFilter.java:49) ~[task/:na]
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:119) ~[spring-web-5.1.14.RELEASE.jar:5.1.14.RELEASE]
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334) ~[spring-security-web-5.1.8.RELEASE.jar:5.1.8.RELEASE]
at org.springframework.security.web.authentication.AbstractAuthenticationProcessingFilter.doFilter(AbstractAuthenticationProcessingFilter.java:200) ~[spring-security-web-5.1.8.RELEASE.jar:5.1.8.RELEASE]
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334) ~[spring-security-web-5.1.8.RELEASE.jar:5.1.8.RELEASE]
at org.springframework.security.web.authentication.logout.LogoutFilter.doFilter(LogoutFilter.java:116) ~[spring-security-web-5.1.8.RELEASE.jar:5.1.8.RELEASE]
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334) ~[spring-security-web-5.1.8.RELEASE.jar:5.1.8.RELEASE]
at org.springframework.web.filter.CorsFilter.doFilterInternal(CorsFilter.java:97) ~[spring-web-5.1.14.RELEASE.jar:5.1.14.RELEASE]
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:119) ~[spring-web-5.1.14.RELEASE.jar:5.1.14.RELEASE]
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334) ~[spring-security-web-5.1.8.RELEASE.jar:5.1.8.RELEASE]
at org.springframework.security.web.header.HeaderWriterFilter.doFilterInternal(HeaderWriterFilter.java:74) ~[spring-security-web-5.1.8.RELEASE.jar:5.1.8.RELEASE]
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:119) ~[spring-web-5.1.14.RELEASE.jar:5.1.14.RELEASE]
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334) ~[spring-security-web-5.1.8.RELEASE.jar:5.1.8.RELEASE]
at org.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:105) ~[spring-security-web-5.1.8.RELEASE.jar:5.1.8.RELEASE]
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334) ~[spring-security-web-5.1.8.RELEASE.jar:5.1.8.RELEASE]
at org.springframework.security.web.context.request.async.WebAsyncManagerIntegrationFilter.doFilterInternal(WebAsyncManagerIntegrationFilter.java:56) ~[spring-security-web-5.1.8.RELEASE.jar:5.1.8.RELEASE]
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:119) ~[spring-web-5.1.14.RELEASE.jar:5.1.14.RELEASE]
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334) ~[spring-security-web-5.1.8.RELEASE.jar:5.1.8.RELEASE]
at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:215) ~[spring-security-web-5.1.8.RELEASE.jar:5.1.8.RELEASE]
at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:178) ~[spring-security-web-5.1.8.RELEASE.jar:5.1.8.RELEASE]
at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:358) ~[spring-web-5.1.14.RELEASE.jar:5.1.14.RELEASE]
at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:271) ~[spring-web-5.1.14.RELEASE.jar:5.1.14.RELEASE]
at com.amazonaws.serverless.proxy.internal.servlet.FilterChainHolder.doFilter(FilterChainHolder.java:90) ~[aws-serverless-java-container-core-1.4.jar:na]
at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201) ~[spring-web-5.1.14.RELEASE.jar:5.1.14.RELEASE]
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:119) ~[spring-web-5.1.14.RELEASE.jar:5.1.14.RELEASE]
at com.amazonaws.serverless.proxy.internal.servlet.FilterChainHolder.doFilter(FilterChainHolder.java:90) ~[aws-serverless-java-container-core-1.4.jar:na]
at com.lukeshay.restapi.config.CORSConfiguration.doFilter(CORSConfiguration.java:43) ~[task/:na]
at com.amazonaws.serverless.proxy.internal.servlet.FilterChainHolder.doFilter(FilterChainHolder.java:90) ~[aws-serverless-java-container-core-1.4.jar:na]
at com.amazonaws.serverless.proxy.internal.servlet.AwsLambdaServletContainerHandler.doFilter(AwsLambdaServletContainerHandler.java:150) ~[aws-serverless-java-container-core-1.4.jar:na]
at com.amazonaws.serverless.proxy.spring.SpringBootLambdaContainerHandler.handleRequest(SpringBootLambdaContainerHandler.java:143) ~[aws-serverless-java-container-springboot2-1.4.jar:na]
at com.amazonaws.serverless.proxy.spring.SpringBootLambdaContainerHandler.handleRequest(SpringBootLambdaContainerHandler.java:43) ~[aws-serverless-java-container-springboot2-1.4.jar:na]
at com.amazonaws.serverless.proxy.internal.LambdaContainerHandler.proxy(LambdaContainerHandler.java:211) ~[aws-serverless-java-container-core-1.4.jar:na]
at com.amazonaws.serverless.proxy.internal.LambdaContainerHandler.proxyStream(LambdaContainerHandler.java:246) ~[aws-serverless-java-container-core-1.4.jar:na]
at com.lukeshay.restapi.StreamLambdaHandler.handleRequest(StreamLambdaHandler.java:56) ~[task/:na]
at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:na]
at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source) ~[na:na]
at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source) ~[na:na]
at java.base/java.lang.reflect.Method.invoke(Unknown Source) ~[na:na]
at lambdainternal.EventHandlerLoader$StreamMethodRequestHandler.handleRequest(EventHandlerLoader.java:373) ~[aws-lambda-java-runtime-0.2.0.jar:na]
at lambdainternal.EventHandlerLoader$2.call(EventHandlerLoader.java:897) ~[aws-lambda-java-runtime-0.2.0.jar:na]
at lambdainternal.AWSLambda.startRuntime(AWSLambda.java:228) ~[aws-lambda-java-runtime-0.2.0.jar:na]
at lambdainternal.AWSLambda.startRuntime(AWSLambda.java:162) ~[aws-lambda-java-runtime-0.2.0.jar:na]
at lambdainternal.AWSLambda.main(AWSLambda.java:157) ~[aws-lambda-java-runtime-0.2.0.jar:na]
</code></pre>

<p><s>My source code can be found here: <a href=""https://github.com/LukeShay/route-rating-rest-api/tree/switch-to-lambda"" rel=""nofollow noreferrer"">https://github.com/LukeShay/route-rating-rest-api/tree/switch-to-lambda</a></s></p>

<p>Yes I know running Spring Boot on a lambda is dumb, but it is temporary. So, I don't have to pay for an EC2 instance. This is also not refined yet so I know there will be other things I should change.</p>

<p>EDIT: Here is the relevant code</p>

<p>This first one is the lambda handler. I originally had the commented out code but switched to the other because the start time was substantially faster. Both methods give the same error.</p>

<p>The second one is the class where the error is being encountered, which is my auth filter. I added the comment <code>// ERROR HERE</code> to the second snippet so it is easier to see the exact location of where the exception is being thrown.</p>

<pre class=""lang-java prettyprint-override""><code>package com.lukeshay.restapi;

import com.amazonaws.serverless.exceptions.ContainerInitializationException;
import com.amazonaws.serverless.proxy.internal.LambdaContainerHandler;
import com.amazonaws.serverless.proxy.model.AwsProxyRequest;
import com.amazonaws.serverless.proxy.model.AwsProxyResponse;
import com.amazonaws.serverless.proxy.spring.SpringBootLambdaContainerHandler;
import com.amazonaws.serverless.proxy.spring.SpringBootProxyHandlerBuilder;
import com.amazonaws.services.lambda.runtime.Context;
import com.amazonaws.services.lambda.runtime.RequestStreamHandler;
import com.google.common.base.Splitter;
import com.lukeshay.restapi.config.CORSConfiguration;
import java.io.IOException;
import java.io.InputStream;
import java.io.OutputStream;
import java.time.Instant;
import java.util.EnumSet;
import javax.servlet.DispatcherType;
import javax.servlet.FilterRegistration;
import javax.ws.rs.core.Application;

public class StreamLambdaHandler implements RequestStreamHandler {
  private static final Splitter SPLITTER = Splitter.on(',')
          .trimResults()
          .omitEmptyStrings();

  private static SpringBootLambdaContainerHandler&lt;AwsProxyRequest, AwsProxyResponse&gt; handler;

//  static {
//    try {
//      long startTime = Instant.now().toEpochMilli();
//
//      LambdaContainerHandler.getContainerConfig().setInitializationTimeout(20_000);
//
//      //      handler =
//      // SpringBootLambdaContainerHandler.getAwsProxyHandler(RestApiApplication.class);
//
//      handler =
//          new SpringBootProxyHandlerBuilder()
//              .defaultProxy()
//              .asyncInit(startTime)
//              .springBootApplication(RestApiApplication.class)
//              .profiles(""lambda"")
//              .buildAndInitialize();
//
//      // we use the onStartup method of the handler to register our custom filter
//      handler.onStartup(
//          servletContext -&gt; {
//            FilterRegistration.Dynamic registration =
//                servletContext.addFilter(""CORSConfiguration"", CORSConfiguration.class);
//            registration.addMappingForUrlPatterns(EnumSet.of(DispatcherType.REQUEST), true, ""/*"");
//          });
//    } catch (ContainerInitializationException e) {
//      e.printStackTrace();
//      throw new RuntimeException(""Could not initialize Spring Boot application"", e);
//    }
//  }

  @Override
  public void handleRequest(InputStream inputStream, OutputStream outputStream, Context context)
      throws IOException {
    if (handler == null) {
      try {
        String listOfActiveSpringProfiles = System.getenv(""SPRING_PROFILES_ACTIVE"");
        if (listOfActiveSpringProfiles != null) {
          handler = SpringBootLambdaContainerHandler.getAwsProxyHandler(Application.class,
                  splitToArray(listOfActiveSpringProfiles));
        } else {
          long startTime = Instant.now().toEpochMilli();

          LambdaContainerHandler.getContainerConfig().setInitializationTimeout(20_000);

          handler =
                  new SpringBootProxyHandlerBuilder()
                          .defaultProxy()
                          .asyncInit(startTime)
                          .springBootApplication(RestApiApplication.class)
                          .profiles(""lambda"")
                          .buildAndInitialize();
        }
      } catch (ContainerInitializationException e) {
        // if we fail here. We re-throw the exception to force another cold start
        e.printStackTrace();
        throw new RuntimeException(""Could not initialize Spring Boot application"", e);
      }
    }
    handler.proxyStream(inputStream, outputStream, context);

    outputStream.flush();
    outputStream.close();
  }

  private static String[] splitToArray(String activeProfiles) {
    return SPLITTER.splitToList(activeProfiles).toArray(new String[0]);
  }
}
</code></pre>

<pre class=""lang-java prettyprint-override""><code>package com.lukeshay.restapi.security;

import com.lukeshay.restapi.jwt.JwtService;
import com.lukeshay.restapi.jwt.JwtServiceImpl;
import com.lukeshay.restapi.session.SessionService;
import com.lukeshay.restapi.session.SessionServiceImpl;
import com.lukeshay.restapi.user.User;
import com.lukeshay.restapi.user.UserRepository;
import io.jsonwebtoken.Claims;
import io.jsonwebtoken.ExpiredJwtException;
import java.io.IOException;
import javax.servlet.FilterChain;
import javax.servlet.ServletContext;
import javax.servlet.ServletException;
import javax.servlet.http.HttpServletRequest;
import javax.servlet.http.HttpServletResponse;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.security.authentication.AuthenticationManager;
import org.springframework.security.authentication.UsernamePasswordAuthenticationToken;
import org.springframework.security.core.Authentication;
import org.springframework.security.core.context.SecurityContextHolder;
import org.springframework.security.web.authentication.www.BasicAuthenticationFilter;
import org.springframework.web.context.WebApplicationContext;
import org.springframework.web.context.support.WebApplicationContextUtils;

public class JwtAuthorizationFilter extends BasicAuthenticationFilter {

  private static Logger LOG = LoggerFactory.getLogger(JwtAuthorizationFilter.class.getName());

  private UserRepository userRepository;
  private JwtService jwtService;
  private SessionService sessionService;

  public JwtAuthorizationFilter(
      AuthenticationManager authenticationManager, UserRepository userRepository) {
    super(authenticationManager);
    this.userRepository = userRepository;
  }

  @Override
  protected void doFilterInternal(
      HttpServletRequest request, HttpServletResponse response, FilterChain chain)
      throws IOException, ServletException {

    if (jwtService == null) {
      ServletContext servletContext = request.getSession().getServletContext();
      WebApplicationContext webApplicationContext =
          WebApplicationContextUtils.getWebApplicationContext(servletContext);  // ERROR HERE 

      jwtService = webApplicationContext.getBean(JwtServiceImpl.class);
      sessionService = webApplicationContext.getBean(SessionServiceImpl.class);
    }

    String header = request.getHeader(SecurityProperties.JWT_HEADER_STRING);

    if (header == null || !header.startsWith(SecurityProperties.TOKEN_PREFIX)) {
      chain.doFilter(request, response);
      return;
    }

    Authentication authentication = getUsernamePasswordAuthentication(request, response);
    SecurityContextHolder.getContext().setAuthentication(authentication);

    chain.doFilter(request, response);
  }

  private Authentication getUsernamePasswordAuthentication(
      HttpServletRequest request, HttpServletResponse response) {

    String jwtToken =
        request
            .getHeader(SecurityProperties.JWT_HEADER_STRING)
            .replace(SecurityProperties.TOKEN_PREFIX, """");

    if (jwtToken.trim().length() == 0) {
      return null;
    }

    Claims jwtClaims;

    try {
      jwtClaims = jwtService.parseJwtToken(jwtToken);
    } catch (ExpiredJwtException expiredJwtException) {

      Claims refreshClaims;

      try {
        String refreshToken =
            request
                .getHeader(SecurityProperties.REFRESH_HEADER_STRING)
                .replace(SecurityProperties.TOKEN_PREFIX, """");

        refreshClaims = jwtService.parseJwtToken(refreshToken);
        jwtClaims = expiredJwtException.getClaims();

        if (refreshClaims.getId().equals(jwtClaims.getId())
            &amp;&amp; refreshClaims.getSubject().equals(SecurityProperties.REFRESH_HEADER_STRING)) {

          String newJwtToken = jwtService.buildToken(jwtClaims);

          response.addHeader(
              SecurityProperties.JWT_HEADER_STRING, SecurityProperties.TOKEN_PREFIX + newJwtToken);
        }
      } catch (ExpiredJwtException | NullPointerException ignored) {
        LOG.debug(""No refresh token present or refresh token is expired."");
        jwtClaims = null;
      }
    }

    UsernamePasswordAuthenticationToken authentication = null;

    if (jwtClaims != null &amp;&amp; jwtClaims.getSubject().equals(SecurityProperties.JWT_HEADER_STRING)) {
      User user = userRepository.findById(jwtClaims.getId()).orElse(null);

      if (user == null) {
        return null;
      }

      UserPrincipal principal = new UserPrincipal(user);

      LOG.debug(""This user token is being created."");

      authentication =
          new UsernamePasswordAuthenticationToken(principal, null, principal.getAuthorities());
    }

    return authentication;
  }
}
</code></pre>

<p>For more context, my source code can be found here: <a href=""https://github.com/LukeShay/route-rating-rest-api/tree/switch-to-lambda"" rel=""nofollow noreferrer"">https://github.com/LukeShay/route-rating-rest-api/tree/switch-to-lambda</a></p>
",11842525.0,,11842525.0,,2020-03-26 14:36:57,2020-03-26 14:36:57,AWS Lambda handler for Spring Boot is not passing ServletContext,<java><spring><amazon-web-services><spring-boot><aws-lambda>,0,3,,,,CC BY-SA 4.0,When triggering my Spring Boot application on an AWS Lambda  I keep getting the error   I am unsure why this is happening and would like some more information  Here is the stack trace:  My source code can be found here:  Yes I know running Spring Boot on a lambda is dumb  but it is temporary  So  I dont have to pay for an EC2 instance  This is also not refined yet so I know there will be other things I should change  EDIT: Here is the relevant code This first one is the lambda handler  I originally had the commented out code but switched to the other because the start time was substantially faster  Both methods give the same error  The second one is the class where the error is being encountered  which is my auth filter  I added the comment  to the second snippet so it is easier to see the exact location of where the exception is being thrown    For more context  my source code can be found here:  
60894096,1,,,2020-03-27 21:04:12,,1,377,"<p>I'm working on AWS Lambda with the java SDK provided by AWS. I need to get an object from an S3 bucket.
I use an S3Client for it (with apache HTTP as custom configuration)
Times are acceptable.</p>

<p>Now I'm looking to replace my S3Client by an S3AsyncClient (netty).
I use custom configuration as explained in the AWS documentation.</p>

<p>I ve defined a StopWatch to count the full lambda processing time.
I can see some improvement :</p>

<p>total time: 254.4 ms (from my StopWatch)</p>

<p>BUT AWS adds +/- 2250 ms of billing time!</p>

<p>16:34:22
END RequestId: 3a8a2313-f3c7-4933-85a3-2f18e8876364
16:34:22
REPORT RequestId: 3a8a2313-f3c7-4933-85a3-2f18e8876364 Duration: 2460.98 ms Billed Duration: 2500 ms </p>

<p>So I pay a lot more!</p>

<p>Where does this difference come from?
I think that some netty/executor threads keep in WAITING STATE for while then AWS kills them ... But not sure about it ...</p>

<p>I have configured my executor service.
At the end for my lambda I try to stop all threads by using an executor.shutdown();
But It doesn't change anything...</p>

<p>As S3AsyncClient the extends AutoCloseable I 've defined it in a try with resources statement.</p>

<p>I precise that it occurs at both cold / hot start. </p>

<p>Need some help to understand this behaviour ...?</p>

<p>Thank you</p>
",7681004.0,,7681004.0,,2020-03-28 21:51:20,2020-03-28 21:51:20,S3Client vs S3AsyncClient,<multithreading><amazon-web-services><aws-lambda><netty><aws-billing>,0,0,,,,CC BY-SA 4.0,Im working on AWS Lambda with the java SDK provided by AWS  I need to get an object from an S3 bucket  I use an S3Client for it (with apache HTTP as custom configuration) Times are acceptable  Now Im looking to replace my S3Client by an S3AsyncClient (netty)  I use custom configuration as explained in the AWS documentation  I ve defined a StopWatch to count the full lambda processing time  I can see some improvement : total time: 254 4 ms (from my StopWatch) BUT AWS adds +/- 2250 ms of billing time  16:34:22 END RequestId: 3a8a2313-f3c7-4933-85a3-2f18e8876364 16:34:22 REPORT RequestId: 3a8a2313-f3c7-4933-85a3-2f18e8876364 Duration: 2460 98 ms Billed Duration: 2500 ms  So I pay a lot more  Where does this difference come from  I think that some netty/executor threads keep in WAITING STATE for while then AWS kills them     But not sure about it     I have configured my executor service  At the end for my lambda I try to stop all threads by using an executor shutdown(); But It doesnt change anything    As S3AsyncClient the extends AutoCloseable I ve defined it in a try with resources statement  I precise that it occurs at both cold / hot start   Need some help to understand this behaviour      Thank you 
46939619,1,,,2017-10-25 18:27:07,,0,30,"<p>I have an endpoint in my api that supports writes.  The resource in question is collaborative, so it is reasonable to expect that there will be parallel write requests arriving concurrently.</p>

<p>If the number of writes is small, then this is relatively straight forward to do with a simple lambda - read the current state, compute the new state, compare and swap, spin until the swap succeeds or until we give up.  In either case, we compute the appropriate http response and return it to the caller.</p>

<p>If the API is successful, then eventually the waste of conflicting writes becomes expensive enough to address.</p>

<p>It looks as though the natural response is to copy the requests into a queue, with a function that consumes batches; within each batch, we process the requests in sequence, storing the new write, and computing the appropriate response to the request.</p>

<p>What are the options for getting those computed responses copied into the http responses, and what are the trade offs to be be considered?</p>

<p>My sense is that in handling the http request, after (synchronously) enqueue the message, I need to block/poll on <em>something</em> that will eventually be populated with the response to the request.</p>
",54734.0,,,,,2017-10-25 23:06:06,How do I close the loop on batched writes in AWS?,<aws-lambda><aws-api-gateway><amazon-sqs>,1,0,,,,CC BY-SA 3.0,I have an endpoint in my api that supports writes   The resource in question is collaborative  so it is reasonable to expect that there will be parallel write requests arriving concurrently  If the number of writes is small  then this is relatively straight forward to do with a simple lambda - read the current state  compute the new state  compare and swap  spin until the swap succeeds or until we give up   In either case  we compute the appropriate http response and return it to the caller  If the API is successful  then eventually the waste of conflicting writes becomes expensive enough to address  It looks as though the natural response is to copy the requests into a queue  with a function that consumes batches; within each batch  we process the requests in sequence  storing the new write  and computing the appropriate response to the request  What are the options for getting those computed responses copied into the http responses  and what are the trade offs to be be considered  My sense is that in handling the http request  after (synchronously) enqueue the message  I need to block/poll on something that will eventually be populated with the response to the request  
48418581,1,,,2018-01-24 09:08:34,,1,1011,"<p>I've been messing around with serverless and postgresql. It seems that connection pooling is possible, but when I declared a connection pool to my postgresql instance outside:</p>

<pre><code>var pool = new pg.Pool(config);
</code></pre>

<p>Not calling <code>pool.end()</code> at the end of request handlers seem to cause <code>lambda-local</code> to not terminate when I call it.</p>

<p>If I call <code>pool.end()</code> lambda-local does terminate, but I wonder if this means that the function will stop working?</p>

<p>If I don't call pool.end(), will the function run forever on AWS, costing me a lot of money?</p>
",277941.0,,,,,2020-08-19 20:59:16,connection pooling and lambda termination,<node.js><connection-pooling><serverless-framework><serverless>,1,0,,,,CC BY-SA 3.0,Ive been messing around with serverless and postgresql  It seems that connection pooling is possible  but when I declared a connection pool to my postgresql instance outside:  Not calling  at the end of request handlers seem to cause  to not terminate when I call it  If I call  lambda-local does terminate  but I wonder if this means that the function will stop working  If I dont call pool end()  will the function run forever on AWS  costing me a lot of money  
60994174,1,,,2020-04-02 14:38:42,,0,73,"<p>I have a script copying a daily backup file to Blob storage using azcopy. On the Blob objects I want to create a lifecycle policy, like GFS, for the backups. Then I want to move older data (perhaps the yearly backup files) to colder storage automatically. Then I need to charge my client per GB storage and need a monthly report, maybe an e-mail, to our finance department with the month's maximum storage GB value. I will develop this and need guidance on where to start. Please point me in the right direction. As cheap and serverless as possible. I will answer my own question with the scripts etc to share the knowledge. Thanks!</p>
",579523.0,,,,,2020-04-02 14:41:05,How to run scheduled serverless lifecycle script on objects in Azure Blob storage,<azure><backup><azure-blob-storage><serverless>,1,0,,,,CC BY-SA 4.0,I have a script copying a daily backup file to Blob storage using azcopy  On the Blob objects I want to create a lifecycle policy  like GFS  for the backups  Then I want to move older data (perhaps the yearly backup files) to colder storage automatically  Then I need to charge my client per GB storage and need a monthly report  maybe an e-mail  to our finance department with the months maximum storage GB value  I will develop this and need guidance on where to start  Please point me in the right direction  As cheap and serverless as possible  I will answer my own question with the scripts etc to share the knowledge  Thanks  
66245987,1,,,2021-02-17 16:26:39,,3,607,"<p>I am creating a design for my eCommerce application. It will have multiple services backed by AWS Lambdas. Orderservice, InventoryService, PaymentService, LoggingService, dashboardService are some of those services. I cannot give the exact number of services but it will surely be more than 15. As per this link <a href=""https://microservices.io/patterns/apigateway.html"" rel=""nofollow noreferrer"">microservices </a>, a good microservices architecture should have one gateway that will route to the corresponding services. The code for my AWS ApiGateway with order Lambda function looks like below. <br/> My question is that each of Orderservice, inventoryservice, paymentservice etc can have multiple routes for get, post, delete, put. Most of them will have nested resources. In this situation should I include api routes and lambda functions for all these services within the same SAM template. If yes, wouldn't it be a monolith Template. If I need to change in any service, I have to deploy the whole template and breaks the microservice principles. Ideally, I want to deploy all these services independently and yet share the ApiGateway. Is this possible ?. If not, should I create separate ApiGateway for each service in different SAM template so they can be deployed separately. This will cause authentication, authorisation, monitoring be repeated in all gateways which again doesn't sound like way to go. <br/>
Please suggest what is the right way to do this.</p>
<pre class=""lang-yaml prettyprint-override""><code>AWSTemplateFormatVersion: &quot;2010-09-09&quot;
Transform: &quot;AWS::Serverless-2016-10-31&quot;
Description: ApiGateway for the Ecommerce 

Resources:
  EcomApi:
    Type: AWS::Serverless::Api
    Properties:
      DefinitionBody:
        swagger: &quot;2.0&quot;
        info:
          title: Ecommerce Api
        schemes:
          - &quot;https&quot;
        x-amazon-apigateway-request-validator: Validate body and params
        paths:
          /order:
            get:
              summary: Get the orders
              consumes:
                - &quot;application/json&quot;
              produces:
                - &quot;application/json&quot;
              responses:
                &quot;200&quot;:
                  description: &quot;200 response&quot;
                  schema:
                    $ref: &quot;#/definitions/Empty&quot;
                  headers:
                    Access-Control-Allow-Origin:
                      type: &quot;string&quot;
              security:
                - Auth: []
              x-amazon-apigateway-integration:
                type: &quot;AWS_PROXY&quot;
                httpMethod: &quot;POST&quot;
                uri: !Sub arn:aws:apigateway:${AWS::Region}:lambda:path/2015-03-31/functions/${GetOrders.Arn}/invocations
                responses:
                  default:
                    statusCode: &quot;200&quot;
                    responseParameters:
                      method.response.header.Access-Control-Allow-Origin: &quot;'*'&quot;
                passthroughBehavior: &quot;when_no_match&quot;
                contentHandling: &quot;CONVERT_TO_TEXT&quot;


  GetOrders:
    Type: AWS::Serverless::Function
    Properties:
      CodeUri: .
      Handler: src/handlers/getOrders.handler
      Role:
        Fn::GetAtt:
          - TestRole
          - Arn
      Events:
        List:
          Type: Api
          Properties:
            Path: /orders
            Method: get
            RestApiId: !Ref EcomApi

</code></pre>
",1592389.0,,1592389.0,,2021-02-18 02:42:36,2021-02-18 02:42:36,AWS single ApiGateway with multiple services,<amazon-web-services><aws-lambda><microservices><amazon-cloudformation><aws-api-gateway>,1,0,1.0,,,CC BY-SA 4.0,I am creating a design for my eCommerce application  It will have multiple services backed by AWS Lambdas  Orderservice  InventoryService  PaymentService  LoggingService  dashboardService are some of those services  I cannot give the exact number of services but it will surely be more than 15  As per this link   a good microservices architecture should have one gateway that will route to the corresponding services  The code for my AWS ApiGateway with order Lambda function looks like below   My question is that each of Orderservice  inventoryservice  paymentservice etc can have multiple routes for get  post  delete  put  Most of them will have nested resources  In this situation should I include api routes and lambda functions for all these services within the same SAM template  If yes  wouldnt it be a monolith Template  If I need to change in any service  I have to deploy the whole template and breaks the microservice principles  Ideally  I want to deploy all these services independently and yet share the ApiGateway  Is this possible    If not  should I create separate ApiGateway for each service in different SAM template so they can be deployed separately  This will cause authentication  authorisation  monitoring be repeated in all gateways which again doesnt sound like way to go   Please suggest what is the right way to do this   
66247419,1,,,2021-02-17 17:55:11,,1,173,"<p>I'e read AWS Lambda documentation for Quarkus ad it looks pretty interesting.</p>
<p>I'd like to deploy on AWS some Quarkus native lambda as explained here <a href=""https://aws.amazon.com/blogs/architecture/field-notes-optimize-your-java-application-for-aws-lambda-with-quarkus/"" rel=""nofollow noreferrer"">https://aws.amazon.com/blogs/architecture/field-notes-optimize-your-java-application-for-aws-lambda-with-quarkus/</a>, my goal is to leverage Java ecosystem power (and not have JS NPM based Lambda) and still having very low cold and warm execution time, in order to lower AWS billing.</p>
<p>I do not want to code the logic who maps the requests on the right &quot;controller's&quot; method, like this :</p>
<pre><code>[...]
   switch (httpMethod) {

            case &quot;GET&quot;:
                Map&lt;String, String&gt; queryStringParameters = request.getQueryStringParameters();

                String userId = null;

                if (pathParameters != null)
                    userId = pathParameters.get(&quot;userId&quot;);
                else if (queryStringParameters != null)
                    userId = queryStringParameters.get(&quot;userId&quot;);

                if (userId == null || userId.length() == 0) {
                    LOGGER.info(&quot;Getting all users&quot;);
                    userList = userService.findAll();
                    LOGGER.info(&quot;GET: &quot; + userList);
                    try {
                        result = mapper.writeValueAsString(userList);
                    } catch (JsonProcessingException exc) {
                        LOGGER.error(exc);
                    }
[...]
</code></pre>
<p>Too expensive to code and error prone, it's a show stopper for me</p>
<p>Rather I'd like to leverage a mechanism such as the one described here <a href=""https://aws.amazon.com/blogs/opensource/java-apis-aws-lambda/"" rel=""nofollow noreferrer"">https://aws.amazon.com/blogs/opensource/java-apis-aws-lambda/</a> : long story short the Lambda Handler maps the Lambda input data (Stream, Context, etc..) to a Http Request and the JAX-RS implementation does the rest of the work, or something similar, responsibilities are not completely clear to me.</p>
<p>This is very interesting because you can continue write your controllers as always, with annotation and all the stuff and your lambda stays very simple and just maps the objects :</p>
<pre><code>public void handleRequest(InputStream inputStream, OutputStream outputStream, Context context)
            throws IOException {
    handler.proxyStream(inputStream, outputStream, context);
</code></pre>
<p>Do a ProxyStream implementation exist for Jersey, Spring and other framework, do a similar ProxyImplementation exist for Quarkus JAX-RS implementation (Jackson ?)</p>
",4265390.0,,174777.0,,2021-02-17 20:52:14,2021-06-30 16:54:41,Quarkus AWS Lambda Generic Handler,<java><amazon-web-services><aws-lambda><quarkus>,1,0,,,,CC BY-SA 4.0,Ie read AWS Lambda documentation for Quarkus ad it looks pretty interesting  Id like to deploy on AWS some Quarkus native lambda as explained here   my goal is to leverage Java ecosystem power (and not have JS NPM based Lambda) and still having very low cold and warm execution time  in order to lower AWS billing  I do not want to code the logic who maps the requests on the right controllers method  like this :  Too expensive to code and error prone  its a show stopper for me Rather Id like to leverage a mechanism such as the one described here  : long story short the Lambda Handler maps the Lambda input data (Stream  Context  etc  ) to a Http Request and the JAX-RS implementation does the rest of the work  or something similar  responsibilities are not completely clear to me  This is very interesting because you can continue write your controllers as always  with annotation and all the stuff and your lambda stays very simple and just maps the objects :  Do a ProxyStream implementation exist for Jersey  Spring and other framework  do a similar ProxyImplementation exist for Quarkus JAX-RS implementation (Jackson  ) 
61040234,1,,,2020-04-05 09:03:29,,0,57,"<p>Is there a way to get current status from an AWS account either through AWS cli or programmatically?</p>

<p>I am conducting some research in relation to FaaS and would like to get and log the cost after each experiment but haven't been able to find a solution for this. </p>

<p>Would really appreciate if somebody could point me in the right direction here :)</p>

<p>Thanks </p>
",8733126.0,,5535604.0,,2020-04-05 14:59:22,2020-04-05 14:59:22,Programmatically get cloud spending/balance for AWS Lambda,<amazon-web-services><aws-lambda><faas>,1,1,,,,CC BY-SA 4.0,Is there a way to get current status from an AWS account either through AWS cli or programmatically  I am conducting some research in relation to FaaS and would like to get and log the cost after each experiment but havent been able to find a solution for this   Would really appreciate if somebody could point me in the right direction here :) Thanks  
63929609,1,,,2020-09-17 00:26:39,,1,1555,"<p>I'm trying to build a small site that gets its data from a database (currently I use Firebase's Cloud Firestore).</p>
<p>I've build it using next.js and thought to host it on vercel. It looks very nice and was working well.</p>
<p>However, the site needs to handle ~1000 small documents - serve, search, and rarely update. In order to reduce calls to the database on every request, which is costly both in time, and in database pricing, I thought it would be better if the server could get the full list of item when it starts (or on the first request), and then hold them in memory and make data request get the data from its memory.</p>
<p>It worked well in the local dev server, but when I deployed it to <code>vercel</code>, it didn't work. It seems <a href=""https://github.com/vercel/next.js/issues/9397#issuecomment-555307503"" rel=""nofollow noreferrer"">it forces me to work in serverless mode</a>, where each request is separate, and I can't use a common in-memory cache to get the data.</p>
<p>Am I missing something and there is a way to achieve something like that with <code>next.js</code> on <code>vercel</code>?</p>
<p>If not, can you recommend other free cloud services that can provide what I'm looking for?</p>
",46635.0,,,,,2020-09-25 07:17:18,How to cache data in next.js server on vercel?,<caching><next.js><in-memory><vercel>,1,1,1.0,,,CC BY-SA 4.0,Im trying to build a small site that gets its data from a database (currently I use Firebases Cloud Firestore)  Ive build it using next js and thought to host it on vercel  It looks very nice and was working well  However  the site needs to handle ~1000 small documents - serve  search  and rarely update  In order to reduce calls to the database on every request  which is costly both in time  and in database pricing  I thought it would be better if the server could get the full list of item when it starts (or on the first request)  and then hold them in memory and make data request get the data from its memory  It worked well in the local dev server  but when I deployed it to   it didnt work  It seems   where each request is separate  and I cant use a common in-memory cache to get the data  Am I missing something and there is a way to achieve something like that with  on   If not  can you recommend other free cloud services that can provide what Im looking for  
61091659,1,61092974.0,,2020-04-08 00:49:02,,1,1927,"<p>I've been reading some articles regarding this topic and have preliminary thoughts as what I should do with it, but still want to see if anyone can share comments if you have more experience with running machine learning on AWS. I was doing a project for a professor at school, and we decided to use AWS. I need to find a cost-effective and efficient way to deploy a forecasting model on it. </p>

<p>What we want to achieve is:</p>

<ul>
<li>read the data from S3 bucket monthly (there will be new data coming in every month), </li>
<li>run a few python files (.py) for custom-built packages and install dependencies (including the files, no more than 30kb), </li>
<li>produce predicted results into a file back in S3 (JSON or CSV works), or push to other endpoints (most likely to be some BI tools - tableau etc.) - but really this step can be flexible (not web for sure) </li>
</ul>

<p><strong>First thought I have is AWS sagemaker</strong>. However, we'll be using ""fb prophet"" model to predict the results, and we built a customized package to use in the model, therefore, I don't think the notebook instance is gonna help us. (Please correct me if I'm wrong) My understanding is that sagemaker is a environment to build and train the model, but we already built and trained the model. Plus, we won't be using AWS pre-built models anyways.</p>

<p>Another thing is if we want to use custom-built package, we will need to create container image, and I've never done that before, not sure about the efforts to do that.</p>

<p><strong>2nd option is to create multiple lambda functions</strong></p>

<ul>
<li><p>one that triggers to run the python scripts from S3 bucket (2-3 .py files) every time a new file is imported into S3 bucket, which will happen monthly.</p></li>
<li><p>one that trigger after the python scripts are done running and produce results and save into S3 bucket.</p></li>
</ul>

<p>3rd option will combine both options:
 - Use lambda function to trigger the implementation on the python scripts in S3 bucket when the new file comes in.
 - Push the result using sagemaker endpoint, which means we host the model on sagemaker and deploy from there.</p>

<p>I am still not entirely sure how to put pre-built model and python scripts onto sagemaker instance and host from there.</p>

<p>I'm hoping whoever has more experience with AWS service can help give me some guidance, in terms of more cost-effective and efficient way to run model.</p>

<p>Thank you!! </p>
",12725603.0,,,,,2020-04-10 11:42:00,Should I run forecast predictive model with AWS lambda or sagemaker?,<amazon-web-services><amazon-s3><aws-lambda><amazon-sagemaker><facebook-prophet>,2,0,,,,CC BY-SA 4.0,Ive been reading some articles regarding this topic and have preliminary thoughts as what I should do with it  but still want to see if anyone can share comments if you have more experience with running machine learning on AWS  I was doing a project for a professor at school  and we decided to use AWS  I need to find a cost-effective and efficient way to deploy a forecasting model on it   What we want to achieve is:  read the data from S3 bucket monthly (there will be new data coming in every month)   run a few python files ( py) for custom-built packages and install dependencies (including the files  no more than 30kb)   produce predicted results into a file back in S3 (JSON or CSV works)  or push to other endpoints (most likely to be some BI tools - tableau etc ) - but really this step can be flexible (not web for sure)   First thought I have is AWS sagemaker  However  well be using fb prophet model to predict the results  and we built a customized package to use in the model  therefore  I dont think the notebook instance is gonna help us  (Please correct me if Im wrong) My understanding is that sagemaker is a environment to build and train the model  but we already built and trained the model  Plus  we wont be using AWS pre-built models anyways  Another thing is if we want to use custom-built package  we will need to create container image  and Ive never done that before  not sure about the efforts to do that  2nd option is to create multiple lambda functions  one that triggers to run the python scripts from S3 bucket (2-3  py files) every time a new file is imported into S3 bucket  which will happen monthly  one that trigger after the python scripts are done running and produce results and save into S3 bucket   3rd option will combine both options:  - Use lambda function to trigger the implementation on the python scripts in S3 bucket when the new file comes in   - Push the result using sagemaker endpoint  which means we host the model on sagemaker and deploy from there  I am still not entirely sure how to put pre-built model and python scripts onto sagemaker instance and host from there  Im hoping whoever has more experience with AWS service can help give me some guidance  in terms of more cost-effective and efficient way to run model  Thank you    
61098504,1,61157443.0,,2020-04-08 10:27:34,,1,116,"<p>Before I even start, I'll say that I was not 100% sure whether SO is the appropriate SX for this question. Let me know if I should ask this on some other SX.  </p>

<p>The question is about FaaS in general, but if you can better explain this in a context of a particular FaaS platform/provider, that's great as well.  </p>

<p>I'm currently reading up on serverless computing (FaaS to be more specific) and trying to get myself somewhat comfortable with the subject. 
Now almost everywhere I turn, I encounter the following statements about FaaS:<br>
1) Most FaaS platforms support down-to-zero scaling;<br>
2) FaaS providers charge their users based on their function execution time (usually measured in ms);<br>
3) Potential cold starts (i.e., creating a new instance instead of reusing an existing one) are an issue in FaaS as they considerably degrade performance of your application;</p>

<p>Points 1 and 2 are considered benefits - you get exactly what you need (including nothing at all, if applicable) andy pay for exactly what you get.<br>
Point 3 is considered a drawback - the request takes considerably more time to complete. I've seen authors describing cold starts as a sign of FaaS platforms not yet being mature. I've seen practitioners saying that they set up periodic requests just to keep their functions from becoming inactive and ""going under"" thus triggering the cold start the next time it's called.</p>

<p>My question is - <strong>why are cold starts viewed as undesirable instead of as a trade-off</strong>?<br>
What I mean is, considering that the user pays for execution time in FaaS, wouldn't it usually be in their best interests to avoid having warm, but idle function instances? To me it seems like a cost vs high availibility decision. Do I misunderstand something? Does having a warm, but idle function instance does not count towards one's execution time? Even if so:<br>
a) isn't it undesirable from the providers' perspective (having to allocate resources that are neither used nor paid for)?<br>
b) sending periodic requests (as mentioned above) surely does cost you, right?</p>
",9158370.0,,,,,2020-04-11 13:22:08,Cold start vs scaling to zero trade-off in FaaS,<cloud><devops><serverless><faas>,1,0,,,,CC BY-SA 4.0,Before I even start  Ill say that I was not 100% sure whether SO is the appropriate SX for this question  Let me know if I should ask this on some other SX    The question is about FaaS in general  but if you can better explain this in a context of a particular FaaS platform/provider  thats great as well    Im currently reading up on serverless computing (FaaS to be more specific) and trying to get myself somewhat comfortable with the subject   Now almost everywhere I turn  I encounter the following statements about FaaS: 1) Most FaaS platforms support down-to-zero scaling; 2) FaaS providers charge their users based on their function execution time (usually measured in ms); 3) Potential cold starts (i e   creating a new instance instead of reusing an existing one) are an issue in FaaS as they considerably degrade performance of your application; Points 1 and 2 are considered benefits - you get exactly what you need (including nothing at all  if applicable) andy pay for exactly what you get  Point 3 is considered a drawback - the request takes considerably more time to complete  Ive seen authors describing cold starts as a sign of FaaS platforms not yet being mature  Ive seen practitioners saying that they set up periodic requests just to keep their functions from becoming inactive and going under thus triggering the cold start the next time its called  My question is - why are cold starts viewed as undesirable instead of as a trade-off  What I mean is  considering that the user pays for execution time in FaaS  wouldnt it usually be in their best interests to avoid having warm  but idle function instances  To me it seems like a cost vs high availibility decision  Do I misunderstand something  Does having a warm  but idle function instance does not count towards ones execution time  Even if so: a) isnt it undesirable from the providers perspective (having to allocate resources that are neither used nor paid for)  b) sending periodic requests (as mentioned above) surely does cost you  right  
63936826,1,,,2020-09-17 11:13:05,,0,438,"<p>Here's my thing:
I got 2 functions A &amp; B:</p>
<ul>
<li>A is in a default Lambda VPC, provided by AWS, that has open internet access
inbound and outbound</li>
<li>B is in a specific VPC with a RDS DB, with inbound access but no
Internet access (no NAT Gateway).</li>
</ul>
<p>The process is this :</p>
<ul>
<li>A sends data to B (via API Gateway) while B inserts that data in a RDS Database</li>
<li>B is supposed to send confirmation (via another API Gateway) of whether it's okay or not but A keeps getting a 502 error (timeout I guess).</li>
</ul>
<p>At first I thought it was a stupid Lambda proxy output format error but now I realised it's more serious than that.
I'm looking for some easy/cheap way out (i'm a student on a budget and this is a PoC). Is there some kind of easy solution to this problem?</p>
",10437727.0,,,,,2020-09-17 11:34:53,How to have 2 Lambda functions communicate with each other.. when one of them doesn't have Internet access?,<amazon-web-services><aws-lambda><amazon-vpc>,1,3,,,,CC BY-SA 4.0,Heres my thing: I got 2 functions A &amp; B:  A is in a default Lambda VPC  provided by AWS  that has open internet access inbound and outbound B is in a specific VPC with a RDS DB  with inbound access but no Internet access (no NAT Gateway)   The process is this :  A sends data to B (via API Gateway) while B inserts that data in a RDS Database B is supposed to send confirmation (via another API Gateway) of whether its okay or not but A keeps getting a 502 error (timeout I guess)   At first I thought it was a stupid Lambda proxy output format error but now I realised its more serious than that  Im looking for some easy/cheap way out (im a student on a budget and this is a PoC)  Is there some kind of easy solution to this problem  
61112628,1,,,2020-04-09 01:36:56,,1,36,"<p>Imagine I need a temporary DynamoDB table (created from the latest BACKUP) to run a performance test and as soon as the test is finished I would DROP/REMOVE that table (and its underlying storage).</p>

<p>Let's say that test required 1TB of storage and ran for 4 hours. How much would I be charged FOR THE STORAGE?</p>

<p>Would I have to pay for the whole month (720 hours) or for the fraction (4/720) of the time I actually used the STORAGE? </p>

<p>Cheers</p>
",1005234.0,,,,,2020-04-09 01:36:56,DynamodDB - Pay per hour/minute (STORAGE COST),<amazon-web-services><amazon-dynamodb><serverless>,0,0,,,,CC BY-SA 4.0,Imagine I need a temporary DynamoDB table (created from the latest BACKUP) to run a performance test and as soon as the test is finished I would DROP/REMOVE that table (and its underlying storage)  Lets say that test required 1TB of storage and ran for 4 hours  How much would I be charged FOR THE STORAGE  Would I have to pay for the whole month (720 hours) or for the fraction (4/720) of the time I actually used the STORAGE   Cheers 
63940867,1,64011266.0,,2020-09-17 15:00:00,,0,137,"<p>I have a JAVA 8  AWS lambda function that has some pretty expensive setup when the container is first spun up. It must make calls to pull various credentials/cacerts. I would like to cache this set up (the output of which is an SSLContext object that is used for making calls to another api).</p>
<p>I have not had to do this before, and my question that I cannot seem to find an answer for is this:</p>
<p>Are there any issues reusing the SSLContext object over and over again while the Lambda Container is alive? this could be 15 minutes or 5 hours, or 2 days, etc.. as long as there is traffic coming through it, it will be alive.</p>
<p>None of the credentials will change, and the SSLContext object would be identical between all invocations.</p>
<p>Do SSLContext objects have a TTL? The code to create the SSLConext is fairly boilerplate. This method is called after I have done the expensive pulls to get the certs/cred and I want to cache this SSLContext object:</p>
<pre><code>public SSLContext getContext(){
        KeyStore clientStore = KeyStore.getInstance(KEY_INSTANCE);
        keyStoreInputstream = //GET STREAM
        clientStore.load(keyStoreInputstream, caCertCred.toCharArray());

        KeyManagerFactory kmf = KeyManagerFactory.getInstance(KeyManagerFactory.getDefaultAlgorithm());
        kmf.init(clientStore, KEY.toCharArray());
        KeyManager[] kms = kmf.getKeyManagers();

        trustStoreInputStream =  //GET STREAM
        KeyStore trustStore = KeyStore.getInstance(TRUST_INSTANCE);
        trustStore.load(trustStoreInputStream, caCertCred.toCharArray());

        TrustManagerFactory tmf = TrustManagerFactory.getInstance(TrustManagerFactory.getDefaultAlgorithm());
        tmf.init(trustStore);
        TrustManager[] tms = tmf.getTrustManagers();

        SSLContext sslContext = SSLContext.getInstance(&quot;TLS&quot;);
        sslContext.init(kms, tms, new SecureRandom());
        return sslContext;
</code></pre>
<p>}</p>
",,user8072194,,,,2020-09-22 13:53:37,Reuse SSLContext object in AWS Lambda Environment,<java><aws-lambda><sslcontext>,1,1,,,,CC BY-SA 4.0,I have a JAVA 8  AWS lambda function that has some pretty expensive setup when the container is first spun up  It must make calls to pull various credentials/cacerts  I would like to cache this set up (the output of which is an SSLContext object that is used for making calls to another api)  I have not had to do this before  and my question that I cannot seem to find an answer for is this: Are there any issues reusing the SSLContext object over and over again while the Lambda Container is alive  this could be 15 minutes or 5 hours  or 2 days  etc   as long as there is traffic coming through it  it will be alive  None of the credentials will change  and the SSLContext object would be identical between all invocations  Do SSLContext objects have a TTL  The code to create the SSLConext is fairly boilerplate  This method is called after I have done the expensive pulls to get the certs/cred and I want to cache this SSLContext object:  } 
63946207,1,63954773.0,,2020-09-17 21:09:03,,1,683,"<p>Hi I'm looking for a way to store user session/metadata with the least amount of latency and that will not cost me an arm and a leg.</p>
<p>Brief problem description.</p>
<p>I have a bot that helps users download files from Google Drive.</p>
<p>It uses a Webhook of an AWS lambda function.</p>
<p>Users are provided with clickable filenames, e.g.</p>
<p>/File.pdf</p>
<p>Once they click on it, it needs to be downloaded and sent to the user.</p>
<p>The problem is I need a way of knowing what file the user chose without having to use a database or iterating through all my files by name.</p>
<p><strong>E.g.</strong> Is there a way of adding metadata to the clickable message? Such that I can add that metadata to the clickable and if a user clicks /File.pdf, I'll be able to extract the metadata.</p>
",6076761.0,,3722635.0,,2020-09-18 11:21:04,2020-09-24 08:29:02,Telegram add and retrieve metadata from message,<aws-lambda><telegram><telegram-bot>,1,0,,,,CC BY-SA 4.0,Hi Im looking for a way to store user session/metadata with the least amount of latency and that will not cost me an arm and a leg  Brief problem description  I have a bot that helps users download files from Google Drive  It uses a Webhook of an AWS lambda function  Users are provided with clickable filenames  e g  /File pdf Once they click on it  it needs to be downloaded and sent to the user  The problem is I need a way of knowing what file the user chose without having to use a database or iterating through all my files by name  E g  Is there a way of adding metadata to the clickable message  Such that I can add that metadata to the clickable and if a user clicks /File pdf  Ill be able to extract the metadata  
63954863,1,,,2020-09-18 11:26:28,,1,78,"<p>In legacy AWS Lambda node.js project there is a <code>publish</code> property in <code>serverless.yml</code>.</p>
<pre class=""lang-yaml prettyprint-override""><code>service:
  name: service-name
  publish: false
</code></pre>
<p>I didn't pay attention to it before, but after upgrading <code>serverless</code> to v.2.0.0 I get the warning on <code>serverless deploy</code>:</p>
<pre><code>Serverless: Configuration warning at 'service': unrecognized property 'publish'
</code></pre>
<p>It seems this property has become deprecated, but what did it do? Is it safe to remove it from <code>serverless.yml</code>?</p>
",3872362.0,,3872362.0,,2020-09-18 11:40:24,2020-09-18 11:40:24,'publish' property in serverless config,<serverless>,0,0,,,,CC BY-SA 4.0,In legacy AWS Lambda node js project there is a  property in    I didnt pay attention to it before  but after upgrading  to v 2 0 0 I get the warning on :  It seems this property has become deprecated  but what did it do  Is it safe to remove it from   
66269219,1,,,2021-02-18 22:39:55,,1,92,"<p>The question is mostly self-explanatory.
If I run <code>aws s3 cp &lt;s3_bucket&gt; &lt;ec2_host&gt;</code> locally (assuming both the bucket and the instance are on the same region), does the data go from s3 to ec2 internally (in which case there are no data transfer costs), or does it go first from the bucket to my computer, and only then to the instance (in which case there are s3 download data transfer costs)?</p>
<p>Thanks a lot.</p>
",5640923.0,,,,,2021-02-19 01:18:16,Will there be data transfer fees using aws s3 cp (locally) to move data from my S3 bucket to my EC2 instance?,<amazon-web-services><amazon-s3><amazon-ec2><aws-lambda><aws-cli>,2,1,,,,CC BY-SA 4.0,The question is mostly self-explanatory  If I run  locally (assuming both the bucket and the instance are on the same region)  does the data go from s3 to ec2 internally (in which case there are no data transfer costs)  or does it go first from the bucket to my computer  and only then to the instance (in which case there are s3 download data transfer costs)  Thanks a lot  
61548553,1,61561780.0,,2020-05-01 18:32:54,,2,569,"<p>Our institution is trying to make an automatic grading platform set up for an introductory class in java, and we were hoping to use an AWS lambda function for the grading. The problem is, like any sane person, our dev team far prefers python over java, so the grader itself is written in python, but the student programs are written in java.</p>

<p>Since the grader has to call the student functions, it would be best if we could have a lambda runtime that has binaries for both python and java. Does anyone else know how we could go about doing that?</p>

<p>As a side note, it would be possible to use a separate function to run the java files and return the output to the python function, but since amazon charges per function execution, this would be less than ideal.</p>
",13355577.0,,,,,2020-05-02 15:24:55,AWS lambda with both python and java language support,<java><python><automation><aws-lambda>,1,0,,,,CC BY-SA 4.0,Our institution is trying to make an automatic grading platform set up for an introductory class in java  and we were hoping to use an AWS lambda function for the grading  The problem is  like any sane person  our dev team far prefers python over java  so the grader itself is written in python  but the student programs are written in java  Since the grader has to call the student functions  it would be best if we could have a lambda runtime that has binaries for both python and java  Does anyone else know how we could go about doing that  As a side note  it would be possible to use a separate function to run the java files and return the output to the python function  but since amazon charges per function execution  this would be less than ideal  
65458856,1,65459029.0,,2020-12-26 17:20:22,,4,290,"<p>Many AWS reference architectures for serverless real-time analytics, suggest pushing processed data from Lambda to S3 through Kinesis Firehose.</p>
<p>e.g.
<a href=""https://aws.amazon.com/blogs/big-data/create-real-time-clickstream-sessions-and-run-analytics-with-amazon-kinesis-data-analytics-aws-glue-and-amazon-athena/"" rel=""nofollow noreferrer"">https://aws.amazon.com/blogs/big-data/create-real-time-clickstream-sessions-and-run-analytics-with-amazon-kinesis-data-analytics-aws-glue-and-amazon-athena/</a></p>
<p>Why cant we push data from Lambda to S3 directly? Isn't it better to avoid complexity and additional cost by skipping the mediator Kinesis Firehose component? Is there any problem with writing real-time data by Lambda directly to S3?</p>
",4680381.0,,4680381.0,,2020-12-27 03:09:46,2020-12-27 03:09:46,"In near real time analytics, why is Lambda-->Firehose-->S3 preferred over Lambda -->S3?",<amazon-web-services><amazon-s3><aws-lambda><streaming><amazon-kinesis-firehose>,1,0,,,,CC BY-SA 4.0,Many AWS reference architectures for serverless real-time analytics  suggest pushing processed data from Lambda to S3 through Kinesis Firehose  e g   Why cant we push data from Lambda to S3 directly  Isnt it better to avoid complexity and additional cost by skipping the mediator Kinesis Firehose component  Is there any problem with writing real-time data by Lambda directly to S3  
66993650,1,,,2021-04-07 20:26:07,,0,183,"<p>I am getting the following error message from the Azure logs:</p>
<pre><code>Result: Failure Exception: KeyError: 'referer' Stack: File &quot;/azure-functions-
host/workers/python/3.7/LINUX/X64/azure_functions_worker/dispatcher.py&quot;, line 372, in 
_handle__invocation_request self.__run_sync_func, invocation_id, fi.func, args) File 
&quot;/usr/local/lib/python3.7/concurrent/futures/thread.py&quot;, line 57, in run result = self.fn(*self.args,
 **self.kwargs) File &quot;/azure-functions 
host/workers/python/3.7/LINUX/X64/azure_functions_worker/dispatcher.py&quot;, line 548, in __run_sync_func 
return func(**params) File &quot;/home/site/wwwroot/createCheckoutSession/__init__.py&quot;, line 18, in main 
logging.info(req.headers[&quot;referer&quot;]) File &quot;/azure-functions- 
host/workers/python/3.7/LINUX/X64/azure/functions/_http.py&quot;, line 27, in __getitem__ return 
self.__http_headers__[key.lower()]
</code></pre>
<p>I think the offending code is:</p>
<pre><code>logging.info(req.headers[&quot;referer&quot;])
logging.info(os.environ[&quot;stripe_api_key&quot;])
BASEURL = req.headers[&quot;referer&quot;] + &quot;#&quot;
</code></pre>
<p>Azure seems to be struggling to pull the referer from the req.headers.  It works in VScode.  I use the referer to build a redirect URL after a Stripe payment request. This redirect is obviously different in DEV, TEST and PROD.
Any ideas what I'm doing wrong.</p>
",1353324.0,,,,,2021-05-07 11:28:20,"azure python function runs locally in vscode but not in azure PAYG functon app. Issue with httpRequest.headers[""referer""]?",<python><azure><serverless>,2,0,,,,CC BY-SA 4.0,I am getting the following error message from the Azure logs:  I think the offending code is:  Azure seems to be struggling to pull the referer from the req headers   It works in VScode   I use the referer to build a redirect URL after a Stripe payment request  This redirect is obviously different in DEV  TEST and PROD  Any ideas what Im doing wrong  
65496327,1,,,2020-12-29 17:36:32,,0,215,"<p>I need to load a spacy model with more than 600MB to AWS Lambda, but I just found by downloading the model from S3 to /tmp, but it is larger than allowed (512MB), so an error occurs.</p>
<p>I also tried to use EFS and it worked, but the value was very expensive because it was necessary to configure NAT gateway.</p>
<p>Is it possible to load the model directly from the S3?</p>
<p>I have another idea that can work, which is using container no lambda, but first I would like to know if it is possible to do it with S3.</p>
",5565024.0,,,,,2020-12-29 17:36:32,Load Spacy Wordnet Model From S3,<amazon-web-services><amazon-s3><aws-lambda><spacy>,0,5,,,,CC BY-SA 4.0,I need to load a spacy model with more than 600MB to AWS Lambda  but I just found by downloading the model from S3 to /tmp  but it is larger than allowed (512MB)  so an error occurs  I also tried to use EFS and it worked  but the value was very expensive because it was necessary to configure NAT gateway  Is it possible to load the model directly from the S3  I have another idea that can work  which is using container no lambda  but first I would like to know if it is possible to do it with S3  
61587158,1,,,2020-05-04 07:29:45,,0,229,"<p>Actually i want to reduce AWS bill. I am using some of the instances in my project. I want a notification after certain time period(every 3rd/4th hour) on my login email id and shutdown on a particular time duration. So, how can i do this? </p>

<pre><code>if current time - instance start time &gt;= 3 hr (send a mail)
if current time - instance start time &gt;= 4 hr (send a mail)
if current time - instance start time &gt;= 5 hr (shut down that instance)

</code></pre>
",8735580.0,,8735580.0,,2020-05-04 08:40:00,2020-05-25 09:47:17,Notification from AWS based on instances status and instances duration,<amazon-web-services><amazon-ec2><aws-lambda><amazon-cloudwatch><amazon-sns>,3,7,0.0,,,CC BY-SA 4.0,Actually i want to reduce AWS bill  I am using some of the instances in my project  I want a notification after certain time period(every 3rd/4th hour) on my login email id and shutdown on a particular time duration  So  how can i do this    
47747326,1,,,2017-12-11 06:09:33,,2,284,"<p>I have created case for increasing service rate limit of aws lambda for handling heavy load in our application. I want to get information about whether aws take any charges monthly/one time for increasing limit?</p>
",6569232.0,,,,,2017-12-11 09:46:38,AWS Service Limit,<amazon-web-services><aws-lambda>,2,0,0.0,,,CC BY-SA 3.0,I have created case for increasing service rate limit of aws lambda for handling heavy load in our application  I want to get information about whether aws take any charges monthly/one time for increasing limit  
65501733,1,,,2020-12-30 03:40:43,,0,446,"<p>I'm developing a game using AWS Amplify. The game state will be stored in DynamoDB tables and will be queried and modified with GraphQL. There isn't a pressing need for realtime or low-latency communication; However, I need to detect when a player joins or disconnects from a game. What's the best mechanism for implementing this?</p>
<p>What I had in mind was an event that fires when a WebSocket connection is established or broken. The best I could glean from the Amplify docs was using <a href=""https://docs.amplify.aws/lib/pubsub/getting-started/q/platform/js"" rel=""nofollow noreferrer"">PubSub with AWS IoT</a>, but I don't know if this will work. If possible, I would like to avoid incurring additional API costs.</p>
<p>I already implemented a version of this where the client updates a <code>lastSeen</code> field in the database every 30 seconds or so but it felt pretty janky.</p>
",2880811.0,,,,,2020-12-30 05:20:24,How to detect AWS Amplify client disconnect from server-side?,<amazon-web-services><aws-lambda><aws-amplify><aws-appsync><aws-iot>,2,0,,,,CC BY-SA 4.0,Im developing a game using AWS Amplify  The game state will be stored in DynamoDB tables and will be queried and modified with GraphQL  There isnt a pressing need for realtime or low-latency communication; However  I need to detect when a player joins or disconnects from a game  Whats the best mechanism for implementing this  What I had in mind was an event that fires when a WebSocket connection is established or broken  The best I could glean from the Amplify docs was using   but I dont know if this will work  If possible  I would like to avoid incurring additional API costs  I already implemented a version of this where the client updates a  field in the database every 30 seconds or so but it felt pretty janky  
61590633,1,,,2020-05-04 10:58:29,,1,1260,"<p>TLDR; reading with my AWS lambda <code>doc</code>, <code>docx</code> files that are stored on S3.</p>

<p>On my local machine I just use <code>textract.process(file_path)</code> to read both doc and docx files. </p>

<p>So the intuitive way to do the same on lambda is to download the file from s3 to the local storage (<code>tmp</code>) on the lambda and then process the <code>tmp</code> files like I do on my local machine.</p>

<p>That's not cost-effective...</p>

<p>Is there a way to make a pipeline from the S3 object straight into some parser like <code>textract</code> that'll just convert the <code>doc</code>/<code>docx</code> files into a readable object like <code>string</code>?</p>

<p>My code so far for reading files like txt.</p>

<pre><code>import boto3

print('Loading function')


def lambda_handler(event, context):
    try:  # Read s3 file
        bucket_name = ""appsresults""
        download_path = 'Folder1/file1.txt'
        filename = download_path
        s3 = boto3.resource('s3')
        content_object = s3.Object(bucket_name, filename)        

        file_content = content_object.get()['Body'].read().decode('utf-8')

        print(file_content)

    except Exception as e:
        print(""Couldnt read the file from s3 because:\n {0}"".format(e))

    return event  # return event
</code></pre>
",10349423.0,,,,,2021-06-23 16:05:32,"Reading doc, docx files from s3 within lambda",<python><amazon-s3><aws-lambda><docx><doc>,2,3,0.0,,,CC BY-SA 4.0,TLDR; reading with my AWS lambda    files that are stored on S3  On my local machine I just use  to read both doc and docx files   So the intuitive way to do the same on lambda is to download the file from s3 to the local storage () on the lambda and then process the  files like I do on my local machine  Thats not cost-effective    Is there a way to make a pipeline from the S3 object straight into some parser like  thatll just convert the / files into a readable object like   My code so far for reading files like txt   
31728414,1,38064721.0,,2015-07-30 15:47:23,,58,19471,"<p>I'm looking to create a RESTful API using AWS Lambda/API Gateway connected to a MongoDB database. I've read that connections to MongoDB are relatively expensive so it's best practice to retain a connection for reuse once its been established rather than making new connections for every new query.</p>

<p>This is pretty straight forward for normal applications as you can establish a connection during start up and reuse it during the applications lifetime. But, since Lambda is designed to be stateless retaining this connection seems to be less straight forward.</p>

<p>Therefore, I'm wondering what would be the best way to approach this database connection issue? Am I forced to make new connections every time a Lambda function is invoked or is there a way to pool/cache these connections for more efficient queries?</p>

<p>Thanks.</p>
",1811211.0,,427457.0,,2018-03-01 19:16:19,2021-12-13 06:52:33,MongoDB connections from AWS Lambda,<node.js><mongodb><amazon-web-services><aws-lambda>,10,6,14.0,,,CC BY-SA 3.0,Im looking to create a RESTful API using AWS Lambda/API Gateway connected to a MongoDB database  Ive read that connections to MongoDB are relatively expensive so its best practice to retain a connection for reuse once its been established rather than making new connections for every new query  This is pretty straight forward for normal applications as you can establish a connection during start up and reuse it during the applications lifetime  But  since Lambda is designed to be stateless retaining this connection seems to be less straight forward  Therefore  Im wondering what would be the best way to approach this database connection issue  Am I forced to make new connections every time a Lambda function is invoked or is there a way to pool/cache these connections for more efficient queries  Thanks  
61605991,1,61611616.0,,2020-05-05 04:49:48,,3,1867,"<p>Do I get charged for transfer from S3 to aws Lambda?</p>

<p>For example a user uploads a file client side to my S3 bucket. My aws lambda retrieves the file to generate thumbnails. Does that count as S3 out to internet or S3 to cloudfront or something else?</p>

<p>Lambda and S3 are in the same region.</p>
",1279109.0,,174777.0,,2020-05-05 11:03:49,2020-05-05 11:03:49,S3 pricing with aws Lambda,<amazon-web-services><amazon-s3><aws-lambda>,3,0,1.0,,,CC BY-SA 4.0,Do I get charged for transfer from S3 to aws Lambda  For example a user uploads a file client side to my S3 bucket  My aws lambda retrieves the file to generate thumbnails  Does that count as S3 out to internet or S3 to cloudfront or something else  Lambda and S3 are in the same region  
47771693,1,,,2017-12-12 11:36:02,,10,3889,"<p>I am trying to implement an event-driven architecture using Amazon Kinesis as the central event log of the platform. The idea is pretty much the same to the one presented by <a href=""https://read.acloud.guru/serverless-event-sourcing-at-nordstrom-ea69bd8fb7cc"" rel=""noreferrer"">Nordstrom's with the Hello-Retail project</a>.</p>

<p>I have done similar things with Apache Kafka before, but Kinesis seems to be a cost-effective alternative to Kafka and I decided to give it a shot. I am, however, facing some challenges related to event persistence and replaying. I have two questions: </p>

<ol>
<li>Are you guys using Kinesis for such use-case OR do you recommend using it?</li>
<li>Since Kinesis is not able to retain the events forever (like Kafka does), how to handle replays from consumers?</li>
</ol>

<p>I'm currently using a lambda function (Firehose is also an option) to persist all events to Amazon S3. Then, one could read past events from the storage and then start listening to new events coming from the stream. But I'm not happy with this solution. Consumers are not be able to use Kinesis' checkpoints (Kafka's consumer offsets). Plus, Java's <a href=""https://github.com/awslabs/amazon-kinesis-client/issues/133"" rel=""noreferrer"">KCL does not support the AFTER_SEQUENCE_NUMBER yet</a>, which would be useful in such implementation.</p>
",5735967.0,,,,,2017-12-12 18:45:16,Event Sourcing with Kinesis - Replaying and Persistence,<amazon-web-services><cqrs><amazon-kinesis><event-sourcing><serverless-architecture>,1,0,1.0,,,CC BY-SA 3.0,I am trying to implement an event-driven architecture using Amazon Kinesis as the central event log of the platform  The idea is pretty much the same to the one presented by   I have done similar things with Apache Kafka before  but Kinesis seems to be a cost-effective alternative to Kafka and I decided to give it a shot  I am  however  facing some challenges related to event persistence and replaying  I have two questions:   Are you guys using Kinesis for such use-case OR do you recommend using it  Since Kinesis is not able to retain the events forever (like Kafka does)  how to handle replays from consumers   Im currently using a lambda function (Firehose is also an option) to persist all events to Amazon S3  Then  one could read past events from the storage and then start listening to new events coming from the stream  But Im not happy with this solution  Consumers are not be able to use Kinesis checkpoints (Kafkas consumer offsets)  Plus  Javas   which would be useful in such implementation  
61620005,1,,,2020-05-05 18:09:23,,-1,198,"<p>I am trying to pull budget codes out of tags on aws resources I have the following code: </p>

<pre><code>for resource in awsresources[""ResourceTagMappingList""]:
        resourcearn = resource[""ResourceARN""]
        for tags in resource[""Tags""]:
            if tags['Key'] == 'Budget':
                budgetCode = tags['Value']
                pattern = ""^[\d]{3}-[\d]{4}-[\d]{1}-[\d]{6}-[\d]{4}-[\d]{4}-[\d]{4}$""
                result = re.search(pattern, budgetCode)

                if result == None:
                    print(resourcearn + "" Contains an non-valid budget code tag.  The tag value is "" + tags['Value'] )
</code></pre>

<p>When the code runs it keep coming back as with one tag coming back as invalid 070-0702-1-000000-5309-7000-0000.  I then go and run it locally with just this code for testing and it seems to come back find and it returns it as a valid number.  Is there something I am missing why my lambda code is giving me different results? </p>

<pre><code>budgetCode = ""070-0702-1-000000-5309-7000-0000""
pattern = ""^[\d]{3}-[\d]{4}-[\d]{1}-[\d]{6}-[\d]{4}-[\d]{4}-[\d]{4}$""
result = re.search(pattern, budgetCode)

if result == None:
    print(""Not a valid budget code"") 
</code></pre>
",2479302.0,,5535604.0,,2020-05-05 21:41:02,2020-05-05 21:41:02,Getting aws tag values and compare with regex issue,<python><regex><amazon-web-services><aws-lambda>,1,1,,,,CC BY-SA 4.0,I am trying to pull budget codes out of tags on aws resources I have the following code:   When the code runs it keep coming back as with one tag coming back as invalid 070-0702-1-000000-5309-7000-0000   I then go and run it locally with just this code for testing and it seems to come back find and it returns it as a valid number   Is there something I am missing why my lambda code is giving me different results    
47804191,1,47859243.0,,2017-12-14 00:31:41,,3,700,"<p>I've seen this post here: <a href=""https://dzone.com/articles/making-spring-boot-application-run-serverless-with"" rel=""nofollow noreferrer"">https://dzone.com/articles/making-spring-boot-application-run-serverless-with</a> which gives an example of how to use Spring in a Serverless scenario, but I believe that this still involves creating the Spring context, an expensive thing to do every time a request comes in. And I am wondering if Spring, but also the traditional web application frameworks are even truely compatible with the severless model, as they all tend to assume the server is only going to initialise on start, and then not again till the server is restarted, as opposed to being immediately ready to handle a request and not needing to initialize a Spring context for instance. So then these frameworks tend to do allot of stuff in the start up phase, which is not good I believe when you don't have a server per-say, and you effectively need to start up every time your would call what would be a lambda in AWS.</p>

<p>So my question is are these traditional web frameworks, such as Spring, which perform allot of compute when starting up still applicable in the Serverless model, for instance: AWS lambda.</p>
",4033292.0,,,,,2018-05-25 12:14:27,Is Spring Compatible with Serverless Computing,<spring><aws-lambda><serverless>,2,0,,,,CC BY-SA 3.0,Ive seen this post here:  which gives an example of how to use Spring in a Serverless scenario  but I believe that this still involves creating the Spring context  an expensive thing to do every time a request comes in  And I am wondering if Spring  but also the traditional web application frameworks are even truely compatible with the severless model  as they all tend to assume the server is only going to initialise on start  and then not again till the server is restarted  as opposed to being immediately ready to handle a request and not needing to initialize a Spring context for instance  So then these frameworks tend to do allot of stuff in the start up phase  which is not good I believe when you dont have a server per-say  and you effectively need to start up every time your would call what would be a lambda in AWS  So my question is are these traditional web frameworks  such as Spring  which perform allot of compute when starting up still applicable in the Serverless model  for instance: AWS lambda  
61939294,1,,,2020-05-21 16:32:10,,0,713,"<p>Current AWS configuration which is serving 3 webapps -</p>

<p><a href=""https://i.stack.imgur.com/fxVCj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fxVCj.png"" alt=""enter image description here""></a></p>

<p>Here ELB is taking care of SSL offloading. Currently the Node JS application is deployed on a EC2 machine. It is taking care of handling the backend APIs as well as serving the static files for the 3 SPA react webapps.</p>

<p>We are looking to improve on this by separating out the frontend and backend. We are thinking of moving the 3 SPA react webapps into a single S3 bucket. Each will sit in its own directory on this S3 bucket. The S3 bucket will be configured for static website hosting. We will still have the node JS instance for service backend APIs.</p>

<p><a href=""https://i.stack.imgur.com/JPInV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JPInV.png"" alt=""enter image description here""></a></p>

<p>We are looking to avoid using cloud front to route traffic from ELB to S3 since these are internal webapps with limited number of users from a particular region. </p>

<ol>
<li>Can we use lambda function to route traffic from ELB to Node JS app on EC2 as well as from ELB to the S3 bucket? If possible is it a good practice performance wise and cost wise to take this approach?</li>
</ol>

<p>Also the lambda function has to route the traffic based on rules -</p>

<ul>
<li>/api/* --> route traffic to node js app</li>
<li>/ui/site1/* --> route traffic to S3 bucket site1 directory</li>
<li>/ui/site2/* --> route traffic to S3 bucket site2 directory</li>
<li><p>/ui/site3/* --> route traffic to S3 bucket site3 directory</p>

<ol start=""2"">
<li>Can we write such rules in lambda function?</li>
</ol></li>
</ul>
",1298824.0,,1298824.0,,2020-05-22 17:39:51,2020-05-22 17:39:51,Is it possible to use AWS lambda for routing traffic from ELB to S3 bucket?,<amazon-web-services><amazon-s3><aws-lambda>,2,7,,,,CC BY-SA 4.0,Current AWS configuration which is serving 3 webapps -  Here ELB is taking care of SSL offloading  Currently the Node JS application is deployed on a EC2 machine  It is taking care of handling the backend APIs as well as serving the static files for the 3 SPA react webapps  We are looking to improve on this by separating out the frontend and backend  We are thinking of moving the 3 SPA react webapps into a single S3 bucket  Each will sit in its own directory on this S3 bucket  The S3 bucket will be configured for static website hosting  We will still have the node JS instance for service backend APIs   We are looking to avoid using cloud front to route traffic from ELB to S3 since these are internal webapps with limited number of users from a particular region    Can we use lambda function to route traffic from ELB to Node JS app on EC2 as well as from ELB to the S3 bucket  If possible is it a good practice performance wise and cost wise to take this approach   Also the lambda function has to route the traffic based on rules -  /api/* --&gt; route traffic to node js app /ui/site1/* --&gt; route traffic to S3 bucket site1 directory /ui/site2/* --&gt; route traffic to S3 bucket site2 directory /ui/site3/* --&gt; route traffic to S3 bucket site3 directory  Can we write such rules in lambda function    
44320564,1,44321078.0,,2017-06-02 04:01:36,,3,1017,"<p>AWS Lambda seems nice for running stress tests. </p>

<p>I understand that is it should be able scale up to 1000 instances, and you are charged by 0.1s rather than per hour, which is handy for short stress tests. On the other hand, automatically scaling up gives you even less control over costs than EC2.  For development having explicit budget would be nice. I understand that Amazon doesn't allow for explicit budgets since they can bring down websites in their moment of fame. However, for development having explicit budget would be nice.</p>

<p>Is there a workaround, or best practices for managing cost of AWS Lambda services during development? (For example, reducing the maximum time per request)</p>
",3215004.0,,,,,2017-06-02 04:56:29,Limit AWS-Lambda budget,<amazon-web-services><aws-lambda>,1,0,,,,CC BY-SA 3.0,AWS Lambda seems nice for running stress tests   I understand that is it should be able scale up to 1000 instances  and you are charged by 0 1s rather than per hour  which is handy for short stress tests  On the other hand  automatically scaling up gives you even less control over costs than EC2   For development having explicit budget would be nice  I understand that Amazon doesnt allow for explicit budgets since they can bring down websites in their moment of fame  However  for development having explicit budget would be nice  Is there a workaround  or best practices for managing cost of AWS Lambda services during development  (For example  reducing the maximum time per request) 
47837082,1,,,2017-12-15 17:22:18,,0,46,"<p>I'm implementing a webhooks provider and trying to solve some problems while minimizing the added complexity to my system:</p>

<ol>
<li>Not blocking processing of the API call that triggered the event while calling all the hooks so the response to that call will not be delayed</li>
<li>Not making a flood of calls to my listeners if some client is quickly calling my APIs that trigger hooks (i.e. wait a couple seconds and throw away any earlier calls if duplicates come in later)</li>
</ol>

<p>My environment is Python (Chalice) and AWS Lambda. Ideal solution will be easy to integrate and cheap.</p>
",602543.0,,1252647.0,,2017-12-17 10:40:30,2017-12-17 11:12:13,Design for implementing web hooks (including not blocking and ignoring superseding repeat events),<python><aws-lambda><webhooks>,1,0,,,,CC BY-SA 3.0,Im implementing a webhooks provider and trying to solve some problems while minimizing the added complexity to my system:  Not blocking processing of the API call that triggered the event while calling all the hooks so the response to that call will not be delayed Not making a flood of calls to my listeners if some client is quickly calling my APIs that trigger hooks (i e  wait a couple seconds and throw away any earlier calls if duplicates come in later)  My environment is Python (Chalice) and AWS Lambda  Ideal solution will be easy to integrate and cheap  
66360653,1,,,2021-02-25 00:09:07,,0,490,"<p>For a CRUD application what would be the best approach to design AWS lambda function:</p>
<ol>
<li>Create a single lambda function and differentiate each call with a switch case in the handler.</li>
<li>Create a single lambda function but with a separate handler for each call. Is it possible even?</li>
<li>Create a separate lambda function for each operation.</li>
</ol>
<p>As these are just simple CRUD operations, size is not a big factor. But I am bit concerns with the managing and cost-effectiveness.</p>
",404333.0,,,,,2021-02-25 04:09:37,What is the best approach to design AWS lambda function for CRUD operation,<amazon-web-services><aws-lambda><aws-sam>,1,3,,2021-03-01 23:48:41,,CC BY-SA 4.0,For a CRUD application what would be the best approach to design AWS lambda function:  Create a single lambda function and differentiate each call with a switch case in the handler  Create a single lambda function but with a separate handler for each call  Is it possible even  Create a separate lambda function for each operation   As these are just simple CRUD operations  size is not a big factor  But I am bit concerns with the managing and cost-effectiveness  
65623434,1,65623530.0,,2021-01-08 04:24:18,,2,87,"<p>I need to know how can I run docker image in aws one time on some event(file upload).</p>
<p>For example:
I uploaded files to S3 and then I need to run my docker image one time. I know that I can do something like that with ESC tasks, but in this case I will have constantly running EC2, it's too expensive.</p>
<p>How to run docker image once on every file upload and shutdown it after its running?</p>
<p>P.S. docker image should have at least 8GB memory for working</p>
",14963370.0,,,,,2021-01-08 04:37:26,How to run docker image one time in aws on some event and shut down it after that?,<amazon-web-services><docker><aws-lambda>,1,0,,,,CC BY-SA 4.0,I need to know how can I run docker image in aws one time on some event(file upload)  For example: I uploaded files to S3 and then I need to run my docker image one time  I know that I can do something like that with ESC tasks  but in this case I will have constantly running EC2  its too expensive  How to run docker image once on every file upload and shutdown it after its running  P S  docker image should have at least 8GB memory for working 
44347243,1,,,2017-06-03 18:26:50,,1,273,"<p>I currently have a functional <code>Oauth Authentication Provider</code> implemented with <code>Spring Security Oauth</code> running on an <code>AWS ECS cluster</code>, but I would like to come up with a <code>server-less</code> implementation using <code>AWS Lambda</code> with <a href=""https://github.com/awslabs/serverless-application-model"" rel=""nofollow noreferrer"">AWS SAM</a>.  I am aware that there are wonderful SaaS providers such as <code>Auth0</code>, but the price for the thousands of user accounts we will require is prohibitively expensive.</p>

<p>I have discovered <a href=""https://github.com/danilop/LambdAuth"" rel=""nofollow noreferrer"">danilop/LambdaAuth</a>, which is useful, but not <code>OAuth</code>.  For now, I only require a <code>client_credentials</code> grant, which seems very simple.  But I would like to be able to leverage some <code>library</code> which will make it easier to implement more grants and flows for the future.  I have investigated <code>python's</code> <a href=""https://pypi.python.org/pypi/oauth2"" rel=""nofollow noreferrer"">oauth2</a> and <a href=""https://pypi.python.org/pypi/oauthlib"" rel=""nofollow noreferrer"">oauthlib</a> libraries, but the documentation appears to be useless for my requirements.</p>

<p>Does anyone have any advice on how to proceed?</p>
",1253613.0,,1253613.0,,2017-06-04 15:23:04,2017-06-04 15:23:04,Any advice on Implementing an Oauth Provider with AWS Lambda?,<python><oauth-2.0><aws-lambda><aws-api-gateway><serverless-architecture>,0,2,,,,CC BY-SA 3.0,I currently have a functional  implemented with  running on an   but I would like to come up with a  implementation using  with    I am aware that there are wonderful SaaS providers such as   but the price for the thousands of user accounts we will require is prohibitively expensive  I have discovered   which is useful  but not    For now  I only require a  grant  which seems very simple   But I would like to be able to leverage some  which will make it easier to implement more grants and flows for the future   I have investigated   and  libraries  but the documentation appears to be useless for my requirements  Does anyone have any advice on how to proceed  
48474979,1,,,2018-01-27 10:47:58,,9,6447,"<p>I'm developing a stocks app and have to keep users browser updated with pricing changes</p>

<p>I don't need to access past data, browser just have to get current data whenever it changes</p>

<p>is it possible to filter a dynamodb stream and expose an endpoint (behind api gateway) that could be used with a javascript EventSource?</p>
",888361.0,,8372062.0,,2018-01-28 00:37:12,2020-01-09 22:49:48,Is any aws service suitable for sending real time updates to browser?,<amazon-web-services><aws-lambda><server-sent-events><eventsource>,2,0,1.0,,,CC BY-SA 3.0,Im developing a stocks app and have to keep users browser updated with pricing changes I dont need to access past data  browser just have to get current data whenever it changes is it possible to filter a dynamodb stream and expose an endpoint (behind api gateway) that could be used with a javascript EventSource  
44376491,1,44379306.0,,2017-06-05 19:41:53,,0,1020,"<p>Let's imagine situation like that:</p>

<p>We have node.js app, which is rendering view on server-side and sends html to browser. In generated html we have few static assets (like images, stylesheets etc.).</p>

<h2>Why should I (or not) choose S3 over Lambda to serve this content?</h2>

<p>Here are pros &amp; cons which I see:</p>

<h2>Performance</h2>

<p>I was quite sure that providing content from S3 is much more faster then from Lambda (there is no script which need to be executed)...</p>

<p>...Until I performed some tests (file size ~44kB) average of 10 requests:</p>

<ul>
<li>API GW + S3: 285ms</li>
<li>API GW + Lambda: 290ms</li>
<li>S3: 135ms</li>
</ul>

<p>As you can see there is no difference between providing content from Lambda via API GW then from S3. The only significant difference is between direct link to s3 and two previous tests.</p>

<p>Lambda 1 : S3 1</p>

<h2>Cost</h2>

<p>And here Lambda wins definetely.</p>

<p>First of all we have free triage of 1 000 000 requests,
Second here pricing comes:</p>

<ul>
<li>S3: $0.004 per 10,000 requests</li>
<li>Lambda: around 0,002000624 per 10,000 requests:</li>
</ul>

<p><em>($0.20 per 1 million requests + $0.000000208$ per every 100ms)</em></p>

<p>So in pricing Lambda wins.</p>

<h1>Summarizing</h1>

<p>My observations shows that Lambda is better way to serve even static content (speed is similar to S3, and pricing is twice cheaper).</p>

<p>Is there anything what I am missing?</p>
",2849613.0,,,,,2019-03-21 20:35:56,Why should(n't) I use S3 instead Lambda for static content,<amazon-web-services><amazon-s3><aws-lambda>,2,7,,,,CC BY-SA 3.0,Lets imagine situation like that: We have node js app  which is rendering view on server-side and sends html to browser  In generated html we have few static assets (like images  stylesheets etc )  Why should I (or not) choose S3 over Lambda to serve this content  Here are pros &amp; cons which I see: Performance I was quite sure that providing content from S3 is much more faster then from Lambda (there is no script which need to be executed)       Until I performed some tests (file size ~44kB) average of 10 requests:  API GW + S3: 285ms API GW + Lambda: 290ms S3: 135ms  As you can see there is no difference between providing content from Lambda via API GW then from S3  The only significant difference is between direct link to s3 and two previous tests  Lambda 1 : S3 1 Cost And here Lambda wins definetely  First of all we have free triage of 1 000 000 requests  Second here pricing comes:  S3: $0 004 per 10 000 requests Lambda: around 0 002000624 per 10 000 requests:  ($0 20 per 1 million requests + $0 000000208$ per every 100ms) So in pricing Lambda wins  Summarizing My observations shows that Lambda is better way to serve even static content (speed is similar to S3  and pricing is twice cheaper)  Is there anything what I am missing  
32079113,1,,,2015-08-18 17:38:01,,2,886,"<p>I'm evaluating some scenarios for my needs with media extraction from mp4 video files to mp3 using AWS lambda approach. </p>

<p>The main requirement is once a new mp4 file is available or saved in some S3 bucket by a custom application the Lambda function will be triggered and send metadata to Dynamo DB and then the lambda function will extract the audio and store it in another S3 bucket.</p>

<p>There are available four options as described below to design the Lambda function:</p>

<ol>
<li><p>For example, use Java plus JAVE encoder library to do the job (with
embed  ffmpeg binary in the lib/jar)</p></li>
<li><p>Use NodeJS with some npm package with media encode capacity
(including spawn ffmpeg process)</p></li>
<li><p>Use NodeJS with AWS Elastic Transcode Service </p></li>
<li><p>Use Java with AWS Elastic Transcode Service</p></li>
</ol>

<p>Which of option above will result in less cost in terms of resource (memory/cpu usage tiers)?</p>

<p>Im guessing the two initial scenarios sound more amateur but could be less expensive. But I have doubt if in terms of memory and CPU resources usage choosing between NodeJS  or Java could affect the amount lost because of memory or the time execution of a lambda function will take to to the task. </p>

<p>I need care about these aspects it is desirable to spend the less amount as possible with AWS Lambda for this kind of task?</p>

<p>Doesn't matter choosing between NodeJs or Java? </p>

<p>Should I consider use the Elastic Transcode Service instead a custom library in lambda function?</p>

<p>Thanks for your help.</p>
",220175.0,,7666972.0,,2017-10-04 13:45:30,2017-10-04 13:45:30,Choosing AWS Lambda runtime option in terms of memory usage/cost for media encoding,<java><node.js><amazon-web-services><lambda><aws-lambda>,2,0,,,,CC BY-SA 3.0,Im evaluating some scenarios for my needs with media extraction from mp4 video files to mp3 using AWS lambda approach   The main requirement is once a new mp4 file is available or saved in some S3 bucket by a custom application the Lambda function will be triggered and send metadata to Dynamo DB and then the lambda function will extract the audio and store it in another S3 bucket  There are available four options as described below to design the Lambda function:  For example  use Java plus JAVE encoder library to do the job (with embed  ffmpeg binary in the lib/jar) Use NodeJS with some npm package with media encode capacity (including spawn ffmpeg process) Use NodeJS with AWS Elastic Transcode Service  Use Java with AWS Elastic Transcode Service  Which of option above will result in less cost in terms of resource (memory/cpu usage tiers)  Im guessing the two initial scenarios sound more amateur but could be less expensive  But I have doubt if in terms of memory and CPU resources usage choosing between NodeJS  or Java could affect the amount lost because of memory or the time execution of a lambda function will take to to the task   I need care about these aspects it is desirable to spend the less amount as possible with AWS Lambda for this kind of task  Doesnt matter choosing between NodeJs or Java   Should I consider use the Elastic Transcode Service instead a custom library in lambda function  Thanks for your help  
48630799,1,48634081.0,,2018-02-05 20:17:48,,19,7252,"<p>I have a Lambda that is generating and returning a value. This value can expire. Therefore I need to check the values validity before returning.
As generating is quite expensive (taken from another service) I'd like to store the value somehow.</p>

<p>What is the best practice for storing those 2 values (timestamp and a corresponding value)?</p>

<ul>
<li>DynamoDB, but using a database service for 2 values seems to be a lot of overhead. There will never be more items; The same entry will only get updated.</li>
<li>I thought about S3, but this would also imply creating a S3-Bucket and storing one object containing the information, only for this 2 values (but probably the most ""lean"" way?)</li>
<li>Would love to update Lambdas configuration in order to update the environment variables (but even if this is possible, its probably no best practice?! Also not sure about inconsistencies with Lambda runtimes...)</li>
</ul>

<p>Whats best practice here? Whats the way to go in terms of performance?</p>
",1410677.0,,,,,2020-06-07 17:35:09,Best practice to store single value in AWS Lambda,<amazon-web-services><amazon-s3><aws-lambda><amazon-dynamodb><serverless>,4,2,,,,CC BY-SA 3.0,I have a Lambda that is generating and returning a value  This value can expire  Therefore I need to check the values validity before returning  As generating is quite expensive (taken from another service) Id like to store the value somehow  What is the best practice for storing those 2 values (timestamp and a corresponding value)   DynamoDB  but using a database service for 2 values seems to be a lot of overhead  There will never be more items; The same entry will only get updated  I thought about S3  but this would also imply creating a S3-Bucket and storing one object containing the information  only for this 2 values (but probably the most lean way ) Would love to update Lambdas configuration in order to update the environment variables (but even if this is possible  its probably no best practice   Also not sure about inconsistencies with Lambda runtimes   )  Whats best practice here  Whats the way to go in terms of performance  
67017488,1,67025762.0,,2021-04-09 08:16:43,,0,93,"<p>i would like to implement a lambda in aws which receives as input pixel coordinates (x/y), retrieve that pixel's RGB from one image, and then do something with it.
the catch now is that the image is very large: 21600x10800 pixels (a 684MB tif file).
Many of the image's pixels will likely never be accessed (its a world map so it includes e.g. oceans, for which no lambda calls will happen. But i don't know which pixels will be needed.)
The result of the lambda will be persisted so that the image operation is only done once per pixel.</p>
<p>My main concern is that i would like to avoid large unnecessary processing time and costs. I expect multiple calls per second of the lambda. The naive way would be to throw the image into an s3 bucket, then read it in the lambda to get one pixel - but i would think that then each lambda invoke would become very heavy. I could do some custom solution such as storing the rows separately but was wondering if there is some set of technologies that handles it more elegant.
Right now i am using Node.js 14.x but that's not a strong requirement.
the image is in tif format but i could convert it to another image format beforehand if needed. (just not to the answer of the lambda as that is even bigger)</p>
<p>How can i efficiently design this lambda?</p>
",15589484.0,,,,,2021-04-09 17:31:07,efficiently get single pixels from large images in aws lambda,<node.js><amazon-web-services><image-processing><aws-lambda>,2,6,,,,CC BY-SA 4.0,i would like to implement a lambda in aws which receives as input pixel coordinates (x/y)  retrieve that pixels RGB from one image  and then do something with it  the catch now is that the image is very large: 21600x10800 pixels (a 684MB tif file)  Many of the images pixels will likely never be accessed (its a world map so it includes e g  oceans  for which no lambda calls will happen  But i dont know which pixels will be needed ) The result of the lambda will be persisted so that the image operation is only done once per pixel  My main concern is that i would like to avoid large unnecessary processing time and costs  I expect multiple calls per second of the lambda  The naive way would be to throw the image into an s3 bucket  then read it in the lambda to get one pixel - but i would think that then each lambda invoke would become very heavy  I could do some custom solution such as storing the rows separately but was wondering if there is some set of technologies that handles it more elegant  Right now i am using Node js 14 x but thats not a strong requirement  the image is in tif format but i could convert it to another image format beforehand if needed  (just not to the answer of the lambda as that is even bigger) How can i efficiently design this lambda  
48660663,1,,,2018-02-07 09:51:14,,0,70,"<p>I'm using <code>OkHttp</code> 3.9 to make a <code>POST</code> call in an Android app to update an <code>AWS DynamoDb database</code> by triggering a Lambda function via an Api Gateway. I notice that the database is updated 3 times with each call. Apparently this is due to the default retry option for OkHttp. </p>

<p>As I understand it, the retries can be prevented by setting retryOnConnectionFailure to false when building the client.I tried this but still the database is updated 3 times - so the call is still being made 3 times.</p>

<p>Some suggest to handle this behaviour on my server. The problem is that if I handle the issue in the Lambda function, then it means that the api has been called three times and so has the lambda function, all unecessary overhead and cost. Also, if setting retryOnConnectionFailure to false worked and the api was only called once, it means that there is no mechanism to handle failure.</p>

<p>So, why does it retry 3 times even when each call succeeds? and most importantly, how do I stop this from happening so that the api is only called once and then again only if the call failed (i.e to succeed in triggering the lambda function)? Setting <code>retryOnConnectionFailure</code> to <code>false</code> seems to have no effect.</p>
",4391398.0,,8268794.0,,2018-02-07 11:35:04,2018-02-07 11:35:04,OkHttp post retries corrupts DynamoDb database,<android><aws-lambda><aws-api-gateway><okhttp3>,1,3,,,,CC BY-SA 3.0,Im using  3 9 to make a  call in an Android app to update an  by triggering a Lambda function via an Api Gateway  I notice that the database is updated 3 times with each call  Apparently this is due to the default retry option for OkHttp   As I understand it  the retries can be prevented by setting retryOnConnectionFailure to false when building the client I tried this but still the database is updated 3 times - so the call is still being made 3 times  Some suggest to handle this behaviour on my server  The problem is that if I handle the issue in the Lambda function  then it means that the api has been called three times and so has the lambda function  all unecessary overhead and cost  Also  if setting retryOnConnectionFailure to false worked and the api was only called once  it means that there is no mechanism to handle failure  So  why does it retry 3 times even when each call succeeds  and most importantly  how do I stop this from happening so that the api is only called once and then again only if the call failed (i e to succeed in triggering the lambda function)  Setting  to  seems to have no effect  
66430023,1,,,2021-03-01 21:35:28,,0,125,"<p>Just wondering if I am able to trigger a lambda function with a tweet from a specific user.</p>
<p>My plan is to use the lambda function to send out a pinpoint notification about the tweet that triggered the function.</p>
<p>I'm aware that you are able to link the two via Zapier but you require a premium membership, so was wondering if there is a free method or cheaper.</p>
<p>Any help is greatly appreciated.</p>
",15040580.0,,,,,2021-03-01 22:37:18,Is there any way to trigger an AWS lambda function with a Tweet from a specific User,<amazon-web-services><twitter><aws-lambda><zapier>,1,0,,,,CC BY-SA 4.0,Just wondering if I am able to trigger a lambda function with a tweet from a specific user  My plan is to use the lambda function to send out a pinpoint notification about the tweet that triggered the function  Im aware that you are able to link the two via Zapier but you require a premium membership  so was wondering if there is a free method or cheaper  Any help is greatly appreciated  
48711787,1,,,2018-02-09 18:11:08,,0,557,"<p>I'm making a web app using AWS lambda.  Everything is working great and my cold start requests are under 200ms.  Go has absolutely blazing speed.  But there is an issue with user registration and auth.</p>

<p>Computing a bcrypt hash takes 19 seconds with 128 mb of reserved memory.  Increasing the memory does take the request time down but increases the cost per transaction.</p>

<p>For only two of my calls needing the increased space this seems pretty wasteful.  I'm considering making a separate endpoint for just auth.  They share a DB so would not matter to much.</p>

<p>Is there a better way?
Different auth scheme?
Different infrastructure?</p>
",2581581.0,,,,,2018-02-09 18:11:08,Issue with timeouts using bcrypt on AWS lambda,<go><aws-lambda>,0,6,,,,CC BY-SA 3.0,Im making a web app using AWS lambda   Everything is working great and my cold start requests are under 200ms   Go has absolutely blazing speed   But there is an issue with user registration and auth  Computing a bcrypt hash takes 19 seconds with 128 mb of reserved memory   Increasing the memory does take the request time down but increases the cost per transaction  For only two of my calls needing the increased space this seems pretty wasteful   Im considering making a separate endpoint for just auth   They share a DB so would not matter to much  Is there a better way  Different auth scheme  Different infrastructure  
62641547,1,,,2020-06-29 15:31:27,,3,2649,"<p>Building an app to be launched in production - and unsure how to handle the dev/production environments on AWS.</p>
<p>If I use multiple buckets, multiple DynamoDB tables, multiple Lambda functions, multiple Elastic Search instances, EC2, API gateway - it seems SUPER cumbersome to have a production and a dev environment?</p>
<p>Currently there is only ONE environment, and once the app goes live - any changes will be changing the production environment.</p>
<p>So how do you handle two environments on AWS? The only way I can think of - is to make copies of every lambda function, every database, every EC2 instance, every API and bucket.... But that would cost literally double the price and be super tedious to update once going live.</p>
<p>Any suggestions?</p>
",5923408.0,,13830732.0,,2020-06-30 10:33:23,2020-06-30 10:33:23,How do I handle development and production environments in AWS?,<amazon-web-services><amazon-s3><amazon-ec2><aws-lambda><amazon-elastic-beanstalk>,2,0,2.0,2020-06-29 16:05:06,,CC BY-SA 4.0,Building an app to be launched in production - and unsure how to handle the dev/production environments on AWS  If I use multiple buckets  multiple DynamoDB tables  multiple Lambda functions  multiple Elastic Search instances  EC2  API gateway - it seems SUPER cumbersome to have a production and a dev environment  Currently there is only ONE environment  and once the app goes live - any changes will be changing the production environment  So how do you handle two environments on AWS  The only way I can think of - is to make copies of every lambda function  every database  every EC2 instance  every API and bucket     But that would cost literally double the price and be super tedious to update once going live  Any suggestions  
32274228,1,,,2015-08-28 15:08:12,,27,9148,"<p>I have lambda function and dynamo db table in the same region (us-east-1). In lambda function I perform very simple query:</p>

<pre><code>params =
  TableName: 'users'
  Item:
    email:
      S: event.body.email
  ConditionExpression: 'attribute_not_exists (email)'
dynamodb.putItem(params, context.done)
</code></pre>

<p>There are only few rows in DynamoDB table, there is Hash Key on email and Read/Write throughtputs are set to 5/5.</p>

<p>Lambda function exeutes in ~4 seconds... This is very slow. Am I doing something wrong?</p>

<hr>

<p>I've tested my function with different memory settings for lambda function (it was set to 128mb previously):</p>

<ul>
<li>256mb => ~2000ms</li>
<li>512mb => ~1000ms</li>
<li>1024mb => ~500ms</li>
<li>1536mb => ~300ms</li>
</ul>

<p>So it seems that response time depends 1-1 on memory (well in fact on compute capacity as AWS scales it along with memory). Still this is crazy because to make very simple REST API I have to set 1536mb memory to make it ""responsive"" while my program uses 17mb!</p>

<hr>

<p>Hmm on the other hand I've calculated that it will cost:</p>

<ul>
<li>8.32$ per 1 milion 4000ms requests using 128mb memory</li>
<li>10.004$ per 1 milion 300ms requests using 1536mb memory</li>
</ul>

<p>So it's not so bad I guess...</p>
",606521.0,,606521.0,,2015-08-28 15:51:52,2015-08-28 20:41:45,Very slow requests to dynamodb from lambda function,<amazon-web-services><amazon-dynamodb><aws-lambda>,1,5,9.0,,,CC BY-SA 3.0,I have lambda function and dynamo db table in the same region (us-east-1)  In lambda function I perform very simple query:  There are only few rows in DynamoDB table  there is Hash Key on email and Read/Write throughtputs are set to 5/5  Lambda function exeutes in ~4 seconds    This is very slow  Am I doing something wrong   Ive tested my function with different memory settings for lambda function (it was set to 128mb previously):  256mb =&gt; ~2000ms 512mb =&gt; ~1000ms 1024mb =&gt; ~500ms 1536mb =&gt; ~300ms  So it seems that response time depends 1-1 on memory (well in fact on compute capacity as AWS scales it along with memory)  Still this is crazy because to make very simple REST API I have to set 1536mb memory to make it responsive while my program uses 17mb   Hmm on the other hand Ive calculated that it will cost:  8 32$ per 1 milion 4000ms requests using 128mb memory 10 004$ per 1 milion 300ms requests using 1536mb memory  So its not so bad I guess    
48758001,1,,,2018-02-13 01:02:52,,0,23,"<p>Im writing a remote application, controlled by a server. The client would be some sort of daemon thats pretty much always on. The thing is  these remote commands are unpredictable and sparse. The server could go hours or days without sending a message, or it could send several messages in an hour.</p>

<p>I have no experience with networking, so Im not sure how all this works and I just need pointers for where to look.</p>

<p>Whats the best, most efficient (cheapest) way to do this? Id be using AWS for all of this.</p>

<p>The first option I thought of, was to store in a database the IPs of all the clients associated with their user ID. When a AWS Lambda function is called, it makes a new connection to the IP associated with that user id, and sends the message, then closes the connection as the lambda function exits.</p>

<p>The second option was to host an EC2 instance, and actively keep alive connections to all the users. But this would require hosting the EC2 24/7 with potentially a lot of clients, and could get very expensive.</p>

<p>Im not sure what best practice is here, or even what protocols to look into for that kind of thing. For example, on the first option, how would the server connect to the client? Wouldnt it have to port forward because of firewalls or something?</p>

<p>Again, I dont have any experience with network programming so Ill take all the pointers I can get as to how this is generally accomplished.</p>

<p>Thanks!</p>
",7098058.0,,,,,2018-02-13 01:02:52,Efficiently broadcasting infrequent messages to clients,<networking><amazon-ec2><notifications><aws-lambda>,0,3,,,,CC BY-SA 3.0,Im writing a remote application  controlled by a server  The client would be some sort of daemon thats pretty much always on  The thing is  these remote commands are unpredictable and sparse  The server could go hours or days without sending a message  or it could send several messages in an hour  I have no experience with networking  so Im not sure how all this works and I just need pointers for where to look  Whats the best  most efficient (cheapest) way to do this  Id be using AWS for all of this  The first option I thought of  was to store in a database the IPs of all the clients associated with their user ID  When a AWS Lambda function is called  it makes a new connection to the IP associated with that user id  and sends the message  then closes the connection as the lambda function exits  The second option was to host an EC2 instance  and actively keep alive connections to all the users  But this would require hosting the EC2 24/7 with potentially a lot of clients  and could get very expensive  Im not sure what best practice is here  or even what protocols to look into for that kind of thing  For example  on the first option  how would the server connect to the client  Wouldnt it have to port forward because of firewalls or something  Again  I dont have any experience with network programming so Ill take all the pointers I can get as to how this is generally accomplished  Thanks  
62663403,1,62672056.0,,2020-06-30 17:54:17,,3,423,"<p>I have a python script which copy files from one S3 bucket to another S3 bucket. This script needs to be run every Sunday at some specific time. I was reading some of articles and answers, So I tried to use AWS lambda + Cloudwatch events. This files run for minimum 30 minutes. would it be still good with Lambda as Lambda can run max 15 minutes only. Or is there any other way? I can create an EC2 box and run it as a Cron but that would be expensive. Or any other standard way?</p>
",9128435.0,,,,,2020-07-01 07:47:36,Python Script as a Cron on AWS S3 buckets,<amazon-web-services><amazon-s3><amazon-ec2><aws-lambda>,4,1,0.0,,,CC BY-SA 4.0,I have a python script which copy files from one S3 bucket to another S3 bucket  This script needs to be run every Sunday at some specific time  I was reading some of articles and answers  So I tried to use AWS lambda + Cloudwatch events  This files run for minimum 30 minutes  would it be still good with Lambda as Lambda can run max 15 minutes only  Or is there any other way  I can create an EC2 box and run it as a Cron but that would be expensive  Or any other standard way  
67104220,1,,,2021-04-15 07:38:17,,0,13,"<p>We are developing an application in which we are using serverless architecture. Here each and every action invokes a lambda function the result of which is logged in to cloudwatch.
Now we have a requirement that the logs need to be available in the application and the data should be able to be filtered using different options along with custom search options.
We are looking for a cost effection AWS solutions.</p>
",13711484.0,,,,,2021-04-15 07:38:17,Lambda Logs To show in Application Interface with search option,<aws-lambda>,0,2,,,,CC BY-SA 4.0,We are developing an application in which we are using serverless architecture  Here each and every action invokes a lambda function the result of which is logged in to cloudwatch  Now we have a requirement that the logs need to be available in the application and the data should be able to be filtered using different options along with custom search options  We are looking for a cost effection AWS solutions  
49352907,1,49354832.0,,2018-03-18 21:19:42,,2,513,"<p>My use case is a Lambda function behind API Gateway. I use API as a proxy with /{proxy+} path and method ANY. I want to secure my Lambda function so, that only authorized users can execute it and perform only allowed actions using this function. I know, that the way to go is to create custom authorizer. But I think this approach is pretty ineffective. </p>

<p>Two cons:</p>

<ol>
<li><p>Every time my API is used, two Lambda functions will be executed instead of one: authorizer and main Lambda . Both Lambdas in my case will connect to database. Authorizer - to verify user and decide what policy to return. Main Lambda - to perform its task. It will cost extra money and reduce the speed of my app.</p></li>
<li><p>Authorizer can only decide, whether user is allowed to execute my function or not. It cannot decide whether user is allowed to perform some specific action by the Lambda function. So my access logic will be splitted: part of it will be in authorizer and another part, more specific, in my Lambda. Not very good.</p></li>
</ol>

<p>Isn't it better not to use custom authorizer in my case and just pass all requests to my main Lambda, which will decide whether user is authorized or not? In the latter it will just send response with code 401 (Unauthorized).</p>

<p>I understand that custom authorizer is a more universal approach, because it allows to protect all types of API integrations besides Lambda (HTTP, Mock, etc.), but in my case it is Lambda function.</p>
",1858818.0,,1574023.0,,2018-09-12 07:09:43,2018-09-12 07:09:43,Is custom authorizer really an optimal way to secure AWS Lambda function?,<amazon-web-services><aws-lambda>,1,1,,,,CC BY-SA 4.0,My use case is a Lambda function behind API Gateway  I use API as a proxy with /{proxy+} path and method ANY  I want to secure my Lambda function so  that only authorized users can execute it and perform only allowed actions using this function  I know  that the way to go is to create custom authorizer  But I think this approach is pretty ineffective   Two cons:  Every time my API is used  two Lambda functions will be executed instead of one: authorizer and main Lambda   Both Lambdas in my case will connect to database  Authorizer - to verify user and decide what policy to return  Main Lambda - to perform its task  It will cost extra money and reduce the speed of my app  Authorizer can only decide  whether user is allowed to execute my function or not  It cannot decide whether user is allowed to perform some specific action by the Lambda function  So my access logic will be splitted: part of it will be in authorizer and another part  more specific  in my Lambda  Not very good   Isnt it better not to use custom authorizer in my case and just pass all requests to my main Lambda  which will decide whether user is authorized or not  In the latter it will just send response with code 401 (Unauthorized)  I understand that custom authorizer is a more universal approach  because it allows to protect all types of API integrations besides Lambda (HTTP  Mock  etc )  but in my case it is Lambda function  
67124114,1,,,2021-04-16 11:09:33,,1,280,"<p>I'm running a backend app with several endpoints on Cloud Run(fully-managed). My endpoints are publicly available by its nature so I don't want to authenticate users through my client app hosted on Netlify.</p>
<p>What I do need is to restrict access to my endpoints so that other applications or malicious users can't abuse it. It is not about scaling, I just don't want to exceed the <a href=""https://cloud.google.com/free/docs/gcp-free-tier#free-tier-usage-limits"" rel=""nofollow noreferrer"">Free Tier limits</a> since it is a demo of an opensource application.</p>
<p>I've already set the <a href=""https://cloud.google.com/run/docs/about-concurrency#concurrency_values"" rel=""nofollow noreferrer"">concurrency</a> and <a href=""https://cloud.google.com/run/quotas#cloud_run_limits"" rel=""nofollow noreferrer"">max instance</a> limits to minimum but this alone is not enough. There is also a product named <a href=""https://cloud.google.com/armor"" rel=""nofollow noreferrer"">Google Cloud Armor</a> but it seems an expensive one, not free.</p>
<p>I was expecting to have a simple built-in solution for this but couldn't find it.</p>
<p>What other solutions do I have? How can I block the traffic coming out of my website on Netlify?</p>
",1581921.0,,1581921.0,,2021-04-17 06:49:42,2021-04-17 09:28:37,Restricting access to public Cloud Run endpoints,<google-cloud-platform><serverless><google-cloud-run>,1,2,,,,CC BY-SA 4.0,Im running a backend app with several endpoints on Cloud Run(fully-managed)  My endpoints are publicly available by its nature so I dont want to authenticate users through my client app hosted on Netlify  What I do need is to restrict access to my endpoints so that other applications or malicious users cant abuse it  It is not about scaling  I just dont want to exceed the  since it is a demo of an opensource application  Ive already set the  and  limits to minimum but this alone is not enough  There is also a product named  but it seems an expensive one  not free  I was expecting to have a simple built-in solution for this but couldnt find it  What other solutions do I have  How can I block the traffic coming out of my website on Netlify  
62688990,1,62691694.0,,2020-07-02 04:23:50,,2,1617,"<p>I am very new to AWS Lambda and am struggling to understand its functionalities based on many examples I found online (+reading endless documentations). I understand the primary goal of using such service is to implement a serverless architecture that is cost and probably effort-efficient by allowing Lambda and the API Gateway to take on the role of managing your server (so serverless does not mean you don't use a server, but the architecture takes care of things for you). I organized my research into two general approaches taken by developers to deploy a Flask web application to Lambda:</p>
<ol>
<li><p>Deploy the <strong>entire application</strong> to Lambda using zappa-and zappa configurations (json file) will be the API Gateway authentication.</p>
</li>
<li><p>Deploy <strong>only the function</strong>, the parsing blackbox that transforms user input to a form the backend endpoint is expecting (and backwards as well) -&gt; Grab a proxy url from the API Gateway that configures Lambda proxy -&gt; Have a separate application program that uses the url</p>
</li>
</ol>
<p>(and then there's 3, which does not use the API Gateway and invokes the Lambda function in the application itself-but I really want to get a hands on experience using the API Gateway)</p>
<p>Here are the questions I have for each of the above two approaches:</p>
<p>For 1, I don't understand how Lambda is calling the functions in the Flask application. According to my understanding, Lambda only calls functions that have the parameters event and context-or are the url calls (url formulated by the API Gateway) actually the events calling the separate functions in the Flask application, thus enabling Lambda to function as a 'serverless' environment-this doesn't make sense to me because event, in most of the examples I analyzed, is the user input data. That means some of the functions in the application have no event and some do, which means Lambda somehow magically figures out what to do with different function calls?</p>
<p>I also know that Lambda does have limited capacity, so is this the best way? It seems to be the standard way of deploying a web application on Lambda.</p>
<p>For 2, I understand the steps leading to incorporating the API Gateway urls in the Flask application. The Flask application therefore will use the url to access the Lambda function and have HTTP endpoints for user access. HOWEVER, that means, if I have the Flask application on my local computer, the application will only be hosted when I run the application on my computer-I would like it to have persistent public access (hopefully). I read about AWS Cloud9-would this be a good solution? Where should I deploy the application itself to optimize this architecture-without using services that take away the architecture's severless-ness (like an EC2 instance maybe OR on S3, where I would be putting my frontend html files, and host a website)? Also, going back to 1 (sorry I am trying to organize my ideas in a coherent way, and it's not working too well), will the application run consistently as long as I leave the API Gateway endpoint open?</p>
<p>I don't know what's the best practice for deploying a Flask application using AWS Lambda and the API Gateway but based on my findings, the above two are most frequently used. It would be really helpful if you could answer my questions so I could actually start playing with AWS Lambda! Thank you! (+I did read all the Amazon documentations, and these are the final-remaining questions I have before I start experimenting:))</p>
",13850966.0,,174777.0,,2020-07-02 08:34:43,2020-07-02 08:34:43,Flask web application deployment via AWS Lambda,<flask><aws-lambda><aws-api-gateway><serverless>,1,0,,,,CC BY-SA 4.0,I am very new to AWS Lambda and am struggling to understand its functionalities based on many examples I found online (+reading endless documentations)  I understand the primary goal of using such service is to implement a serverless architecture that is cost and probably effort-efficient by allowing Lambda and the API Gateway to take on the role of managing your server (so serverless does not mean you dont use a server  but the architecture takes care of things for you)  I organized my research into two general approaches taken by developers to deploy a Flask web application to Lambda:  Deploy the entire application to Lambda using zappa-and zappa configurations (json file) will be the API Gateway authentication   Deploy only the function  the parsing blackbox that transforms user input to a form the backend endpoint is expecting (and backwards as well) -&gt; Grab a proxy url from the API Gateway that configures Lambda proxy -&gt; Have a separate application program that uses the url   (and then theres 3  which does not use the API Gateway and invokes the Lambda function in the application itself-but I really want to get a hands on experience using the API Gateway) Here are the questions I have for each of the above two approaches: For 1  I dont understand how Lambda is calling the functions in the Flask application  According to my understanding  Lambda only calls functions that have the parameters event and context-or are the url calls (url formulated by the API Gateway) actually the events calling the separate functions in the Flask application  thus enabling Lambda to function as a serverless environment-this doesnt make sense to me because event  in most of the examples I analyzed  is the user input data  That means some of the functions in the application have no event and some do  which means Lambda somehow magically figures out what to do with different function calls  I also know that Lambda does have limited capacity  so is this the best way  It seems to be the standard way of deploying a web application on Lambda  For 2  I understand the steps leading to incorporating the API Gateway urls in the Flask application  The Flask application therefore will use the url to access the Lambda function and have HTTP endpoints for user access  HOWEVER  that means  if I have the Flask application on my local computer  the application will only be hosted when I run the application on my computer-I would like it to have persistent public access (hopefully)  I read about AWS Cloud9-would this be a good solution  Where should I deploy the application itself to optimize this architecture-without using services that take away the architectures severless-ness (like an EC2 instance maybe OR on S3  where I would be putting my frontend html files  and host a website)  Also  going back to 1 (sorry I am trying to organize my ideas in a coherent way  and its not working too well)  will the application run consistently as long as I leave the API Gateway endpoint open  I dont know whats the best practice for deploying a Flask application using AWS Lambda and the API Gateway but based on my findings  the above two are most frequently used  It would be really helpful if you could answer my questions so I could actually start playing with AWS Lambda  Thank you  (+I did read all the Amazon documentations  and these are the final-remaining questions I have before I start experimenting:)) 
62699781,1,,,2020-07-02 15:23:19,,0,693,"<p>I'm attempting to use an AWS Lambda function as a scheduling agent to post recurring payments to a payment api (USIO). The function used to make the post request is as follows:</p>
<pre><code>const doPostRequest = async (data) =&gt; {

  return new Promise((resolve, reject) =&gt; {
    const options = {
      host: &quot;api.securepds.com&quot;,
      path: &quot;/2.0/payments.svc/JSON/SubmitTokenPayment&quot;,
      method: 'POST',
      headers: {
        'Content-Type': 'https://application/json',
        'Access-Control-Allow-Origin': '*',
      }
    };

    //create the request object with the callback with the result
    const req = https.request(options, (res) =&gt; {
      res.setEncoding('utf8');
      resolve(JSON.stringify(res.statusCode));
    });

    // handle the possible errors
    req.on('error', (e) =&gt; {
      reject(e.message);
    });

    console.log(req)
    //do the request
    req.write(JSON.stringify(data));

    //finish the request
    req.end();
  });
};
</code></pre>
<p>where data is an object like so:</p>
<pre><code>var obj = `{&quot;MerchantID&quot;:&quot;${merchantId}&quot;,
&quot;Login&quot;:&quot;${merchantLogin}&quot;,
&quot;Password&quot;:&quot;${merchantPassword}&quot;,
&quot;Token&quot;: &quot;${token}&quot;,
&quot;Amount&quot;: &quot;${result.Items[i]['amount']}&quot;
</code></pre>
<p>and the post function is called like so:</p>
<pre><code>await doPostRequest(obj)
  .then(result =&gt; console.log(`Status code: ${result}`))
  .catch(err =&gt; console.log(`Error doing the request for the event: ${JSON.stringify(event)} =&gt; ${err}`));
</code></pre>
<p>Despite verifying that obj inputs are all valid, I continue to receive a &quot;Status code: 404&quot; logged when the POST request is made.  No VPCs have been added to the lambda function.</p>
<p>I'm new to making api calls via lambda, but was able to receive a 200 status code when testing the function with a different api address.  Wondering what may be causing the 404 status code in this instance!</p>
",13792257.0,,,,,2020-08-26 18:52:42,Receiving a 404 Error on AWS Lambda Post Request,<api><http><aws-lambda>,1,0,1.0,,,CC BY-SA 4.0,Im attempting to use an AWS Lambda function as a scheduling agent to post recurring payments to a payment api (USIO)  The function used to make the post request is as follows:  where data is an object like so:  and the post function is called like so:  Despite verifying that obj inputs are all valid  I continue to receive a Status code: 404 logged when the POST request is made   No VPCs have been added to the lambda function  Im new to making api calls via lambda  but was able to receive a 200 status code when testing the function with a different api address   Wondering what may be causing the 404 status code in this instance  
33001798,1,33002028.0,,2015-10-07 20:24:02,,9,1346,"<p>I plan to have the following setup:</p>

<ol>
<li>Completely STATIC front-end web interface (built with AngularJS or the likes)</li>
<li>Serverless Framework back-end APIs</li>
</ol>

<p>I want to store my front-end in S3 and my back-end in Lambda.
Since I'm charged every time the lambda function gets executed, I don't want everyone to be able to make requests directly to it. On the other hand, I want to store my front-end simply in S3 as opposed to a server.</p>

<p>How do I go about protecting my back-end API from abuse or DoS?</p>
",2957293.0,,1353441.0,,2016-01-31 12:01:36,2017-11-12 11:37:16,How to protect Serverless Framework endpoints from abuse / DoS?,<amazon-web-services><serverless-framework>,4,0,2.0,,,CC BY-SA 3.0,I plan to have the following setup:  Completely STATIC front-end web interface (built with AngularJS or the likes) Serverless Framework back-end APIs  I want to store my front-end in S3 and my back-end in Lambda  Since Im charged every time the lambda function gets executed  I dont want everyone to be able to make requests directly to it  On the other hand  I want to store my front-end simply in S3 as opposed to a server  How do I go about protecting my back-end API from abuse or DoS  
33004830,1,,,2015-10-08 00:50:04,,-1,131,"<p>I want to create a few HTTP points where mobile clients, servers &amp; IoT devices will be posting data. I may need to preprocess the events &amp; act up on them. Eventually I want to access all the raw data &amp; make queries using Domo, Cloud Business Intelligence | Chartio or Tableau .</p>

<p>I need to understand what are the differences &amp; advantages for the following architectures:</p>

<ol>
<li>AWS API Management + Lambda + Redshift: I can create an HTTP endpoint &amp; a lambda function that will parse the data, compute &amp; store in Redshift</li>
<li>Kinesis Firehose + Redshift (how do I stream the data over HTTP here?)</li>
<li>S3 + Kinesis + Redshift (I can use an HTTP endpoint that writes data to S3)</li>
<li>S3 + Kinesis Firehose + Redshift</li>
<li>S3 + Lambda + Redshift</li>
</ol>

<p>I feel like 3, 4 &amp; 5 create redundancy because of S3.
Will the execution of Lambda functions have a significant cost overhead over using Kinesis?</p>
",1438479.0,,,,,2018-12-12 19:35:36,What AWS technologies should I use in order to do light weight processing in real time before storing the data in Redshift?,<amazon-web-services><amazon-s3><amazon-redshift><aws-lambda><amazon-kinesis>,2,0,,,,CC BY-SA 3.0,I want to create a few HTTP points where mobile clients  servers &amp; IoT devices will be posting data  I may need to preprocess the events &amp; act up on them  Eventually I want to access all the raw data &amp; make queries using Domo  Cloud Business Intelligence | Chartio or Tableau   I need to understand what are the differences &amp; advantages for the following architectures:  AWS API Management + Lambda + Redshift: I can create an HTTP endpoint &amp; a lambda function that will parse the data  compute &amp; store in Redshift Kinesis Firehose + Redshift (how do I stream the data over HTTP here ) S3 + Kinesis + Redshift (I can use an HTTP endpoint that writes data to S3) S3 + Kinesis Firehose + Redshift S3 + Lambda + Redshift  I feel like 3  4 &amp; 5 create redundancy because of S3  Will the execution of Lambda functions have a significant cost overhead over using Kinesis  
62713941,1,,,2020-07-03 10:54:27,,1,583,"<p>I have an AWS Lambda within VPC (connects to RDS) which suffers from a typical cold start issue. I have read a couple of articles on how to make things faster so I deployed <a href=""https://aws.amazon.com/blogs/compute/new-for-aws-lambda-predictable-start-up-times-with-provisioned-concurrency/"" rel=""nofollow noreferrer"">Provisioned concurrency</a></p>
<p>Details: .NET Core 3.1 environment, ASP.NET Core project running in AWS Lambda, connecting PostgreSQL RDS dbo, VPC environment.</p>
<p>During the testing phase, to keep costs as low as possible, I have just 3 Provisioned Concurrency ($10)</p>
<p>Before Provisioned Concurrency, the first request after few hours of inactivity took between 15 to 20 seconds. Next requests, fired shortly after, took less than a second. Some of the requests then randomly took again 15 seconds (I think due to parallel invocation).</p>
<p>After Provissioned Concurrency first requests after a day of inactivity took 14 seconds.</p>
<p>My expectations of Provisioned Concurrency was to get rid of the Cold Start. AWS Blogs or other Blogs have supported my wishes, however, there is still a gotcha. I think this is due to VPC.</p>
<p>Thanks for any hints or experience sharing!</p>
",8244989.0,,13836474.0,,2020-07-03 11:02:41,2020-07-03 11:53:51,AWS Lambda Provisioned Concurrency in VPC (ColdStart),<c#><.net-core><aws-lambda><amazon-vpc>,1,0,,,,CC BY-SA 4.0,I have an AWS Lambda within VPC (connects to RDS) which suffers from a typical cold start issue  I have read a couple of articles on how to make things faster so I deployed  Details:  NET Core 3 1 environment  ASP NET Core project running in AWS Lambda  connecting PostgreSQL RDS dbo  VPC environment  During the testing phase  to keep costs as low as possible  I have just 3 Provisioned Concurrency ($10) Before Provisioned Concurrency  the first request after few hours of inactivity took between 15 to 20 seconds  Next requests  fired shortly after  took less than a second  Some of the requests then randomly took again 15 seconds (I think due to parallel invocation)  After Provissioned Concurrency first requests after a day of inactivity took 14 seconds  My expectations of Provisioned Concurrency was to get rid of the Cold Start  AWS Blogs or other Blogs have supported my wishes  however  there is still a gotcha  I think this is due to VPC  Thanks for any hints or experience sharing  
62721295,1,62721366.0,,2020-07-03 18:59:11,,1,53,"<p>I am trying to build a <strong>secure teaching platform</strong> in AWS. I am planning to host my videos on vooplayer (now spotlightr) and just have the front end web page with authentication, few pages to display the course videos. I am thinking I can host the website on <strong>S3</strong>, use <strong>Cognito</strong> for authentication and authorization and store user data and course data i.e. video urls in <strong>DynamoDB</strong> and may be few <strong>Lambda functions</strong> and <strong>API Gateway</strong>.</p>
<p>Am I going in the right direction in choosing this <strong>serverless architecture</strong> for the use case?</p>
<p>I am just going to store few collection in DynamoDB</p>
<ul>
<li>User - email, name</li>
<li>Course - course id, course name, external (vooplayer/spotlightr) video url, cost</li>
<li>Purchase - email, course, date of purchase, amount paid, currency</li>
<li>Activity - email, course id, started at, valid till</li>
</ul>
<p>I estimate there will be 100 users in next 6 months, may be more if things goes well.</p>
<p>I hope storing this data shouldn't cost much in the long run.</p>
<p>I chose <strong>vooplayer/spotlightr</strong> for their encryption and making it really difficult for malicious users to download and play the videos unlike other popular video streaming platforms we know.</p>
<p>I hope streaming video from external hosting sites with front end hosted on AWS won't get charged. Please correct me and point me in the right direction otherwise.</p>
<p>I know I could use off-the-shelf products like Udemy, Teachable, etc. I am between jobs and want to build something and learn something new while building it. Please advise.</p>
",5754094.0,,174777.0,,2020-07-04 06:05:58,2020-07-04 06:05:58,Teaching platform on AWS Serverless Architecture,<amazon-web-services><architecture><aws-serverless>,1,0,0.0,,,CC BY-SA 4.0,I am trying to build a secure teaching platform in AWS  I am planning to host my videos on vooplayer (now spotlightr) and just have the front end web page with authentication  few pages to display the course videos  I am thinking I can host the website on S3  use Cognito for authentication and authorization and store user data and course data i e  video urls in DynamoDB and may be few Lambda functions and API Gateway  Am I going in the right direction in choosing this serverless architecture for the use case  I am just going to store few collection in DynamoDB  User - email  name Course - course id  course name  external (vooplayer/spotlightr) video url  cost Purchase - email  course  date of purchase  amount paid  currency Activity - email  course id  started at  valid till  I estimate there will be 100 users in next 6 months  may be more if things goes well  I hope storing this data shouldnt cost much in the long run  I chose vooplayer/spotlightr for their encryption and making it really difficult for malicious users to download and play the videos unlike other popular video streaming platforms we know  I hope streaming video from external hosting sites with front end hosted on AWS wont get charged  Please correct me and point me in the right direction otherwise  I know I could use off-the-shelf products like Udemy  Teachable  etc  I am between jobs and want to build something and learn something new while building it  Please advise  
62723924,1,,,2020-07-03 23:53:35,,4,960,"<p>I'm planning to promote a set of Azure Functions (~15) to production. Today we're using for development purposes the Consumption plan but the cold start nature of this plan might impact the overall experience.</p>
<p>I've been searching for a best cost-benefit plan for deploying the application and found that the Elastic Plan EP1 would fit our needs (e.g. no cold start, rapid scale out, &quot;... share an App Service Plan accross multiple funcion apps ...&quot;, etc).</p>
<p>The problem is that I didn't find precisely how this Plan would be charged...</p>
<p>In the scenario exposed, would I be charged 388.67 BRL for each of the approx. 15 functions deployed to the Plan? Or would the charge be for the Plan itself and the Functions would share the resources of the Plan?</p>
<p>And also, all the Function Apps on the Plan would be pre-warmed?</p>
<p><a href=""https://i.stack.imgur.com/5nzn9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5nzn9.png"" alt=""enter image description here"" /></a></p>
<hr />
<p><strong>EDIT:</strong></p>
<p>Even not finding the answers clearly on the official documentation, I created the EP1 Plan and deployed the Functions.</p>
<ol>
<li>I found that for a given App Service Plan (e.g. Elastic Plan EP1), I can deploy many Function Apps to it, but they share resources of that Plan, increasing the &quot;<a href=""https://azure.github.io/AppService/2019/05/21/App-Service-Plan-Density-Check.html"" rel=""nofollow noreferrer"">app density</a>&quot;.</li>
<li>I still don't get the answer for the cold start question: for that Plan, if I deploy the 15 Function Apps to it would them be pre-warmed? I found that I can set &quot;pre-warmed=1&quot; in every Function App, but still experiencing cold starts.</li>
</ol>
",1864645.0,,1864645.0,,2020-07-10 00:11:51,2020-07-10 00:11:51,Pricing for deploying multiple Azure Functions to the same App Service Plan (Elastic Premium Plan - EP1),<azure><azure-functions><serverless>,1,1,,,,CC BY-SA 4.0,Im planning to promote a set of Azure Functions (~15) to production  Today were using for development purposes the Consumption plan but the cold start nature of this plan might impact the overall experience  Ive been searching for a best cost-benefit plan for deploying the application and found that the Elastic Plan EP1 would fit our needs (e g  no cold start  rapid scale out      share an App Service Plan accross multiple funcion apps      etc)  The problem is that I didnt find precisely how this Plan would be charged    In the scenario exposed  would I be charged 388 67 BRL for each of the approx  15 functions deployed to the Plan  Or would the charge be for the Plan itself and the Functions would share the resources of the Plan  And also  all the Function Apps on the Plan would be pre-warmed    EDIT: Even not finding the answers clearly on the official documentation  I created the EP1 Plan and deployed the Functions   I found that for a given App Service Plan (e g  Elastic Plan EP1)  I can deploy many Function Apps to it  but they share resources of that Plan  increasing the   I still dont get the answer for the cold start question: for that Plan  if I deploy the 15 Function Apps to it would them be pre-warmed  I found that I can set pre-warmed=1 in every Function App  but still experiencing cold starts   
67184556,1,,,2021-04-20 18:40:02,,0,434,"<p>I want to redirect all traffic to my pricing page via a subdomain to my homepage. I can't figure out how to add wildcards in the has value.</p>
<p>This syntax does not work:</p>
<pre><code>// next.config.js
module.exports = {
  target: 'serverless',
  async redirects() {
    return [
        {
          source: '/pricing',
          has: [
            {
              type: 'host',
              value: '*.*.*',
            },
          ],
          permanent: false,
          destination: 'https://example.com/'
        }
      ]
  },
}
</code></pre>
<p>Any ideas?</p>
",6519174.0,,,,,2021-04-22 10:33:59,Redirect all traffic from a subdomain on NextJS,<next.js><vercel>,2,2,,,,CC BY-SA 4.0,I want to redirect all traffic to my pricing page via a subdomain to my homepage  I cant figure out how to add wildcards in the has value  This syntax does not work:  Any ideas  
67279181,1,,,2021-04-27 08:05:49,,0,450,"<p>I am building out an e-commerce site, with next.js and stripe checkout. I keep running into this error when I am going to checkout. I am using the <code>use-shopping-cart</code> package as well, and I'm starting to think it maybe causing the below response error from <code>stripe-checkout</code></p>
<pre><code>  &quot;error&quot;: {
    &quot;message&quot;: &quot;Invalid API Key provided: undefined&quot;,
    &quot;type&quot;: &quot;invalid_request_error&quot;
  }
}
</code></pre>
<p>What is super confusing is that the checkout session in created, along with the cart and payment intent (I know this to be true because I can verify this information thru the stripe dashboard). Currently the stripe dashboard does not indicate there is an error either.</p>
<p>Has anyone experienced this, and does anyone have any idea on how to fix it?</p>
<p>Thanks in advance!</p>
",10056500.0,,,,,2021-04-28 01:19:25,Error when redirection to stripe checkout,<javascript><reactjs><stripe-payments><next.js><vercel>,1,3,,,,CC BY-SA 4.0,I am building out an e-commerce site  with next js and stripe checkout  I keep running into this error when I am going to checkout  I am using the  package as well  and Im starting to think it maybe causing the below response error from   What is super confusing is that the checkout session in created  along with the cart and payment intent (I know this to be true because I can verify this information thru the stripe dashboard)  Currently the stripe dashboard does not indicate there is an error either  Has anyone experienced this  and does anyone have any idea on how to fix it  Thanks in advance  
67300765,1,67303546.0,,2021-04-28 13:02:12,,0,269,"<p>Since there is aditional costs for using HTTP and REST apis on AWS lambda, i would like to know if i could make AWS Lambda receive gets and posts without the need of these HTTP API services.</p>
<p>In this example it seems to be possible:</p>
<p><a href=""https://github.com/serverless/examples/tree/master/aws-node-simple-http-endpoint"" rel=""nofollow noreferrer"">https://github.com/serverless/examples/tree/master/aws-node-simple-http-endpoint</a></p>
",7314165.0,,13070.0,,2021-04-28 13:06:37,2021-04-28 16:04:48,How to use aws lambda without HTTP api?,<amazon-web-services><aws-lambda><aws-http-api>,2,0,,,,CC BY-SA 4.0,Since there is aditional costs for using HTTP and REST apis on AWS lambda  i would like to know if i could make AWS Lambda receive gets and posts without the need of these HTTP API services  In this example it seems to be possible:  
67308220,1,,,2021-04-28 21:34:13,,0,27,"<p>We are considering using azure functions to run some compute on. However our computes could take up lots of memory, lets say more than 5GB.</p>
<p>As I understand there is no easy way to scale azure functions based on memory usage. Ie If you reach 15GB start a new instance (since you don't want it to run over the maximum memory of your instance)?</p>
<p><strong>Is there a way around this limitation?</strong></p>
<p>OR</p>
<p><strong>Is there another technical alternative to azure functions that provide pay per use and allows rapid scaling on demand?</strong></p>
",834815.0,,,,,2021-04-28 21:54:29,On memory intensive on-demand compute,<aws-lambda><azure-functions>,1,0,,,,CC BY-SA 4.0,We are considering using azure functions to run some compute on  However our computes could take up lots of memory  lets say more than 5GB  As I understand there is no easy way to scale azure functions based on memory usage  Ie If you reach 15GB start a new instance (since you dont want it to run over the maximum memory of your instance)  Is there a way around this limitation  OR Is there another technical alternative to azure functions that provide pay per use and allows rapid scaling on demand  
